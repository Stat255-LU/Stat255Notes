[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 255 Notes",
    "section": "",
    "text": "Preface\nThese notes serve as the primary textual resource for Stat 255: Statistics for Data Science at Lawrence University.\nWhat is this course about?\nStat 255 provides an introduction to essential statistical tasks including modeling, inference, prediction, and computation. The course employs a modern approach, intended to equip students with skills needed for working with today’s complex data. Traditional concepts, like interval estimation and hypothesis testing, are introduced through the lens of multivariate models and simulation. Data computation in R plays a central role throughout the course.\nThe course’s overarching learning outcomes are:\n\nVisualize and wrangle data using statistical software R.\n\nBuild and assess multivariate models to predict future outcomes.\n\nUse statistics from samples to draw inferences about larger populations or processes.\n\nQuantify uncertainty associated with estimates and predictions.\n\nExplain the assumptions associated with statistical models, and evaluate whether these assumptions are reasonably satisfied in context.\n\nWrite reproducible analyses, using statistical software.\n\nMake ethical decisions based on data.\n\nMore specific learning tasks, related to these outcomes are provided in each chapter.\nWho is this course intended for?\nThis course is intended for students who are interested in learning statistical modeling and data computation skills that might prove useful in further courses, research, or career.\nStat 255 can serve as either:\n\na first course in statistics for students with a strong quantitative background, typically including calculus.\na second course in statistics, building on introductory topics taught in courses like Lawrence’s Stat 107: Principles of Statistics or AP Statistics.\n\nAt Lawrence, this course is required for the statistics track of the mathematics major, the economics and mathematics-economics majors, the business analytics track of the business and entrepreneurship major, and the statistics and data science minor. It also satisfies the statistics requirement for several other majors and minors.\nThe prerequisite for the course is either 1) a prior college-level course in statistics (i.e. STAT 107, BIOL 170 or 280, ANTH 207, AP Stats) OR 2) Calculus. (Math 140, AP Calculus, or equivalent).\nThe course does not assume any prior knowledge of statistics, but does move more rapidly than a typical introductory statistics course. Students engage rigorously in statistical thinking and computation, intended to equip them with essential skills for further study in statistics and data science.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Ch1.html",
    "href": "Ch1.html",
    "title": "1  Visualizing and Summarizing Data",
    "section": "",
    "text": "1.1 Getting Started in R\nWe’ll work with data on houses that sold in King County, WA, (home of Seattle) between 2014 and 2015.\nWe begin by loading the tidyverse package which can be used to create professional data graphics and summaries.\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch1.html#getting-started-in-r",
    "href": "Ch1.html#getting-started-in-r",
    "title": "1  Visualizing and Summarizing Data",
    "section": "",
    "text": "1.1.1 Previewing the Data\nhead()\nThe head() function displays the first 5 rows of the dataset.\n\nhead(Houses)\n\n# A tibble: 6 × 9\n     Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront\n  &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;     \n1     1 1225         4      4.5         5420   101930 average   No        \n2     2  885.        4      2.5         2830     5000 average   No        \n3     3  385.        4      1.75        1620     4980 good      No        \n4     4  253.        2      1.5         1070     9643 average   No        \n5     5  468.        2      1           1160     6000 good      No        \n6     6  310.        3      1           1430    19901 good      No        \n# ℹ 1 more variable: yr_built &lt;dbl&gt;\n\n\nThe rows of the dataset are called observations. In this case, the observations are the houses.\nThe columns of the dataset, which contain information about the houses, are called variables.\nglimpse\nThe glimpse() command shows the number of observations (rows), and the number of variables, (columns). We also see the name of each variable and its type. Variable types include\n\nCategorical variables, which take on groups or categories, rather than numeric values. In R, these might be coded as logical &lt;logi&gt;, character &lt;chr&gt;, factor &lt;fct&gt; and ordered factor &lt;ord&gt;.\nQuantitative variables, which take on meaningful numeric values. These include numeric &lt;num&gt;, integer &lt;int&gt;, and double &lt;dbl&gt;.\nDate and time variables take on values that are dates and times, and are denoted &lt;dttm&gt;\n\n\nglimpse(Houses)\n\nRows: 100\nColumns: 9\n$ Id          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ price       &lt;dbl&gt; 1225.00, 885.00, 385.00, 252.70, 468.00, 310.00, 550.00, 4…\n$ bedrooms    &lt;dbl&gt; 4, 4, 4, 2, 2, 3, 4, 4, 3, 3, 3, 4, 5, 3, 4, 4, 3, 4, 3, 3…\n$ bathrooms   &lt;dbl&gt; 4.50, 2.50, 1.75, 1.50, 1.00, 1.00, 1.00, 1.00, 1.00, 2.25…\n$ sqft_living &lt;dbl&gt; 5420, 2830, 1620, 1070, 1160, 1430, 1660, 1600, 960, 1660,…\n$ sqft_lot    &lt;dbl&gt; 101930, 5000, 4980, 9643, 6000, 19901, 34848, 4300, 6634, …\n$ condition   &lt;fct&gt; average, average, good, average, good, good, poor, good, a…\n$ waterfront  &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No…\n$ yr_built    &lt;dbl&gt; 2001, 1995, 1947, 1985, 1942, 1927, 1933, 1916, 1952, 1979…\n\n\nThere are 100 houses in the dataset, and 9 variables on each house.\nsummary\nsummary displays the mean, minimum, first quartile, median, third quartile, and maximum for each numeric variable, and the number of observations in each category, for categorical variables.\n\nsummary(Houses)\n\n       Id             price           bedrooms      bathrooms    \n Min.   :  1.00   Min.   : 180.0   Min.   :1.00   Min.   :0.750  \n 1st Qu.: 25.75   1st Qu.: 322.9   1st Qu.:3.00   1st Qu.:1.500  \n Median : 50.50   Median : 507.5   Median :3.00   Median :2.000  \n Mean   : 50.50   Mean   : 735.4   Mean   :3.39   Mean   :2.107  \n 3rd Qu.: 75.25   3rd Qu.: 733.8   3rd Qu.:4.00   3rd Qu.:2.500  \n Max.   :100.00   Max.   :5300.0   Max.   :6.00   Max.   :6.000  \n  sqft_living      sqft_lot          condition  waterfront    yr_built   \n Min.   : 440   Min.   :  1044   poor     : 1   No :85     Min.   :1900  \n 1st Qu.:1410   1st Qu.:  5090   fair     : 1   Yes:15     1st Qu.:1948  \n Median :2000   Median :  7852   average  :59              Median :1966  \n Mean   :2291   Mean   : 13205   good     :30              Mean   :1965  \n 3rd Qu.:2735   3rd Qu.: 12246   very_good: 9              3rd Qu.:1991  \n Max.   :8010   Max.   :101930                             Max.   :2014  \n\n\n\n\n1.1.2 Modifying the Data\nNext we’ll look at how to manipulate the data and create new variables.\n\nAdding a New Variable\nWe can use the mutate() function to create a new variable based on variables already in the dataset.\nLet’s add a variable giving the age of the house, as of 2015.\n\nHouses &lt;- Houses |&gt; mutate(age = 2015-yr_built)\nhead(Houses)\n\n# A tibble: 6 × 10\n     Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront\n  &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;     \n1     1 1225         4      4.5         5420   101930 average   No        \n2     2  885.        4      2.5         2830     5000 average   No        \n3     3  385.        4      1.75        1620     4980 good      No        \n4     4  253.        2      1.5         1070     9643 average   No        \n5     5  468.        2      1           1160     6000 good      No        \n6     6  310.        3      1           1430    19901 good      No        \n# ℹ 2 more variables: yr_built &lt;dbl&gt;, age &lt;dbl&gt;\n\n\n\n\nSelecting Columns\nIf the dataset contains a large number of variables, narrow down to the ones you are interested in working with. This can be done with the select() command. If there are not very many variables to begin with, or you are interested in all of them, then you may skip this step.\nLet’s create a smaller version of the dataset, with only the columns price, sqft_living, and waterfront. We’ll call this Houses_3var.\n\nHouses_3var &lt;- Houses |&gt; select(price, sqft_living, waterfront)\nhead(Houses_3var)\n\n# A tibble: 6 × 3\n  price sqft_living waterfront\n  &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n1 1225         5420 No        \n2  885.        2830 No        \n3  385.        1620 No        \n4  253.        1070 No        \n5  468.        1160 No        \n6  310.        1430 No        \n\n\n\n\nFiltering by Row\nThe filter() command narrows a dataset down to rows that meet specified conditions.\nWe’ll filter the data to include only houses built after 2000.\n\nNew_Houses &lt;- Houses |&gt; filter(yr_built&gt;=2000)\nhead(New_Houses)\n\n# A tibble: 6 × 10\n     Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront\n  &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;     \n1     1 1225         4      4.5         5420   101930 average   No        \n2    16 3075         4      5           4550    18641 average   Yes       \n3    23  862.        5      2.75        3595     5639 average   No        \n4    24  360.        4      2.5         2380     5000 average   No        \n5    25  625.        4      2.5         2570     5520 average   No        \n6    27  488.        3      2.5         3160    13603 average   No        \n# ℹ 2 more variables: yr_built &lt;dbl&gt;, age &lt;dbl&gt;\n\n\nNow, we’ll filter the data to include only houses on the waterfront.\n\nNew_Houses &lt;- Houses |&gt; filter(waterfront == \"Yes\")\nhead(New_Houses)\n\n# A tibble: 6 × 10\n     Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront\n  &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;     \n1    16 3075         4      5           4550    18641 average   Yes       \n2    19  995.        3      4.5         4380    47044 average   Yes       \n3    34  825.        2      1           1150    12775 good      Yes       \n4    40 2400.        4      2.5         3650     8354 average   Yes       \n5    42  290.        2      0.75         440     8313 good      Yes       \n6    46 5111.        5      5.25        8010    45517 average   Yes       \n# ℹ 2 more variables: yr_built &lt;dbl&gt;, age &lt;dbl&gt;",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch1.html#summary-statistics",
    "href": "Ch1.html#summary-statistics",
    "title": "1  Visualizing and Summarizing Data",
    "section": "1.2 Summary Statistics",
    "text": "1.2 Summary Statistics\n\n1.2.1 Measures of Center\nCommon ways to characterize the center of a distribution include mean, median, and mode.\nFor a set of \\(n\\) values \\(y_i, \\ldots, y_n\\):\n\nmean (\\(\\bar{y}\\)) represents the numerical average and is calculated by \\(\\bar{y} =\\frac{1}{n}\\displaystyle\\sum_{i=1}^n y_i\\).\nmedian represents the middle number when the values are arranged from least to greatest. If there are an even number of values in the dataset, then the median is given by the average of the middle two numbers.\n\nThe median of the upper half of the values is called the upper (or 3rd) quartile. This represents the 75th percentile in the distribution.\nThe median of the upper half of the values is called the lower (or 1st) quartile. This represents the 25th percentile in the distribution.\n\nmode is the most frequently occurring number in the data.\n\n\n\n1.2.2 Measures of Spread\nCommon ways of measuring the amount of spread, or variability, in a variable include:\n\nrange: the difference between the maximum and minimum values\ninterquartile range: the difference between the upper and lower quartiles (i.e. the range of the middle 50% of the values).\nstandard deviation (\\(s\\)): standard deviation is approximately the average deviation between an observation and the mean. It is calculated by\n\\(s =\\sqrt{\\displaystyle\\sum_{i=1}^n \\frac{(y_i-\\bar{y})^2}{n-1}}\\).\nThe square of the standard deviation, called the variance is denoted \\(s^2\\).\n\n\n\n1.2.3 Calcularing Summary Statistics in R\nLet’s calculate the mean, median, and standard deviation, in prices.\n\nHouses_Summary &lt;- Houses |&gt; summarize(Mean_Price = mean(price, na.rm=TRUE), \n                                          Median_Price = median(price, na.rm=TRUE), \n                                          StDev_Price = sd(price, na.rm = TRUE),\n                                          Number_of_Houses = n()) \nHouses_Summary\n\n# A tibble: 1 × 4\n  Mean_Price Median_Price StDev_Price Number_of_Houses\n       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;            &lt;int&gt;\n1       735.         507.        835.              100\n\n\nNotes:\n1. The n() command calculates the number of observations.\n2. The na.rm=TRUE command removes missing values, so that summary statistics can be calculated. It’s not needed here, since this dataset doesn’t include missing values, but if the dataset does include missing values, you will need to include this, in order to do the calculation.\nThe kable() function in the knitr() package creates tables with professional appearance.\n\nlibrary(knitr)\nkable(Houses_Summary)\n\n\n\n\nMean_Price\nMedian_Price\nStDev_Price\nNumber_of_Houses\n\n\n\n\n735.3525\n507.5\n835.1231\n100\n\n\n\n\n\n\n\n1.2.4 Grouped Summaries\ngroup_by()\nThe group_by() command allows us to calculate summary statistics, with the data broken down by by category.We’ll compare waterfront houses to non-waterfront houses.\n\nHouses_Grouped_Summary &lt;- Houses |&gt; group_by(waterfront) |&gt; \n                                      summarize(Mean_Price = mean(price, na.rm=TRUE),\n                                                Median_Price = median(price, na.rm=TRUE), \n                                                StDev_Price = sd(price, na.rm = TRUE),\n                                                Number_of_Houses = n()) \nkable(Houses_Grouped_Summary)\n\n\n\n\nwaterfront\nMean_Price\nMedian_Price\nStDev_Price\nNumber_of_Houses\n\n\n\n\nNo\n523.7595\n450\n295.7991\n85\n\n\nYes\n1934.3800\n1350\n1610.7959\n15\n\n\n\n\n\nNote: arrange(desc(Mean_Gross)) arranges the table in descending order of Mean_Gross. To arrange in ascending order, use arrange(Mean_Gross).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch1.html#data-visualization",
    "href": "Ch1.html#data-visualization",
    "title": "1  Visualizing and Summarizing Data",
    "section": "1.3 Data Visualization",
    "text": "1.3 Data Visualization\nNext, we’ll create graphics to help us visualize the distributions and relationships between variables. We’ll use the ggplot() function, which is part of the tidyverse package.\n\n1.3.1 Histogram\nHistograms are useful for displaying the distribution of a single quantitative variable. In a histogram, the x-axis breaks the variable into ranges of values, and the y-axis displays the number of observations with a value falling in that category (frequency).\nGeneral Template for Histogram\n\nggplot(data=DatasetName, aes(x=VariableName)) + \n  geom_histogram(fill=\"colorchoice\", color=\"colorchoice\") + \n  ggtitle(\"Plot Title\") +\n  xlab(\"x-axis label\") + \n  ylab(\"y-axis label\")\n\nHistogram of House Prices\n\nggplot(data=Houses, aes(x=price)) + \n  geom_histogram(fill=\"lightblue\", color=\"white\") + \n  ggtitle(\"Distribution of House Prices\") +\n  xlab(\"Price (in thousands)\") + \n  ylab(\"Frequency\")\n\n\n\n\n\n\n\n\nWe see that the distribution of house prices is right-skewed. Most houses cost less than $1,000,000, though there are a few houses that are much more expensive. The most common price range is around $400,000 to $500,000.\n\n\n1.3.2 Density Plot\nDensity plots show the distribution for a quantitative variable price. Scores can be compared across categories, like whether or not the house is on a waterfront.\nGeneral Template for Density Plot\n\nggplot(data=DatasetName, aes(x=QuantitativeVariable,\n                             color=CategoricalVariable, fill=CategoricalVariable)) + \n  geom_density(alpha=0.2) + \n  ggtitle(\"Plot Title\") +\n  xlab(\"Axis Label\") + \n  ylab(\"Frequency\") \n\nalpha, ranging from 0 to 1 dictates transparency.\nDensity Plot of House Prices\n\nggplot(data=Houses, aes(x=price, color=waterfront, fill=waterfront)) + \n  geom_density(alpha=0.2) + \n  ggtitle(\"Distribution of Prices\") +\n  xlab(\"House price (in thousands)\") + \n  ylab(\"Frequency\") \n\n\n\n\n\n\n\n\nWe see that on average, houses on the waterfront tend to be more expensive and have a greater price range than houses not on the waterfront.\n\n\n1.3.3 Boxplot\nBoxplots can be used to compare a quantitative variable with a categorical variable. The middle 50% of observations are contained in the “box”, with the upper and lower 25% of the observations in each tail.\nGeneral Template for Boxplot\n\nggplot(data=DatasetName, aes(x=CategoricalVariable, \n                             y=QuantitativeVariable)) + \n  geom_boxplot() + \n  ggtitle(\"Plot Title\") + \n  xlab(\"Variable Name\") + ylab(\"Variable Name\") \n\nYou can make the plot horizontal by adding + coordflip(). You can turn the axis text vertical by adding theme(axis.text.x = element_text(angle = 90)).\nBoxplot Comparing Price by Waterfront Status\n\nggplot(data=Houses, aes(x=waterfront, y=price)) + geom_boxplot() + \n  ggtitle(\"House Price by Waterfront Status\") + \n  xlab(\"Waterfront\") + ylab(\"Price (in thousands)\") + coord_flip()\n\n\n\n\n\n\n\n\nFor houses not on the waterfront, the median price is about $400,000, and the middle 50% of prices range from about $300,000 to $600,000.\nFor waterfront houses, the median price is about $1,500,000, and the middle 50% of prices range from about $900,000 to $1,900,000.\n\n\n1.3.4 Violin Plot\nViolin plots are an alternative to boxplots. The width of the violin tells us the density of observations in a given range.\nGeneral Template for Violin Plot\n\nggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable, \n                             fill=CategoricalVariable)) + \n  geom_violin() + \n  ggtitle(\"Plot Title\") + \n  xlab(\"Variable Name\") + ylab(\"Variable Name\") \n\nViolin Plot Comparing Prices by Waterfront\n\nggplot(data=Houses, aes(x=waterfront, y=price, fill=waterfront)) + \n  geom_violin() + \n  ggtitle(\"Price by Waterfront Status\") + \n  xlab(\"Waterfront\") + ylab(\"Price (in thousands)\") + \n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nAgain, we see that houses on the waterfront tend to be more expensive than those not on the waterfront, and have a wider range in prices.\n\n\n1.3.5 Scatterplot\nScatterplots are used to visualize the relationship between two quantitative variables.\nScatterplot Template\n\nggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable)) + \n  geom_point() +\n  ggtitle(\"Plot Title\") + \n  ylab(\"Axis Label\") + \n  xlab(\"Axis Label\")\n\nScatterplot Comparing Price and Square Feet of Living Space\n\nggplot(data=Houses, aes(x=sqft_living, y=price)) + \n  geom_point() +\n  ggtitle(\"Price and Living Space\") + \n  ylab(\"Price (in thousands)\") + \n  xlab(\"Living Space in sq. ft. \")\n\n\n\n\n\n\n\n\nWe see that there is an upward trend, indicating that houses with more living space tend to, on average, be higher priced than those with less living space. The relationship appears to be roughly linear, though there might be some curvature, as living space gets very large. There are some exceptions to this trend, most notably a house with more than 7,000 square feet, priced just over $1,000,000.\nWe can also add color, size, and shape to the scatterplot to display information about other variables.\nWe’ll use color to illustrate whether the house is on the waterfront, and size to represent the square footage of the entire lot (including the yard and the house).\n\nggplot(data=Houses, \n       aes(x=sqft_living, y=price, color=waterfront, size=sqft_lot)) + \n  geom_point() +\n  ggtitle(\"Price of King County Houses\") + \n  ylab(\"Price (in thousands)\") + \n  xlab(\"Living Space in sq. ft. \")\n\n\n\n\n\n\n\n\nWe notice that many of the largest and most expensive houses are on the waterfront.\n\n\n1.3.6 Bar Graph\nBar graphs can be used to visualize one or more categorical variables. A bar graph is similar to a histogram, in that the y-axis again displays frequency, but the x-axis displays categories, instead of ranges of values.\nBar Graph Template\n\nggplot(data=DatasetName, aes(x=CategoricalVariable)) + \n  geom_bar(fill=\"colorchoice\",color=\"colorchoice\")  + \n  ggtitle(\"Plot Title\") + \n  xlab(\"Variable Name\") + \n  ylab(\"Frequency\") \n\nBar Graph by Condition\n\nggplot(data=Houses, aes(x=condition)) + \n  geom_bar(fill=\"lightblue\",color=\"white\")  + \n  ggtitle(\"Number of Houses by Condition\") + \n  xlab(\"Condition\") + \n  ylab(\"Frequency\") +   \n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nWe see that the majority of houses are in average condition. Some are in good or very good condition, while very few are in poor or very poor condition.\n\n\n1.3.7 Stacked and Side-by-Side Bar Graphs\nStacked Bar Graph Template\n\nggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, \n                                         fill = CategoricalVariable2)) +\n    stat_count(position=\"fill\")  +\n  theme_bw() + ggtitle(\"Plot Title\") + \n  xlab(\"Variable 1\") + \n  ylab(\"Proportion of Variable 2\") +   \n  theme(axis.text.x = element_text(angle = 90)) \n\nStacked Bar Graph Example\nThe stat_count(position=\"fill\") command creates a stacked bar graph, comparing two categorical variables. Let’s explore whether waterfront status is related to condition.\n\nggplot(data = Houses, mapping = aes(x = waterfront, fill = condition)) +\n    stat_count(position=\"fill\")  +\n  theme_bw() + ggtitle(\"Condition by Waterfront Status\") + \n  xlab(\"Waterfront Status\") + \n  ylab(\"Condition\") +   \n  theme(axis.text.x = element_text(angle = 90)) \n\n\n\n\n\n\n\n\nWe see that a higher proportion of waterfront houses are in good or excellent condition than non-waterfront houses.\nSide-by-side Bar Graph Template\nWe can create a side-by-side bar graph, using position=dodge.\n\nggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, \n                                         fill = CategoricalVariable2)) +\n    geom_bar(position = \"dodge\") +\n  ggtitle(\"Plot Title\") + \n  xlab(\"Genre\") + \n  ylab(\"Frequency\") \n\nSide-by-side Bar Graph Example\n\nggplot(data = Houses, mapping = aes(x = waterfront, fill = condition)) +\n    geom_bar(position = \"dodge\") +\n  ggtitle(\"Condition by Waterfront Status\") + \n  xlab(\"Waterfront Status\") + \n  ylab(\"Condition\") +   \n  theme(axis.text.x = element_text(angle = 90)) \n\n\n\n\n\n\n\n\nIn this case, since there are so few waterfront houses, the graph is hard to read and not very useful.\nThe stacked bar graph is a better way to convey information in this instance, though you may find that for a different dataset, the side-by-side bar graph could be a better choice.\n\n\n1.3.8 Correlation Plot\nCorrelation plots can be used to visualize relationships between quantitative variables. Correlation is a number between -1 and 1, describing the strength of the linear relationship between two variables. Variables with strong positive correlations will have correlation close to +1, while variables with strong negative correlations will have correlations close to -1. Variables with little to no relationship will have correlation close to 0.\nThe cor() function calculates correlations between quantitative variables. We’ll use select_if to select only numeric variables. The `use=“complete.obs” command tells R to ignore observations with missing data.\n\ncor(select_if(Houses, is.numeric), use=\"complete.obs\") |&gt; round(2)\n\n               Id price bedrooms bathrooms sqft_living sqft_lot yr_built   age\nId           1.00  0.03    -0.06     -0.01       -0.03    -0.07    -0.02  0.02\nprice        0.03  1.00     0.40      0.67        0.81     0.42     0.17 -0.17\nbedrooms    -0.06  0.40     1.00      0.58        0.58     0.15     0.26 -0.26\nbathrooms   -0.01  0.67     0.58      1.00        0.85     0.45     0.50 -0.50\nsqft_living -0.03  0.81     0.58      0.85        1.00     0.54     0.36 -0.36\nsqft_lot    -0.07  0.42     0.15      0.45        0.54     1.00     0.14 -0.14\nyr_built    -0.02  0.17     0.26      0.50        0.36     0.14     1.00 -1.00\nage          0.02 -0.17    -0.26     -0.50       -0.36    -0.14    -1.00  1.00\n\n\nThe corrplot() function in the corrplot() package provides a visualization of the correlations. Larger, thicker circles indicate stronger correlations.\n\nlibrary(corrplot)\nCorr &lt;- cor(select_if(Houses, is.numeric), use=\"complete.obs\")\ncorrplot(Corr)\n\n\n\n\n\n\n\n\nWe see that price has a strong positive correlation with square feet of living space, and is also positively correlated with number of bedrooms and bathrooms. Living space, bedrooms, and bathrooms are all positively correlated, which makes sense, since we would expect bigger houses to have more bedrooms and bathrooms. Price does not show much correlation with the other variables. We notice that bathrooms is negatively correlated with age, which means older houses tend to have fewer bathrooms than newer ones. Not surprisingly, age is very strongly correlated with year built.\n\n\n1.3.9 Scatterplot Matrix\nA scatterplot matrix is a grid of plots. It can be created using the ggpairs() function in the GGally package.\nThe scatterplot matrix shows us:\n\nAlong the diagonal are density plots for quantitative variables, or bar graphs for categorical variables, showing the distribution of each variable.\n\nUnder the diagonal are plots showing the relationships between the variables in the corresponding row and column. Scatterplots are used when both variables are quantitative, bar graphs are used when both variables are categorical, and boxplots are used when one variable is categorical, and the other is quantitative.\n\nAbove the diagonal are correlations between quantitative variables.\n\nIncluding too many variables can make these hard to read, so it’s a good idea to use select to narrow down the number of variables.\n\nlibrary(GGally)\nggpairs(Houses |&gt; select(price, sqft_living, condition, age))\n\n\n\n\n\n\n\n\nThe scatterplot matrix is useful for helping us notice key trends in our data. However, the plot can hard to read as it is quite dense, especially when there are a large number of variables. These can help us look for trends from a distance, but we should then focus in on more specific plots.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch2.html",
    "href": "Ch2.html",
    "title": "2  Introduction to Statistical Models",
    "section": "",
    "text": "2.1 Fitting Models to Data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch2.html#fitting-models-to-data",
    "href": "Ch2.html#fitting-models-to-data",
    "title": "2  Introduction to Statistical Models",
    "section": "",
    "text": "2.1.1 Terminology\nIn this section, we’ll use statistical models to predict the prices of houses in King County, WA.\nIn a statistical model,\n\nThe variable we are trying to predict (price) is called the response variable (denoted \\(Y\\)).\nVariable(s) we use to help us make the prediction is(are) called explanatory variables (denoted \\(X\\)). These are also referred to as predictor variables or covariates.\n\nIn this section, we’ll attempt to predict the price of a house, using information about its size (in square feet), and whether or not it is on the waterfront. The price is our response variable, while size and waterfront location are explanatory variables.\n\nCategorical variables are variables that take on groups or categories, rather than numeric values, for example, whether or not the house is on the waterfront.\nQuantitative variables take on meaningful numeric values, for example the number of square feet in the house.\n\n\n\n2.1.2 Quantitative Explanatory Variable\nWe’ll first predict the price of the house, using the number of square feet of living space as our explanatory variable.\nWe’ll assume that price changes linearly with square feet, and fit a trend line to the data.\n\nggplot(data=Houses, aes(x=sqft_living, y=price)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", se=FALSE) +\n  ggtitle(\"Price and Living Space\") + \n  ylab(\"Price\") + \n  xlab(\"Living Space in sq. ft. \")\n\n\n\n\n\n\n\n\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = b_0 + b_1\\times\\text{Sq.Ft.}\n\\]\nNote, the symbol over the response variable (Price) is read as “hat”, and means “predicted price”.\nWe fit the model in R, using the lm (linear model) command. The output gives the estimates of \\(b_0\\) and \\(b_1\\).\n\nM_House_sqft &lt;- lm(data=Houses, price~sqft_living)\nM_House_sqft\n\n\nCall:\nlm(formula = price ~ sqft_living, data = Houses)\n\nCoefficients:\n(Intercept)  sqft_living  \n  -484.9575       0.5328  \n\n\nThe estimates are \\(b_0=-484.9575\\) and \\(b_1=0.5328\\).\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = -484.9575 + 0.5328\\times\\text{Sq.Ft.}\n\\]\nInterpretations\nThe intercept \\(b_0\\) represents the expected (or average) value of the response variable, when the explanatory variable is equal to 0. This is not always a meaningful interpretation in context.\nThe slope \\(b_1\\) represents the expected (or average) change in the response variable for each one-unit increase in the explanatory variable.\n\nOn average, a house with 0 square feet is expected to cost -485 thousand dollars. This is not a sensible interpretation, as there are no houses with 0 square feet.\nFor each additional square foot in living space, the price of the house is expected to increase by 0.5328 thousand dollars (or $533).\n\n\nSince a 1 square ft. increase is very small, it makes more sense to give the interpretation in terms of a 100-square foot increase. For each additional 100 square feet in living space, the price of the house is expected to increase by 53.28 thousand dollars.\n\n\nPrediction\nWe can predict the price of a house with a given number of square feet by plugging the square feet into the model equation.\nThe predicted price of a house with 1,500 square feet is\n\\[\n\\widehat{\\text{Price}} = -484.9575 + 0.5328\\times 1500 = \\$314{ \\text{ thousand}}\n\\]\nWe can calculate this directly in R using the predict command.\n\npredict(M_House_sqft, newdata=data.frame(sqft_living=1500))\n\n       1 \n314.1803 \n\n\nWe should only try to make predictions on houses within the range of the observed data. Since the largest house in the dataset is 8,000 square feet we should not try to predict the price of house with 10,000 square feet.\n\n\n2.1.3 Categorical Explanatory Variable\nNext, we’ll predict the price of a house based on whether or not it is on the waterfront.\nThe boxplot shows the distribution of prices for waterfront and nonwaterfront houses. The red dots indicate the mean.\n\nggplot(data=Houses, aes(x=waterfront, y=price)) + geom_boxplot() + \n  ggtitle(\"House Price by Waterfront Status\") + \n  xlab(\"Waterfront\") + ylab(\"Price\") + coord_flip() + \n  stat_summary(fun.y=mean, geom=\"point\", shape=20, color=\"red\", fill=\"red\")\n\n\n\n\n\n\n\n\nThe table displays the price summary by waterfront status.\n\nHouses_Grouped_Summary &lt;- Houses %&gt;% group_by(waterfront) %&gt;% \n                                      summarize(Mean_Price = mean(price, na.rm=TRUE),\n                                                Median_Price = median(price, na.rm=TRUE), \n                                                StDev_Price = sd(price, na.rm = TRUE),\n                                                Number_of_Houses = n()) \nkable(Houses_Grouped_Summary)\n\n\n\n\nwaterfront\nMean_Price\nMedian_Price\nStDev_Price\nNumber_of_Houses\n\n\n\n\nNo\n523.7595\n450\n295.7991\n85\n\n\nYes\n1934.3800\n1350\n1610.7959\n15\n\n\n\n\n\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = b_0 + b_1\\times\\text{Waterfront}\n\\]\nThe waterfront variable takes on value of 1 if the house is on the waterfront, and 0 otherwise.\n\nM_House_wf &lt;- lm(data=Houses, price~waterfront)\nM_House_wf\n\n\nCall:\nlm(formula = price ~ waterfront, data = Houses)\n\nCoefficients:\n  (Intercept)  waterfrontYes  \n        523.8         1410.6  \n\n\nThe estimates are \\(b_0=523.8\\) and \\(b_1=1410.6\\).\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = 523.8 + 1410.6\\times \\text{Waterfront}\n\\]\nInterpretations\nThe intercept \\(b_0\\) represents the expected (or average) value of the response variable in the “baseline” category (in this case non-waterfront).\nThe coefficient \\(b_1\\) represents the expected (or average) difference in response between the a category and the “baseline” category.\n\nOn average, a house that is not on the waterfront is expected to cost 523.8 thousand dollars.\nOn average a house that is on the waterfront is expected to cost 1410.6 thousand (or 1.4 million) dollars more than a house that is not on the waterfront.\n\nPrediction\nWe can predict the price of a house with a given number of square feet by plugging in either 1 or 0 for the waterfront variable.\nThe predicted price of a house on the waterfront is:\n\\[\n\\widehat{\\text{Price}} = 523.8 + 1410.6\\times 1 = \\$1934.6{ \\text{ thousand (or 1.9 million)}}\n\\]\nThe predicted price of a house not on the waterfront is:\n\\[\n\\widehat{\\text{Price}} = 523.8 + 1410.6\\times 0 = \\$523.8{ \\text{ thousand}}\n\\]\nCalculations in R:\n\npredict(M_House_wf, newdata=data.frame(waterfront=\"Yes\"))\n\n      1 \n1934.38 \n\n\n\npredict(M_House_wf, newdata=data.frame(waterfront=\"No\"))\n\n       1 \n523.7595 \n\n\nNotice that the predicted prices for each category correspond to the average price for that category.\n\n\n2.1.4 Multiple Explanatory Variables\nWe’ve used square feet and waterfront status as explanatory variables individually. We can also build a model that uses both of these variables at the same time.\nA model with two or more explanatory variables is called a multiple regression model.\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = b_0 + b_1\\times\\text{Sq. Ft} + b_2\\times\\text{Waterfront}\n\\]\nFor a house not on the waterfront, \\(b_2=0\\), so the model equation is:\n\\[\n\\widehat{\\text{Price}} = b_0  + b_1\\text{Sq. Ft}\n\\]\nFor a house on the waterfront, \\(b_2=1\\), so the model equation is:\n\\[\n\\widehat{\\text{Price}} = (b_0 + b_2) + b_1\\times\\text{Sq. Ft}\n\\]\nNotice that the slope is the same, regardless of whether the house is on the waterfront (\\(b_1\\)). The intercept, however, is different (\\(b_0\\) for houses not on the waterfront, and \\(b_0 + b_2\\) for houses on the waterfront). Thus, the model assumes that price increases at the same rate, with respect to square feet, regardless of whether or not it is on the waterfront, but allows the predicted price for a waterfront house to differ from a non-waterfront house of the same size.\n\n\n\n\n\n\n\n\n\nWe fit the model in R.\n\nM_wf_sqft &lt;- lm(data=Houses, price~sqft_living+waterfront)\nM_wf_sqft\n\n\nCall:\nlm(formula = price ~ sqft_living + waterfront, data = Houses)\n\nCoefficients:\n  (Intercept)    sqft_living  waterfrontYes  \n    -407.6549         0.4457       814.3613  \n\n\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = -407.7 + 0.4457\\times\\text{Sq. Ft} + 814.36\\times\\text{Waterfront}\n\\]\nInterpretations\nThe intercept \\(b_0\\) represents the expected (or average) value of the response variable, when all quantitative explanatory variables are equal to 0, and all categorical variables are in the “baseline” category. This interpretion is not always sensible.\nWe interpret coefficients \\(b_j\\) for categorical or quantitative variables, the same way we would in a regression model with only one variable, but we need to state that all other explanatory variables are being held constant.\n\nOn average, a house that is not on the waterfront with 0 square feet is expected to cost -407.7 thousand dollars. This is not a sensible interpretation, since there are no houses with 0 square feet.\n\nFor each 1-square foot increase in size, the price of a house is expected to increase by 0.4457 thousand (or 446 hundred) dollars, assuming waterfront status is the same. Equivalently, for each 100-square foot increase in size, the price of a house is expected to increase by 44.57 thousand dollars, assuming waterfront status is the same.\nOn average, a house on the waterfront is expected to cost 814 thousand dollars more than a house that is not on the waterfront, assuming square footage is the same.\n\nPrediction\nThe predicted price of a 1,500 square foot house on the waterfront is:\n\\[\n\\widehat{\\text{Price}} = -407.7 + 0.4457\\times1500 + 814.36\\times1 = \\$1075{ \\text{ thousand (or 1.075 million)}}\n\\]\nThe predicted price of a 1,500 square foot not on the waterfront is:\n\\[\n\\widehat{\\text{Price}} = -407.7 + 0.4457\\times1500 = \\$260.9{ \\text{ thousand}}\n\\]\nCalculations in R:\n\npredict(M_wf_sqft, newdata=data.frame(waterfront=\"Yes\", sqft_living=1500))\n\n       1 \n1075.227 \n\n\n\npredict(M_wf_sqft, newdata=data.frame(waterfront=\"No\", sqft_living=1500))\n\n       1 \n260.8657 \n\n\n\n\n2.1.5 No Explanatory Variable\nFinally, we’ll consider a model that makes use of no explanatory variables at all. Although this might seem silly, its relevance will be seen in the next section.\nThe histogram shows the distribution of prices, without any information about explanatory variables. The mean price is indicated in red.\n\nggplot(data=Houses, aes(x=price)) + \n  geom_histogram(fill=\"lightblue\", color=\"white\") + \n  ggtitle(\"Distribution of House Prices\") + xlab(\"Price\") + ylab(\"Frequency\") + \n  geom_point(aes(x=mean(Houses$price), y=0), color=\"red\", shape=24, fill=\"red\")\n\n\n\n\n\n\n\n\nThe mean, median, and standard deviation in prices is shown below.\n\nlibrary(knitr)\nkable(Houses_Summary)\n\n\n\n\nMean_Price\nMedian_Price\nStDev_Price\nNumber_of_Houses\n\n\n\n\n735.3525\n507.5\n835.1231\n100\n\n\n\n\n\nSuppose we know that a house sold in King County during this time, and want to predict the price, without knowing anything else about the house.\nThe best we can do is to use the mean price for our prediction. (We’ll define what we mean by “best” later in the chapter.)\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = b_0\n\\]\nWe fit a statistical model in R using the lm command.\n\n# syntax for lm command\n# lm(data=DatasetName, ResponseVariable~ExplanatoryVariable(s))\n\nM0_House &lt;- lm(data=Houses, price ~ 1) # when there are no explanatory variables, use ~1\nM0_House\n\n\nCall:\nlm(formula = price ~ 1, data = Houses)\n\nCoefficients:\n(Intercept)  \n      735.4  \n\n\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = 735.4\n\\]\nInterpretation\nThe expected price of a house in King County is 735.4 thousand dollars.\nPredictions\nWithout knowing anything about any explanatory variables, we would predict the price of any house sold in King County, WA to cost 735.4 thousand dollars.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch2.html#variability-explained-by-a-model",
    "href": "Ch2.html#variability-explained-by-a-model",
    "title": "2  Introduction to Statistical Models",
    "section": "2.2 Variability Explained by a Model",
    "text": "2.2 Variability Explained by a Model\nWe’ve seen four different models for predicting house price. It would be nice to have a way to assess how well the models are predicting prices, and determine which model appears to be the best.\nOf course we won’t know the price of the house we are trying to predict, so we can’t be sure how close or far our prediction is. We do, however, know the prices of the original 100 houses in our dataset. We can assess the models by measuring how far the actual prices of the 100 houses differ from the predicted (mean) price, and by calculating the proportion of total variation in sale price explained by each model.\n\n2.2.1 Total Variability\nLet’s start with our most basic model, which uses no explanatory variables and predicts the price of each simply using the average of all houses in the dataset.\nWe measure the total variability in the response variable by calculating the square difference between each individual response value and the overall average. This quantity is called the total sum of squares (SST).\n\\[\n\\text{SST} = \\displaystyle\\sum_{i=1}^n (y_i - \\bar{y})^2\n\\]\nThe plot below shows a horizontal line at the mean sale price (785 thousand). The points represent prices of individual houses, and the red lines represent the differences between the price of each house and the overall average.\n\n\n\n\n\n\n\n\n\nThe first three houses in the dataset are shown below.\n\nFirst3Houses &lt;- Houses %&gt;% select(Id, price, waterfront, sqft_living) %&gt;% head(3)\nkable(First3Houses)\n\n\n\n\nId\nprice\nwaterfront\nsqft_living\n\n\n\n\n1\n1225\nNo\n5420\n\n\n2\n885\nNo\n2830\n\n\n3\n385\nNo\n1620\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\text{SST} & = \\displaystyle\\sum_{i=1}^{100} (y_i - \\bar{y})^2 \\\\\n& = (1225-785)^2 + (885-785)^2 + (385-785)^2 + \\ldots\n\\end{aligned}\n\\]\nWe could calculate SST by hand for small datasets. For larger datasets, we’ll use R to perform the calculation.\n\nmeanprice &lt;- mean(Houses$price)  #calculate mean price\nSST &lt;- sum((Houses$price - meanprice)^2)  ## calculate SST\nSST\n\n[1] 69045634\n\n\nBy itself, the size of SST does not have much meaning. We cannot say whether a SST value like the one we see here is large or small, since it depends on the size and scale of the variable being measured. An SST value that is very large in one context might be very small in another.\nSST does, however, give us a baseline measure of the total variability in the response variable. We’ll assess the performance of a model with a given explanatory variable by measuring how much of this variability the model accounts for.\n\n\n2.2.2 Residuals\nNow let’s consider our model that uses the size of the house in square feet as the explanatory variable. The figure on the left shows difference between actual and predicted prices, using this linear model. We compare the size of the differences to those resulting from the basic model that does not use any explanatory variables, and predicts each price using the overall average (shown on the right).\n\nResidplot_sqft &lt;- ggplot(data=Houses, aes(x = sqft_living, y = price)) +  geom_point() +\n                geom_segment(aes(xend = sqft_living, yend = M_House_sqft$fitted.values), color=\"red\") +\n                geom_point(aes(y = M_House_sqft$fitted.values), shape = 1) +\n                stat_smooth(method=\"lm\", se=FALSE) + ylim(c(0,5500)) +\n                theme_bw() \n\n\n\n\n\n\n\n\n\n\nNotice that the red lines are shorter in the figure on the left, indicating the predictions are closer to the actual values.\nThe difference between the actual and predicted values is called the residual. The residual for the \\(ith\\) case is\n\\[\nr_i = (y_i-\\hat{y}_i)\n\\]\nWe’ll calculate the residuals for the first three houses in the dataset, shown below.\n\nkable(First3Houses)\n\n\n\n\nId\nprice\nwaterfront\nsqft_living\n\n\n\n\n1\n1225\nNo\n5420\n\n\n2\n885\nNo\n2830\n\n\n3\n385\nNo\n1620\n\n\n\n\n\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = -484.9575 + 0.5328\\times \\text{Sq. Ft}\n\\]\nThe predicted prices for these three houses are:\n\\[\n\\widehat{\\text{Price}_1} = -484.9575 + 0.5328\\times 5420 = 2402.6 \\text{ thousand dollars}\n\\]\n\\[\n\\widehat{\\text{Price}_2} = -484.9575 + 0.5328\\times 2830 = 1022.7 \\text{ thousand dollars}\n\\]\n\\[\n\\widehat{\\text{Price}_3} = -484.9575 + 0.5328\\times 1620 = 378.1 \\text{ thousand dollars}\n\\]\nTo calculate the residuals, we subtract the predicted price from the actual price.\n\\[r_1 = y_1-\\hat{y}_1 = 1225 - 2402.6 = -1177.6 \\text{ thousand dollars}\\]\n\\[r_2 = y_2-\\hat{y}_2 = 885 - 1022.7 = -137.7 \\text{ thousand dollars}\\]\n\\[r_2 = y_2-\\hat{y}_2 = 385 - 378.1 = 6.9 \\text{ thousand dollars}\\]\nThe fact that the first two residuals are negative indicates that these houses sold for less than the model predicts.\nThe predicted values and residuals from a model can be calculated automatically in R. The predicted values and residuals for the first 5 houses are shown below.\n\nPredicted &lt;- predict(M_House_sqft)\nhead(Predicted, 3)\n\n        1         2         3 \n2402.5937 1022.7491  378.1113 \n\n\n\nResidual &lt;- M_House_sqft$residuals\nhead(Residual, 3)\n\n           1            2            3 \n-1177.593665  -137.749128     6.888668 \n\n\n\n\n2.2.3 Variability Explained by Sq. Ft. Model\nThe sum of squared residuals (SSR) measures the amount of unexplained variability in the response variable after accounting for all explanatory variables in the model.\n\\[\n\\text{SSR} = \\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2.  \n\\]\nNote that SSR is similar to SST, except we subtract the model’s predicted values, rather than the overall average. In the special case of a model with no explanatory variables, the predicted values are equal to the overall average, so SSR is equal to SST.\nWe calculate SSR for the model using square feet as the explanatory variable.\n\\[\n\\begin{aligned}\n\\text{SSR} & = \\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2.  \\\\\n& = (1225 - 2402.6)^2 + (885 - 1022.7)^2 + (385 - 378.1)^2 + \\ldots\n\\end{aligned}\n\\]\nWe can calculate the model’s SSR directly in R.\n\nSSR_sqft &lt;- sum(M_House_sqft$residuals^2)\nSSR_sqft\n\n[1] 23767280\n\n\nSSR represents the amount of total variability in saleprice remaining after accounting for the house’s size in square feet.\nThe SSR=23,767,290 value is about one third of the SST value of 69,045,634. This means that about 2/3 of the total variability in sale price is explained by the model that accounts for sale price.\nThe difference (SST-SSR) represents the variability in the response variable that is explained by the model. This quantity is called the model sum of squares (SSM).\n\\[ \\text{SSM} = \\text{SST} - \\text{SSR} \\]\nIt can be shown that \\(\\text{SSM}=\\displaystyle\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2\\).\nThe proportion of total variability in the response variable explained by the model is called the coefficient of determination, denoted \\(R^2\\). We calculate this by dividing SSM by SST.\n\\[\nR^2=\\frac{SSM}{SST}= \\frac{SST-SSR}{SST}\n\\]\nExample: For the model with square feet as the explanatory variable,\n\\[\nSSM = SST-SSR = 69,045,634 - 23,767,290 =45,278,344.\n\\]\n\\[\nR^2 = \\frac{45,278,344}{69,045,634}=0.6557.\n\\]\nApproximately 65.6% of the total variability in sale price is explained by the model using square feet as the explanatory variable.\n\n\n\n\n\n\n\n\n\nWe calculate \\(R^2\\) directly in R.\n\nsummary(M_House_sqft)$r.squared\n\n[1] 0.6557743\n\n\n\n\n2.2.4 Linear Correlation Coefficient\nFor models with a single quantiative explanatory varible, the coefficient of determination is equal to the square of the correlation coefficient \\(r\\), discussed in Chapter 1.\n\nggplot(data=Houses, aes(x=sqft_living, y=price)) + geom_point() + \n  stat_smooth(method=\"lm\", se=FALSE)\n\n\n\n\n\n\n\n\n\nFor linear models with a single quantitative variable, the linear correlation coefficient \\(r=\\sqrt{R^2}\\), or \\(r=-\\sqrt{R^2}\\) (with sign matching the sign on the slope of the line), provides information about the strength and direction of the linear relationship between the variables.\n\\(-1 \\leq r \\leq 1\\), and \\(r\\) close to \\(\\pm1\\) provides evidence of strong linear relationship, while \\(r\\) close to 0 suggests linear relationship is weak.\n\n\ncor(Houses$price,Houses$sqft_living)\n\n[1] 0.8097989\n\n\n\n\\(r\\) is only relevant for models with a single quantitative explanatory variable and a quantitative response variable, while \\(R^2\\) is relevant for any linear model with a quantitative response variable.\n\n\n\n2.2.5 Variability Explained by Waterfront Model\nWe can similarly calculate the proportion of variability explained by the model using waterfront as an explanatory variable.\nRecall that in this model, the predicted price of a house with a waterfront is given by the average price of all waterfront houses, and the predicted price of a non-waterfront house is given by the average price of all non-waterfront houses.\nWe can calculate residuals using these predicted values, and compare them to the residuals resulting from a model with no explanatory variables, which uses the overall average price for all predictions.\nThe left two figures show the residuals resulting from a model that accounts for waterfront status. The figure on the right shows the residuals resulting from the model with no explanatory variables.\n\ngrid.arrange(arrangeGrob(M1aResid,M1bResid, Residplot_M0 + ggtitle(\"Model with no Exp. Vars\"), ncol=3, nrow=1, widths=c(3, 2,5))) \n\n\n\n\n\n\n\n\nNotice that after accounting for waterfront status, the differences between observed and predicted values are bigger than they were in the model that accounted for square feet, though not as big as for the model that doesn’t use any explanatory variables.\nWe use R to calculate SSR for the waterfront model.\n\nSSR_wf &lt;- sum(M_House_wf$residuals^2)\nSSR_wf\n\n[1] 43675043\n\n\n\\[\nSSM = SST-SSR = 69,045,634 - 43,675,043 =25,370,591.\n\\]\n\\[\nR^2 = \\frac{25,370,591}{69,045,634}=0.3674.\n\\]\nApproximately 36.7% of the total variability in sale price is explained by the model using waterfront status as the explanatory variable.\n\n\n\n\n\n\n\n\n\nWe calculate \\(R^2\\) directly in R.\n\nsummary(M_House_wf)$r.squared\n\n[1] 0.3674467\n\n\n\n\n2.2.6 Variability Explained by Multiple Regression Model\nWe’ve seen at the model using square feet accounts for about 2/3 of the total variability in house prices, while the model using waterfront status accounts for about 1/3 of the total variability. Let’s see if we can do better by using both variables together.\nThe left figure shows the residuals resulting from a model that accounts for both waterfront status and square feet. The figure on the right shows the residuals resulting from the model with no explanatory variables.\n\ngrid.arrange(Residplot_MR + ggtitle(\"Multiple Regression Model\") , Residplot_M0 + ggtitle(\"Model with no Exp. Vars\"), ncol=2)\n\n\n\n\n\n\n\n\nWe use R to calculate SSR for the waterfront model.\n\nSSR_wf_sqft &lt;- sum(M_wf_sqft$residuals^2)\nSSR_wf_sqft\n\n[1] 16521296\n\n\n\\[\nSSM = SST-SSR = 69,045,634 - 16,521,296 =52,524,338.\n\\]\n\\[\nR^2 = \\frac{52,524,338}{69,045,634}=0.761.\n\\]\nApproximately 76.1% of the total variability in sale price is explained by the model using square feet and waterfront status as the explanatory variables.\n\n\n\n\n\n\n\n\n\nWe calculate \\(R^2\\) directly in R.\n\nsummary(M_wf_sqft)$r.squared\n\n[1] 0.7607192\n\n\nIncluding both square feet and waterfront status allows us to explain more variability in sale price than models that include one but not both of these variables.\n\n\n2.2.7 Summary: SST, SSR, SSM, \\(R^2\\)\n\nthe total variability in the response variable is the sum of the squared differences between the observed values and the overall average.\n\n\\[\\text{Total Variability in Response Var.}= \\text{SST} =\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y})^2\\]\n\nthe variability remaining unexplained even after accounting for explanatory variable(s) in a model is given by the sum of squared residuals. We abbreviate this SSR, for sum of squared residuals.\n\n\\[\n\\text{SSR} = \\text{Variability Remaining}=\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n\\]\n\nthe variability explained by the model, abbreviated SSM, is given by\n\n\\[ \\text{SSM} = \\text{SST} - \\text{SSR} \\]\n\nThe coefficient of determination (abbreviated \\(R^2\\)) is defined as\n\n\\[R^2=\\frac{\\text{Variability Explained by Model}}{\\text{Total Variability}}=\\frac{\\text{SSM}}{\\text{SST}} =\\frac{\\displaystyle\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2}{\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y})^2}\\]\nNote that some texts use different abbreviations than the ones used here. When working with resources outside this class, be sure to carefully check the notation being used.\nFor the model with a single quantitative explanatory variable.\n\n\n\n\n\n\n\n\n\nModel with a single categorical explanatory variable with 3 categories:\n\n\n\n\n\n\n\n\n\n\nBlue Area = Total Variability (SST)\nRed Area = Variability Remaining Unexplained by Model (SSR)\nBlue Area - Red Area = Variability Explained by Model (SSM)\n\\(R^2 = \\frac{\\text{Area of Blue Squares} - \\text{Area of Red Squares}}{\\text{Area of Blue Squares}} = \\frac{\\text{SST}-\\text{SSR}}{\\text{SST}}= \\frac{\\text{SSM}}{\\text{SST}}\\)\n\n\n\n2.2.8 Model Comparison Summary\n\n\n\n\n\n\n\n\n\n\nModel\nVariables\nUnexplained Variability\nVariability Explained\n\\(R^2\\)\n\n\n\n\n0\nNone\n69045634.1341747\n0\n0\n\n\n1\nSq. Ft.\n23767280.3817707\n45278353.752404\n0.6557743\n\n\n2\nWaterfront\n43675043.0897012\n25370591.0444735\n0.3674467\n\n\n3\nSq. Ft. and Waterfront\n16521296.4889025\n52524337.6452723\n0.7607192\n\n\n\nComments on \\(R^2\\):\n\n\\(R^2\\) will never decrease when a new variable is added to a model.\n\nThis does not mean that adding more variables to a model always improves its ability to make predictions on new data.\n\n\\(R^2\\) measures how well a model fits the data on which it was built.\n\nIt is possible for a model with high \\(R^2\\) to “overfit” the data it was built from, and thus perform poorly on new data. We will discuss this idea extensively later in the course.\nOn some datasets, there is a lot of “natural” variability in the response variable, and no model will achieve a high \\(R^2\\). That’s okay. Even a model with \\(R^2 = 0.10\\) or less can provide useful information.\nThe goal is not to achieve a model that makes perfect predictions, but rather to be able to quantify the amount of uncertainty associated with the predictions we make.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch2.html#models-with-interaction",
    "href": "Ch2.html#models-with-interaction",
    "title": "2  Introduction to Statistical Models",
    "section": "2.3 Models with Interaction",
    "text": "2.3 Models with Interaction\n\n2.3.1 Definition of Interaction\nWe previously used a multiple regression model of the form\n\\[\n\\widehat{Price} = b_0 + b_1\\times\\text{SqFt} + b_2\\times\\text{Waterfront}\n\\]\nRecall that this model assumes the slope relating price and square footage is the same (\\(b_1\\)) for houses on the waterfront as for houses not on the waterfront. An illustration of the model is shown below.\n\nPM3\n\n\n\n\n\n\n\n\nThis assumption of the rate of change in price with respect to living space being the same for waterfront houses, as for non-waterfront houses might be unrealistic.\nLet’s fit separate lines waterfront and non-waterfront houses, without requiring them to have the same slope.\n\nggplot(data=Houses, aes(x=sqft_living, y=price, color=waterfront)) + geom_point()+stat_smooth(method=\"lm\", se=FALSE) + ylim(c(0,5500)) + theme_bw()\n\n\n\n\n\n\n\n\nIt appears that the prices of the houses on the waterfront are increasing more rapidly, with respect to square feet of living space, than the non-waterfront houses. The effect of additional square feet on the price of the house appears to depend on whether or not the house is on the waterfront. This is an example of an interaction between square footage and waterfront status.\nAn interaction between two explanatory variables occurs when the effect of one explanatory variable on the response depends on the other explanatory variable.\n\n\n2.3.2 Interaction Term\nIf we want to allow for different slopes between waterfront and non-waterfront houses, we’ll need to change the mathematical equation of our model. To do that, we’ll add a coefficient \\(b_3\\), multiplied by the product of our two explanatory variables.\nThe model equation is\n\\[\n\\widehat{Price} = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{waterfront} + b_3\\times\\text{Sq.Ft}\\times\\text{Waterfront}\n\\]\nThe last term is called an interaction term.\nFor a house on the waterfront (\\(\\text{waterfront}=1\\)), the equation relating price to square feet is\n\\[\n\\begin{aligned}\n\\widehat{Price} & = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{1} + b_3\\times\\text{Sq.Ft}\\times\\text{1} \\\\\n& = (b_0+b_2) + (b_1+b_3)\\times{\\text{Sq. Ft.}}\n\\end{aligned}\n\\] For a house not on the waterfront (\\(\\text{waterfront}=0\\)), the equation relating price to square feet is\n\\[\n\\begin{aligned}\n\\widehat{Price} & = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{0} + b_3\\times\\text{Sq.Ft}\\times\\text{0} \\\\\n& = b_0 + b_1\\times{\\text{Sq. Ft}}\n\\end{aligned}\n\\]\nThe intercept is \\(b_0\\) for non-waterfront houses, and \\(b_0 + b_2\\) for waterfront houses.\nThe slope is \\(b_1\\) for non-waterfront houses, and \\(b_1 + b_3\\) for waterfront houses.\nThus, the model allows both the slope and intercept to differ between waterfront and non-waterfront houses.\n\n\n2.3.3 Interaction Models in R\nTo fit an interaction model in R, use * instead of +\n\nM_House_Int &lt;- lm(data=Houses, price~sqft_living*waterfront)\nM_House_Int\n\n\nCall:\nlm(formula = price ~ sqft_living * waterfront, data = Houses)\n\nCoefficients:\n              (Intercept)                sqft_living  \n                  67.3959                     0.2184  \n            waterfrontYes  sqft_living:waterfrontYes  \n                -364.5950                     0.4327  \n\n\nThe regression equation is\n\\[\n\\widehat{Price} = 67.4 + 0.2184\\times\\text{Sq. Ft.}  -364.6\\times\\text{waterfront} + 0.4327\\times\\text{Sq.Ft}\\times\\text{Waterfront}\n\\]\nFor a house on the waterfront (\\(\\text{waterfront}=1\\)), the equation is\n\\[\n\\begin{aligned}\n\\widehat{Price} & = 67.4 + 0.2184\\times\\text{Sq. Ft.} -364.6 \\times\\text{1} + 0.4327\\times\\text{Sq.Ft}\\times\\text{1} \\\\\n& = (67.4 - 364.6) + (0.2184+0.4327)\\times{\\text{Sq. Ft.}} \\\\\n& = -297.2 + 0.6511\\times{\\text{Sq. Ft.}}\n\\end{aligned}\n\\] For a house not on the waterfront (\\(\\text{waterfront}=0\\)), the equation is\n\\[\n\\begin{aligned}\n\\widehat{Price} & = 67.4 + 0.2184\\times\\text{Sq. Ft.} -364.6 \\times\\text{0} + 0.4327\\times\\text{Sq.Ft}\\times\\text{0} \\\\\n& = 67.4 0 + 0.2184\\times{\\text{Sq. Ft.}}\n\\end{aligned}\n\\] Interpretation\nWhen interpreting \\(b_0\\) and \\(b_1\\), we need to state that the interpretations apply only to the “baseline” category (in this case non-waterfront houses).\nIn a model with interaction, it does not make sense to talk about holding one variable constant when interpreting the effect of the other, since the effect of one variable depends on the value or category of the other. Instead, we must state the value or category of one variable when interpreting the effect of the other.\nInterpretations:\n\n\\(b_0\\) - On average, a house with 0 square feet that is not on the waterfront is expected to cost 67 thousand dollars. This is not a sensible interpretation since there are no houses with 0 square feet.\n\\(b_1\\) - For each additional square foot in size, the price of a non-waterfront house is expected to increase by 0.2184 thousand dollars.\n\\(b_2\\) - On average, the price of a waterfront house with 0 square feet is expected to be 364.6 thousand dollars less than the price of a non-waterfront house with 0 square feet. This is not a sensible interpretation in this case.\n\\(b_3\\) - For each additional square foot in size, the price of a waterfront house is expected to increase by 0.4327 thousand dollars more than a non-waterfront house.\n\nAlternatively, we could interpret \\(b_0+b_2\\) and \\(b_1+b_3\\) together.\n\n\\(b_0 + b_2\\) - On average, a house with 0 square feet that is on the waterfront is expected to cost -297.2 thousand dollars. This is not a sensible interpretation since there are no houses with 0 square feet.\n\\(b_1\\) - For each additional square foot in size, the price of a waterfront house is expected to increase by 0.6511 thousand dollars.\n\nPrediction\nWe calculate predicted prices for the following houses:\n\nHouses[c(1,16), ] %&gt;% select(Id, price, sqft_living, waterfront)\n\n# A tibble: 2 × 4\n     Id price sqft_living waterfront\n  &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n1     1  1225        5420 No        \n2    16  3075        4550 Yes       \n\n\n\\[\n\\widehat{Price}_1 = 67.4 + 0.2184\\times5420  -364.6\\times0 + 0.4327\\times5420 \\times 0 = 1191 \\text{ thousand dollars}\n\\]\n\\[\n\\widehat{Price}_{16} = 67.4 + 0.2184\\times4450  -364.6\\times1 + 0.4327\\times4450 \\times 1 = 2600 \\text{ thousand dollars}\n\\]\n\n\n2.3.4 \\(R^2\\) for Interaction Model\nWe can calculate residuals, as well as SSR, SSM, SST, and \\(R^2\\), in the same manner we’ve seen previously.\nWe’ll perform these calculations using R.\n\nSSR_int &lt;- sum(M_House_Int$residuals^2)\nSSR_int\n\n[1] 10139974\n\n\n\\[\nSSM = SST-SSR = 69,045,634 - 10,139,974 =58,905,660.\n\\]\n\\[\nR^2 = \\frac{58,905,660}{69,045,634}=0.8531.\n\\]\nApproximately 85.3% of the total variability in sale price is explained by the model using square feet and waterfront status, as well as an interaction between them as the explanatory variables.\n\n\n\n\n\n\n\n\n\nWe calculate \\(R^2\\) directly in R.\n\nsummary(M_House_Int)$r.squared\n\n[1] 0.853141\n\n\nWe see that adding an interaction term improved the proportion of variability in house price explained by the model from 0.76 to 0.85. This is a fairly notable increase.\n\n\n2.3.5 Considerations for Using Interactions\nIt might be tempting to think we should always add an interaction term to a model when using two or more explanatory variables. After all, an interaction term is just another term added to the model, meaning that \\(R^2\\) will never go down.\nAdding an interaction term is not always a good idea, though. We saw that doing so makes interpretations more complicated. Increasing the complexity of a model also increases the risk of overfitting, potentially hurting predictive performance on new data.\nWe should only add an interaction term if we have strong reason to believe that the rate of change in the response variable with respect to one explanatory variable really does depend on the other variable. This might come from background knowledge about the subject, or consultation with an expert in the area. It could also come from data visualization, and the increase in variability in the response variable explained when an interaction term is added to the model.\nIn the house price dataset, we might expect that the price of waterfront houses might increase more rapidly as they get bigger than the price of non-waterfront houses. The fact that the lines shown in the scatterplot are not close to being parallel provides further evidence of a difference in rate of increase, providing justification for the use of an interaction term in the model. Furthermore, \\(R^2\\) increases notably (from 0.76 to 0.85), when an interaction term is added. All of these reasons support using an interaction term in this context.\nWhen examining a scatterplot, we should note that even if there is truly no interaction among all houses, the lines probably won’t be exactly parallel, due to random deviations among the sample of houses chosen. If the lines are reasonably close to parallel, then an interaction term is likely not needed.\nWe’ll look more at criteria for determining whether to add an interaction term to a model in the coming sections.\n\n\n2.3.6 Interaction vs Correlation\nIt is easy to confuse the concept of interaction with that of correlation. These are, in fact, very different concepts.\nA correlation between two variables means that as one increases, the other is more likely to increase or decrease. We only use the word correlation to describe two quantitative variables, but we could discuss the similar notion of a relationship between categorical variables.\nAn interaction between two explanatory variables means that the effect of one on the response depends on the other.\nExamples of Correlations (or relationships)\n\nHouses on the waterfront tend to be bigger than houses not on the waterfront, so there is a relationship between square feet and waterfront status.\nHouses with large amounts of living space in square feet are likely to have more bedrooms, so there is a correlation between living space and bedrooms.\nSuppose that some genres of movies (drama, comedy, action, etc.) tend to be longer than others. This is an example of a relationship between genre and length.\n\nThe fact that there is a correlation between explanatory variables is NOT a reason to add an interaction term involving those variables in a model. Correlation is something entirely different than interaction!\nExamples of Interactions\n\nAs houses on the waterfront increase in size, their price increases more rapidly than for houses not on the waterfront. This means there is an interaction between size and waterfront location.\nSuppose that the effect of additional bedrooms on price is different for houses with lots of living space than for houses with little living space. This would be an example of an interaction between living space and number of bedrooms.\nSuppose that audiences become more favorable to dramas as they get longer, but less favorable to comedies as they get longer. In this scenario, the effect of movie length on audience rating depends on the genre of the movie, indicating an interaction between length and genre.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch2.html#least-squares-estimation-lse",
    "href": "Ch2.html#least-squares-estimation-lse",
    "title": "2  Introduction to Statistical Models",
    "section": "2.4 Least Squares Estimation (LSE)",
    "text": "2.4 Least Squares Estimation (LSE)\n\n2.4.1 Estimating Regression Coefficients\nWe’ve already used R to determine the estimates of \\(b_0\\), \\(b_1\\), \\(b_2\\), and \\(b_3\\) in various kinds of linear models. At this point, it is natural to wonder where these estimates are come from.\nRegression coefficients \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that minimizes the sum of the squared differences between the observed and predicted values. That is, we minimize\n\\[\n\\text{SSR} = \\displaystyle\\sum_{i=1}^{n} (y_i-\\hat{y}_i)^2\n\\]\nBecause \\(\\hat{y}_i\\) is a function of \\(b_0, b_1, \\ldots, b_p\\), we can choose the values of \\(b_0, b_1, \\ldots, b_p\\) in a way that minimizes SSR.\n\\[\\text{SSR} = \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  = \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 \\]\nThe process of estimating regression coefficients \\(b_0, b_1, \\ldots, b_p\\) in a way that minimizes SSR is called least-squares estimation.\nExample: Model with one quantitative variable\nWe start with an example of estimating the regression coefficients for a model with a single explanatory variable. This is easy to illustrate, since we can draw a scatter plot displaying our explanatory and response variable.\nThe figure below illustrates four possible trend lines that could be fit to a set of 10 points in a scatter plot. The first line is the line of best fit, in that it makes the sum of the squared residuals the smallest of all possible lines that could be drawn. The second through fourth plots all show examples of other trend lines that are not the line of best fit. The sum of squared residuals for each of these models is bigger than for the first one.\nIn the illustration, SSR is represented by the total area of the squares. The line of best fit is the one that make the intercept the smallest.\n\n\n\n\n\n\n\n\n\n\nThis Rossman-Chance applet provides an illustration of the line of best fit.\n\nReturning to the model for predicting price of a house, using only size in square feet as an explanatory variable, the scatter plot, along with the slope and intercept of the regression line are shown below.\n\nggplot(data=Houses, aes(x=sqft_living, y=price)) + geom_point() + \n  stat_smooth(method=\"lm\", se=FALSE) + theme_bw()\n\n\n\n\n\n\n\n\n\nM_House_sqft\n\n\nCall:\nlm(formula = price ~ sqft_living, data = Houses)\n\nCoefficients:\n(Intercept)  sqft_living  \n  -484.9575       0.5328  \n\n\nThe line \\(\\text{Price} = -485 + 0.5328 \\times \\text{Square Feet}\\) is the “line of best fit” in the sense that it minimizes the sum of the squared residuals (SSR). Any other choices for the slope or intercept of the regression line would result in larger SSR than this line.\n\n\n2.4.2 Mathematics of LSE for SLR\n\nConsider a simple linear regression(SLR) model, which is one with a singe quantitative explanatory variable \\(x\\).\n\\(\\hat{y}_i = b_0+b_1x_i\\)\nwe need to choose the values of \\(b_0\\) and \\(b_1\\) that minimize:\n\n\\[\n\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n(y_i-(b_0+b_1x_i))^2\n\\]\nWe setup the equation by substituting in the values of \\(y_i\\) and \\(x_i\\) seen in the data.\nRecall the first 3 houses in the dataset:\n\nkable(First3Houses)\n\n\n\n\nId\nprice\nwaterfront\nsqft_living\n\n\n\n\n1\n1225\nNo\n5420\n\n\n2\n885\nNo\n2830\n\n\n3\n385\nNo\n1620\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\displaystyle\\sum_{i=1}^{100}(y_i-\\hat{y}_i)^2 & =\\displaystyle\\sum_{i=1}^n(y_i-(b_0+b_1x_i))^2 \\\\\n& = (1225-(b_0+b_1(5420)))^2 + (885-(b_0+b_1(2830)))^2 + (385-(b_0+b_1(1620)))^2 + \\ldots\n\\end{aligned}\n\\]\nWe need to find the values of \\(b_0\\) and \\(b_1\\) that minimize this expression. This is a 2-dimensional optimization problem that can be solved using multivariable calculus or numerical or graphical methods.\nUsing calculus, it can be shown that this quantity is minimized when\n\n\\(b_1=\\frac{\\displaystyle\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\displaystyle\\sum_{i=1}^{n}(x_i-\\bar{x})^2}=\\frac{\\displaystyle\\sum_{i=1}^{n} x_i y_i-\\frac{\\displaystyle\\sum_{i=1}^{n} x_i \\displaystyle\\sum_{i=1}^{n} y_i }{n}}{\\left(\\displaystyle\\sum_{i=1}^{n} x_i^2 -\\frac{\\left(\\displaystyle\\sum_{i=1}^{n} x_i\\right)^2}{n}\\right)}\\)\n\\(b_0=\\bar{y}-b_1\\bar{x}\\) (where \\(\\bar{y}=\\frac{\\displaystyle\\sum_{i=1}^{n}{y_i}}{n}\\), and \\(\\bar{x}=\\frac{\\displaystyle\\sum_{i=1}^{n}{x_i}}{n}\\)).\n\n\n\n2.4.3 LSE for Categorical Variable\n\nConsider a model with a single categorical variable (such as waterfront), with G+1 categories, numbered \\(g=0,2, \\ldots, G\\)\nThen \\(\\hat{y}_i = b_0 + b_1x_{i1} + \\ldots +b_{G}x_{iG}\\).\nwe need to minimize\n\n\\[\n\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n(y_i-(b_0 + b_1x_{i1} + \\ldots +b_{G}x_{iG}))^2.   \n\\]\n\nIt can be shown that this is achieved when\n\n\\(b_0 = \\bar{y_0}\\) (i.e. the average response in the “baseline group”), and\n\n\\(b_j = \\bar{y_j} - \\bar{y}_0\\)\n\n\n\n\n2.4.4 LSE More Generally\n\nFor multiple regression models, including those involving interaction, the logic is the same. We need to choose \\(b_0, b_1, \\ldots, b_p\\) in order to minimize\n\n\\[ \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  = \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 \\]\n\nThe mathematics, however, are more complicated and require inverting a matrix. This goes beyond the scope of this class, so we will let R do the estimation and use the results.\nMore on least squares estimation in multiple regression can be found here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch2.html#analysis-of-variance",
    "href": "Ch2.html#analysis-of-variance",
    "title": "2  Introduction to Statistical Models",
    "section": "2.5 ANalysis Of VAriance",
    "text": "2.5 ANalysis Of VAriance\n\n2.5.1 Submodels\nWe’ve seen 5 different models for predicting house price using some combination of square feet and waterfront status.\nA model A is defined to be a submodel of another model B, if every term in model A is also included in model B.\n\n\n\n\n\n\n\n\n\n\nModel\nVariables\nUnexplained Variability\nVariability Explained\n\\(R^2\\)\n\n\n\n\n0\nNone\n69045634\n0\n0\n\n\n1\nSq. Ft.\n23767280\n45278354\n0.656\n\n\n2\nWaterfront\n43675043\n25370591\n0.367\n\n\n3\nSq. Ft. and Waterfront\n16521296\n52524338\n0.761\n\n\n4\nSq. Ft., Waterfront, and Interaction\n10139974\n58905661\n0.853\n\n\n\n\nModel 1 is a submodel of Model 3, since all variables used in Model 1 are also used in Model 3.\nModel 2 is also a submodel of Model 3.\nModels 1, 2, and 3 are all submodels of Model 4.\nModel 0 is a submodel of Models 1, 2, 3, and 4.\nModels 1 and 2 are not submodels of each other, since Model 1 contains a variable used in Model 2 and Model 2 contains a variable not used in Model 1.\n\n\n\n2.5.2 F-Statistics\nWhen one model is a submodel of another, we can compare the amount of variability explained by the models, using a technique known as ANalysis Of VAriance (ANOVA).\nReduced Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_qx_{iq}\\)\nFull Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_qx_{iq} + b_{q+1}x_{i{q+1}} \\ldots + b_px_{ip}\\)\np = # terms in Full Model, not including the intercept\nq = # terms in Reduced Model, not including the intercept\nn = number of observations\nWe calculate a statistic called F that measures the amount of variability explained by adding additional variable(s) to the model, relative to the total amount of unexplained variability.\n\\[\n\\begin{aligned}\nF  \n&= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}}\n\\end{aligned}\n\\]\n\nLarge values of F indicate that adding the additional explanatory variables is helpful in explaining variability in the response variable\n\nSmall values of F indicate that adding new explanatory variables variables does not make much of a difference in explaining variability in the response variable\n\nWhat counts as “large” is depends on \\(n, p,\\) and \\(q\\). We will revisit this later in the course.\n\nExample 1\nLet’s Calculate an ANOVA F-Statistic to compare Models 1 and 3.\nReduced Model:\n\\[\n\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft\\_living}\n\\]\nFull Model:\n\\[\n\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft\\_living}+ b_2\\times\\text{Waterfront}\n\\]\n\\[\n\\begin{aligned}\nF &= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\\n&=\\frac{\\frac{23,767,280-16,521,296}{2-1}}{\\frac{16,521,296}{100-(2+1)}} \\\\\n\\end{aligned}\n\\]\n\n((SSR_sqft-SSR_wf_sqft)/(2-1))/((SSR_wf_sqft)/(100-(2+1)))\n\n[1] 42.54269\n\n\nWe can calculate the statistic directly in R, using the anova command.\n\nanova(M_House_sqft, M_wf_SqFt)$F[2]\n\n[1] 42.54269\n\n\nIn the coming chapters, we’ll talk about what to conclude from an F-statistic of 42.5 Is this big enough to say that adding waterfront status to a model already including square feet helps better explain variability in sale price? (Spoiler alert: YES - an F-statistic of 42.5 is quite large and indicative that the full model is a better choice than the reduced model.) We previously saw that the model including both square feet and waterfront status had a \\(R^2\\) value considerably higher than the one including only square feet. This large F-statistic is further evidence to the benefit of considering both variables in our model.\nExample 2\nWe’ll calculate an F-statistic to compare Models 3 and 4. This can help us determine whether it is worthwhile to include an interaction term in our model.\nReduced Model: \\[\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft\\_living} + b_2\\times\\text{Waterfront}\\]\nFull Model: \\[\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft\\_living}+ b_2\\times\\text{Waterfront} + b_3\\times\\text{sqft\\_living}\\times\\text{Waterfront}\\]\n\\[\n\\begin{aligned}\nF &= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\\n&=\\frac{\\frac{16,521,296-10,139,974}{3-2}}{\\frac{10,139,974}{100-(3+1)}} \\\\\n\\end{aligned}\n\\]\n\n((SSR_wf_sqft-SSR_int)/(3-2))/((SSR_int)/(100-(3+1)))\n\n[1] 60.41505\n\n\nWe can calculate the statistic directly in R, using the anova command.\n\nanova(M_wf_SqFt, M_House_Int)$F[2]\n\n[1] 60.41505\n\n\nWe observe an F-statistic of 60, which is even bigger than the one seen previously! This suggests that adding the interaction term does indeed improve the model’s ability to account for variability in prices.\n\n\n2.5.3 Comparing 3 or More Categories\nF-statistics are commonly used when making comparisons involving categorical variables with 3 or more categories.\nOne variable in the houses dataset, which we haven’t looked at yet, is the condition of the house at the time of sale. The table shows the number of houses in each condition listed.\n\nsummary(Houses$condition)\n\naverage or below             good        very_good \n              61               30                9 \n\n\nWe notice that there is only one house in poor condition and one house in fair condition. These sample sizes are too small to analyze. We’ll combine these two houses with those in the “average” category, creating a new category called “average or below).\n\nHouses$condition &lt;- fct_collapse(Houses$condition, \"average or below\" = c(\"poor\",\"fair\", \"average\"))\n\nThe boxplot shows the distribution of houses in each category, and the table below it provides a numerical summary.\n\nggplot(data=Houses, aes(x=condition, y=price)) + geom_boxplot() +coord_flip()\n\n\n\n\n\n\n\n\n\nCond_Tab &lt;- Houses %&gt;% group_by(condition) %&gt;% summarize(Mean_Price = mean(price), \n                                             SD_Price= sd (price), \n                                             N= n())\nkable(Cond_Tab)\n\n\n\n\ncondition\nMean_Price\nSD_Price\nN\n\n\n\n\naverage or below\n700.6349\n768.1179\n61\n\n\ngood\n861.0000\n1048.9521\n30\n\n\nvery_good\n551.8361\n332.8597\n9\n\n\n\n\n\nIt can be helpful to calculate a single statistic that quantifies the size of the differences between the conditions. If we were just comparing two different categories, we could simply find the difference in mean prices between them. But, with three or more categories, we need a way to represent the size of the differences with a single number. An F-statistic can serve this purpose.\nWe’ll calculate an F-statistic for a model that includes condition, compared to a model with only an intercept term.\nReduced Model: \\[\\widehat{\\text{Price}}= b_0\\]\nFull Model: \\[\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{good condition}+ b_2\\times\\text{very good condition}\\]\nNotice that the equation includes separate variables for the “good” and “very” good conditions. These variables take on value 0 if the house is not in that condition, and 1 if the house is in that condition. Here, houses in “average or below” condition are considered the “baseline” category.\nWe’ll fit the model in R. The coefficient estimates for \\(b_0\\), \\(b_1\\) and \\(b_2\\) are shown below.\n\nM_House_Cond &lt;- lm(data=Houses, price~condition)\nM_House_Cond\n\n\nCall:\nlm(formula = price ~ condition, data = Houses)\n\nCoefficients:\n       (Intercept)       conditiongood  conditionvery_good  \n             700.6               160.4              -148.8  \n\n\nThe model equation is\n\\[\n\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{good condition}+ b_2\\times\\text{very good condition}\n\\]\nInterpretations\n\nOn average, houses in average or below condition cost 700.6 thousand dollars.\n\nOn average, houses in good condition cost 160.4 thousand dollars more than those in average or below condition.\n\nOn average, houses in very good condition cost 148.8 thousand dollars less than those in average or below condition.\n\nThis last sentence is surprising and merits further investigation. We’ll leave that for future consideration.\nFor now, we’ll calculate an F-statistic based on the models.\nNote that in this case, the reduced model does not include any explanatory variables, so SSR is equal to SST, which we calculate previously.\n\nSST\n\n[1] 69045634\n\n\nWe calculate SSR for the full model.\n\nSSR_cond &lt;- sum(M_House_Cond$residuals^2)\nSSR_cond  \n\n[1] 68195387\n\n\n\\[\n\\begin{aligned}\nF &= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\\n&=\\frac{\\frac{69,045,634-68,195,387}{2-0}}{\\frac{68,195,387}{100-(2+1)}} \\\\\n\\end{aligned}\n\\]\n\n((SST - SSR_cond)/(2-0))/(SSR_cond/(100-(2+1)))\n\n[1] 0.6046888\n\n\nWe perform the calculation directly in R.\n\nanova(M_House_Cond, M0_House)$F[2]\n\n[1] 0.6046888\n\n\nNotice that the F-statistic of 0.6 is considerably smaller than the F-statistics we’ve seen previously.\nThis indicates that adding condition to a model with no other explanatory variables doesn’t seem to help improve the model’s ability to account for variation in price. Put another way, there doesn’t appear to be much evidence of difference in price between houses in the different conditions.\n\n\n2.5.4 F-Statistic Illustration\nThe figure below gives an illustration of data that would produce a large F-statistic (Scenario 1), and also data that would produce a small F-statistic (Scenario 2), like the one seen in the house condition data.\nAn F-statistic compares the amount of variability between groups to the amount of variability within groups.\nIn scenario 1, we notice considerable differences between the groups, relative to the amount of variability within groups. In this scenario, knowing the group an observation is in will help us predict the response for that group, so we should include account for the groups in our model. We would obtain a large F-statistic when comparing a model that includes group to one that contains only an intercept term.\nIn scenario 2, there is little difference between the overall averages in each group, and more variability between individual observations within each group. In a scenario like this, knowing the group an observation lies in does little to help us predict the response. In this scenario, predictions from a model that includes group as an explantory variable would not be much better than those from a model that does not. Hence, we would obtain a small F-statistic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScenario 1\nScenario 2\n\n\n\n\nvariation between groups\nHigh\nLow\n\n\nvariation within groups\nLow\nHigh\n\n\nF Statistic\nLarge\nSmall\n\n\nResult\nEvidence of Group Differences\nNo evidence of differences\n\n\n\n\n\n2.5.5 Alternative F-Statistic Formula\nThe above illustration suggests alternative (and mathematically equivalent) way to calculate the F-statistic. We calculate the ratio of variability between different groups, relative to the amount of variability within each group\nFor a categorical variable with \\(g\\) groups,\n\nlet \\(\\bar{y}_{1\\cdot}, \\ldots, \\bar{y}_{g\\cdot}\\) represent the mean response for each group.\nlet \\(n_1, \\ldots, n_g\\) represent the sample size for each group\nThen \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}\\) gives a measure of how much the group means differ, and\n\\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}\\) gives a measure of how much individual observations differ within groups\nAn alternative formula for this F-statistic is:\n\n\\[\nF= \\frac{\\text{Variability between groups}}{\\text{Variability within groups}}= \\frac{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}}{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}}\n\\]\n\nIt can be shown that this statistic is equivalent to the one we saw previously.\n\nExample\nLet’s recalculate the F-statistic for the conditions of the houses, using this alternate formula. The first 3 houses are shown.\n\nkable(head(Houses %&gt;% select(Id, price, condition),3))\n\n\n\n\nId\nprice\ncondition\n\n\n\n\n1\n1225\naverage or below\n\n\n2\n885\naverage or below\n\n\n3\n385\ngood\n\n\n\n\n\nWe have seen previously that:\n\n\\(\\bar{y}_{\\cdot\\cdot}=735.3526\\) (overall average price), and \\(n=10\\)\n\\(\\bar{y}_{1\\cdot}=700.6349\\) (average price for average or below houses), and \\(n_1=61\\)\n\n\\(\\bar{y}_{2\\cdot}=861.0\\) (average price for good houses), and \\(n_2=30\\)\n\n\\(\\bar{y}_{3\\cdot}=551.8361\\) (average price for very good houses), and \\(n_3=9\\)\n\nThen,\n\n\\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1} = \\frac{61(700.6349-735.3526)^2+30(861.0-735.3526)^2+9(551.8361-735.3526)^2}{3-1} = \\frac{850247.3}{2}\\), and\n\\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g} = \\frac{(1225.0-700.6349)^2+ (885.0 - 700.6349)^2 + (385.0-861.0)^2+\\ldots}{100-3} = \\frac{68195387}{97}\\)\n\n\\[\nF= \\frac{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}}{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}} = \\frac{\\frac{61(700.6349-735.3526)^2+30(861.0-735.3526)^2+9(551.8361-735.3526)^2}{3-1}}{\\frac{(1225.0-700.6349)^2+ (885.0 - 700.6349)^2 + (385.0-861.0)^2+\\ldots}{100-3}} = \\frac{\\frac{850247.3}{2}}{\\frac{68195387}{97}}\n\\]\n\nNote that the quantity in the the quantity in the third line is equivalent to the sum of the squared residuals using M2. Thus, we can calculate F using:\n\n\n((61*(700.6349-735.3526)^2+30*(861.0-735.3526)^2+9*(551.8361-735.3526)^2)/(3-1))/((SSR_cond)/(100-3))\n\n[1] 0.6046889\n\n\nFor models with only one categorical explanatory variable, “variability within vs variability between” interpretation of an F-statistic is very popular. This statistic is often relevant in studies in the natural and social sciences. Such studies are often referred to as One-Way ANOVA’s. In fact, these are just a special case of the full vs reduced model interpretation of the F-statistic, which can be applied to any two models, as long as one is a submodel of the other.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch3.html",
    "href": "Ch3.html",
    "title": "3  Simulation-Based Inference",
    "section": "",
    "text": "3.1 Sampling Distributions",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch3.html#sampling-distributions",
    "href": "Ch3.html#sampling-distributions",
    "title": "3  Simulation-Based Inference",
    "section": "",
    "text": "3.1.1 Population and Sample\nIn statistics, we often do not have the time, money, or means to collect data on all individuals or units on which we want to draw conclusions. Instead, we might collect data on only a subset of the individuals, and then make inferences about all individuals we are interested in, using the information we collected.\nVocabulary:\n\nA population is the entire set of individuals that we want to draw conclusions about.\nA sample is a subset of a population.\n\nA parameter is a numerical quantity pertaining to an entire population or process.\n\nA statistic is a numerical quantity calculated from a sample.\n\nWe’ll work with a dataset containing information on all 20,591 flights from New York to Chicago in 2013. Our population of interest is all 20,591 flights. We’re interested in the proportion of fights that arrive on time, and the average arrival delay (in minutes). Arrival delay represents how much earlier/later did the flight arrive than expected. Whether or not the flight arrived on time is a categorical variable, while arrival delay is a quantitative variable.\nIn this situation, we have information on the entire population. In statistics, this is rare. It is more common to have information on only subset of flights contained in a sample. If the sample is collected in a way that is representative of the population, such as by sampling at random, then we can use the sample to draw conclusions about the population.\nWe’ll begin by studying the behavior of sample statistics when we know the population parameters, and then use what we learn to handle more real situations where we don’t know about the entire population.\nThe parameter of interest is the proportion of on-time arrivals out of all flights in the population of 20,591. When the parameter is proportion, we’ll denote it with the letter \\(p\\).\nThe first 10 flights, all of which occurred on January 1 are shown below. We see that most of those flights were not on-time.\n\nhead(Flights_NY_CHI, 10)\n\n# A tibble: 10 × 9\n    year month   day carrier origin dest  sched_dep_time arr_delay ontime\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt; \n 1  2013     1     1 UA      EWR    ORD              558        12 N     \n 2  2013     1     1 AA      LGA    ORD              600         8 N     \n 3  2013     1     1 MQ      EWR    ORD              600        32 N     \n 4  2013     1     1 AA      LGA    ORD              630        14 N     \n 5  2013     1     1 AA      LGA    ORD              700         4 N     \n 6  2013     1     1 UA      LGA    ORD              700        20 N     \n 7  2013     1     1 UA      EWR    ORD              713        21 N     \n 8  2013     1     1 AA      LGA    ORD              745       -12 Y     \n 9  2013     1     1 MQ      EWR    ORD              710        49 N     \n10  2013     1     1 B6      JFK    ORD              830        15 N     \n\n\nNote that a negative arrival delay denotes a flight that arrived before expected, thus on time.\nThe bar graph shows the number of flights that arrived on time throughout the year.\n\non_time_plot_POP &lt;- ggplot(data=Flights_NY_CHI, aes(x=ontime)) + \n                  geom_bar(fill=\"blue\") + \n                  ggtitle(\"On-time Flights\")\non_time_plot_POP\n\n\n\n\n\n\n\n\nWe see that the majority of flights did arrive on time.\nWe’ll calculate the proportion of flights arriving on time, among all 20,591 flights in the population.\n\n#proportion of flights on time\np &lt;- sum(Flights_NY_CHI$ontime==\"Y\")/20591\np\n\n[1] 0.6079841\n\n\nWhen the population parameter is a proportion, we’ll denote it with the letter \\(p\\). Here $p=$0.6079841. Keep in mind that in a real situation, we typically won’t know the value of the population parameter \\(p\\), and will need to estimate it from a sample.\nThe histogram shows the distribution of arrival delay times. Negative delays indicate the flight arriving ahead of schedule.\n\nDelay_plot_POP &lt;- ggplot(data=Flights_NY_CHI, aes(x=arr_delay)) + \n  geom_histogram(fill=\"blue\", binwidth=5) + \n  ggtitle(\"Distribution of Arrival Delays\")\nDelay_plot_POP\n\n\n\n\n\n\n\n\nWe see that the distribution of arrival delays is heavily right-skewed. While most flights arrive around the scheduled time, a few were late by 3 or more hours.\nWe’ll calculate the mean arrival delay.\n\n#proportion of flights on time\nmu &lt;- mean(Flights_NY_CHI$arr_delay)\nmu\n\n[1] 7.144772\n\n\nWhen the population parameter represents a mean, we’ll denote it using \\(\\mu\\). Here \\(\\mu=\\) 7.144772.\nWe also calculate the standard deviation of arrival delays.\n\n# mean arrival delay in sample\nSD_delay &lt;- sd(S1$arr_delay)\nSD_delay\n\n[1] 60.10419\n\n\n\n\n3.1.2 Sampling Variability\nWe typically won’t have data on the full population and won’t know the values of parameters like \\(p\\) and \\(\\mu\\). Instead, we’ll have data on just a sample taken from the population.\nTo illustrate, we’ll take a sample of 75 flights. The first 6 flights in the sample are shown below. The ontime variable tells whether or not the flight arrived on time.\n\n# take sample of 75 flights\nset.seed(08082023)\nS1 &lt;- sample_n(Flights_NY_CHI, 75)\nhead(S1)\n\n# A tibble: 6 × 9\n   year month   day carrier origin dest  sched_dep_time arr_delay ontime\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt; \n1  2013     3     8 AA      LGA    ORD             1720       106 N     \n2  2013    12    15 AA      JFK    ORD             1715       -24 Y     \n3  2013    10    22 UA      EWR    ORD             1300       -12 Y     \n4  2013     8    26 UA      EWR    ORD             2110        15 N     \n5  2013     7    23 AA      LGA    ORD             1359        66 N     \n6  2013     5    13 UA      EWR    ORD              900       -21 Y     \n\n\nWe’ll calculate the number, and proportion of flights that arrived on time.\n\nnum_ontime &lt;- sum(S1$ontime == \"Y\") # count number of on-time arrivals\nnum_ontime\n\n[1] 39\n\n\nProportion of on-time arrivals in the sample.\n\n# proportion of on-time flights in sample\np_hat &lt;- num_ontime/75\np_hat\n\n[1] 0.52\n\n\nWhen the sample statistic is a proportion, it is commonly denoted \\(\\hat{p}\\).\nIn our sample \\(\\hat{p}\\) = 52 percent of flights arrived on-time. The sample statistic \\(\\hat{p}\\) is an estimate of the population proportion \\(p\\), the proportion of all flights arriving on time.\nWe also calculate the mean arrival delay in minutes.\n\n# mean arrival delay in sample\ny_bar &lt;- mean(S1$arr_delay)\ny_bar\n\n[1] 19.2\n\n\nWe’ll denote this sample mean \\(\\bar{y}\\). It is an estimate of the population mean, representing the arrival mean delay for all flights, which we’ll denote \\(\\mu\\).\nOf course, this was just one sample of 75 flights. If we took different samples of 75 flights, we would expect the statistics \\(\\hat{p}\\) and \\(\\bar{x}\\) to vary from sample to sample.\nHere’s a different sample of 75 flights.\n\nS2 &lt;- sample_n(Flights_NY_CHI, 75)\n\nProportion arriving on time in second sample:\n\n# proportion arriving on time in second sample\nnum_ontime2 &lt;- sum(S2$ontime == \"Y\") # count number of on-time arrivals\np_hat2 &lt;- num_ontime2/75\np_hat2\n\n[1] 0.5066667\n\n\nMean arrival delay in second sample:\n\n# mean arrival delay in second sample\ny_bar2 &lt;- mean(S2$arr_delay)\ny_bar2\n\n[1] 15.28\n\n\nSample statistics will vary from sample to sample, thus it is not realistic to expect them to exactly match their corresponding population parameters.\nNevertheless we can use the sample to estimate the proportion of all flights in the population that arrive on time.\nLet’s take 10,000 more random samples of 75 flights and record the proportion of on-time arrivals in each sample.\n\nnreps &lt;- 10000  # number of repetitions\np_hat_val &lt;- rep(NA, nreps) # create vector to hold proportion of on-time arrivals\ny_bar_val &lt;- rep(NA, nreps) # create vector to hold mean arrival delay\n\nSample &lt;- 1:nreps\n\nfor(i in 1:nreps){\nS &lt;- sample_n(Flights_NY_CHI, 75) # take sample of 75\nN_ontime &lt;- sum(S$ontime == \"Y\") # count number of on-time arrivals\np_hat_val[i] &lt;- N_ontime/75 # record proportion on-time\ny_bar_val[i] &lt;- mean(S$arr_delay) # record mean arrival delay\n}\n\nSamples_df &lt;- data.frame(Sample, p_hat_val, y_bar_val) # store results in a data frame\n\nThe table shows the proportion of on-time arrivals in the first 20 samples of 75 flights.\n\nkable(head(Samples_df, 20) |&gt; round(2))\n\n\n\n\nSample\np_hat_val\ny_bar_val\n\n\n\n\n1\n0.65\n6.16\n\n\n2\n0.53\n10.75\n\n\n3\n0.59\n12.41\n\n\n4\n0.69\n2.24\n\n\n5\n0.59\n7.33\n\n\n6\n0.67\n1.28\n\n\n7\n0.61\n3.25\n\n\n8\n0.59\n8.93\n\n\n9\n0.65\n-3.16\n\n\n10\n0.65\n5.73\n\n\n11\n0.61\n12.32\n\n\n12\n0.59\n8.79\n\n\n13\n0.61\n5.53\n\n\n14\n0.48\n10.55\n\n\n15\n0.60\n5.40\n\n\n16\n0.44\n24.11\n\n\n17\n0.49\n15.93\n\n\n18\n0.57\n4.45\n\n\n19\n0.64\n6.76\n\n\n20\n0.56\n3.65\n\n\n\n\n\nThe histogram below shows the distribution of the proportion of on-time arrivals in the 10,000 different samples.\n\nProp_Samp_Dist&lt;- ggplot(data=Samples_df, aes(x=p_hat_val)) +\n  geom_histogram(color=\"blue\", fill=\"blue\", binwidth=0.001) + \n  ggtitle(\"Sampling Distribution for Proportion On Time\") + \n  xlab(\"Prop. on time in sample\")\nProp_Samp_Dist\n\n\n\n\n\n\n\n\nWe notice that most of our 10,000 samples yielded proportions of on-time arrivals between 0.5 and 0.7, The distribution of proportion of on-time arrivals is roughly symmetric and bell-shaped.\nThe distribution shown in this histogram is called the sampling distribution for \\(\\hat{p}\\). The sampling distribution for a statistic shows the distribution of the statistic over many samples.\nWe’ll calculate the mean of the sampling distribution for \\(\\hat{p}\\). How does it compare to the true population parameter \\(p\\)?\n\nMean_p_hat &lt;- mean(Samples_df$p_hat_val)\nMean_p_hat\n\n[1] 0.60848\n\n\nWe can gauge how much the proportion of on-time arrivals varies between samples by calculating the standard deviation of this sampling distribution. The standard deviation of a sampling distribution for a statistic is also called the standard error of the statistic. In this case it represents the standard error \\(\\hat{p}\\) (the proportion of on-time arrivals), and is denoted \\(\\text{SE}(\\hat{p})\\). This standard error is shown below.\n\nSE_p_hat &lt;- sd(Samples_df$p_hat_val)\nSE_p_hat\n\n[1] 0.05659102\n\n\nNow, we’ll examine the sampling distribution of the mean arrival time \\(\\bar{y}\\).\n\nMean_Samp_Dist&lt;- ggplot(data=Samples_df, aes(x=y_bar_val)) +\n  geom_histogram(color=\"white\", fill=\"blue\", binwidth=0.5) + \n  ggtitle(\"Sampling Distribution for Mean Arrival Delay\") + \n  xlab(\"Mean Arrival Delay\")\nMean_Samp_Dist\n\n\n\n\n\n\n\n\nHow does the sampling distribution for mean arrival delays compare to the distribution of arrival delays for individual flights? Think about the shape and the variability of the distributions.\n\nmean_y_bar &lt;- mean(Samples_df$y_bar_val)\nmean(mean_y_bar)\n\n[1] 7.081397\n\n\nThe standard error of the mean, \\(SE(\\bar{y})\\) is shown calculated below.\n\nSE_y_bar &lt;- sd(Samples_df$y_bar_val)\nSE_y_bar\n\n[1] 5.563925\n\n\nWhat does this standard error represent? How is it different than the the standard deviation of flight times, which we previously saw was 60.1 minutes?\nVocabulary:\n\nThe sampling distribution of a statistic is the distribution of values the statistic takes on across many different samples of a given size.\n\nThe standard error of a statistic is the standard deviation of that statistic’s sampling distribution. It measures how much the statistic varies between different samples of a given size.\n\n\n\n3.1.3 Sample Size and Standard Error\nQuestion:\nSuppose the sample consisted of 10, or 30, or 500 flights, instead of 75? Would you expect the standard deviation of individual flight times to increase, decrease or stay about the same? What about the standard error of the mean delay?\nThe histogram shows the distribution of individual flight delays in random samples of each size.\n\n\n\n\n\n\n\n\n\nFor each sample, most of the flights have delays slightly below or above 0, though a small percentage of the flights in each sample have much larger delays. The variability in delays is about the same, regardless of sample size.\nThe table shows the standard deviation in each of the samples.\n\n\n\n\n\nSample_Size\nSD\n\n\n\n\n10\n45.38673\n\n\n30\n40.64390\n\n\n75\n43.40787\n\n\n200\n48.01117\n\n\n\n\n\nSample size does not impact the amount of variability between individual flights. Standard deviation in delay times does not systematically increase or decrease based on sample size (of course it varies a little based on the lakes randomly chosen in the sample).\nNow, we’ll examine what happens to the standard error of the mean as the sample size changes.\nDistributions of Mean Between Different Samples\n\n\n\n\n\n\n\n\n\nNotice that as the sample size increases, the sampling distribution of the mean becomes more symmetric and bell-shaped, and also more concentrated around the population mean \\(\\mu\\).\nThe table shows the standard error of the mean for samples of different size:\n\n\n\n\n\nSample_Size\nSE\n\n\n\n\n10\n15.027787\n\n\n30\n8.832375\n\n\n75\n5.461011\n\n\n200\n3.371757\n\n\n\n\n\nAs sample size increases, variability between means of different samples decreases. Standard error of the mean decreases. This is also true of standard errors for other statistics (i.e. difference in means, regression slopes, etc.)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch3.html#confidence-intervals",
    "href": "Ch3.html#confidence-intervals",
    "title": "3  Simulation-Based Inference",
    "section": "3.2 Confidence Intervals",
    "text": "3.2 Confidence Intervals\n\n3.2.1 Constructing Confidence Intervals\nWe saw that while statistics calculated from individual samples deviate from population parameters, over many samples, they approximately average to the population parameter (assuming the samples are chosen randomly).\nThus, when we have only a single sample, we can use the sample statistic as an estimate of the population parameter, provided we allow for a certain margin of error. The question is how much margin of error do we need?\nThe sampling distribution for the proportion of on-time flights is shown again below. The true proportion of on-time flights (\\(p=0.607984\\)) is marked by the green dotted line. The gold bar at the bottom of the histogram represents the range of sample proportions that lie within \\(\\pm 2\\) standard errors of the true population proportion of flights that arrived on time:\n0.608 - 2(0.057) = 0.495 to 0.608 + 2(0.057) = 0.721\n\nProp_Samp_Dist + geom_vline(xintercept=p, color=\"green\", linetype=\"dotted\", linewidth=2) + geom_segment(aes(x=p - 2*SE_p_hat,xend=p + 2*SE_p_hat, y=50, yend=50), color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe calculate the proportion of samples whose proportion of on-time arrivals lies within \\(\\pm 2\\) standard errors of the true proportion.\n\nLower &lt;- p - 2*SE_p_hat\nUpper &lt;- p + 2*SE_p_hat\nsum((Samples_df$p_hat_val &gt;=Lower) & (Samples_df$p_hat_val &lt;= Upper))\n\n[1] 9539\n\n\nApproximately 95% 10,000 samples produced proportions within \\(\\pm 2\\) standard errors of the true population proportion of on-time flights.\nIn a real situation, we won’t have access to the entire population of flights, only the flights in a single sample. For example, recall our original sample of 75 flights, in which we observed a proportion of on-time arrivals of \\(\\hat{p}=\\) 0.52.\nSince we now know that 95% of all samples produce proportions that lie within two standard errors of the population proportion, we can obtain an estimate of the population proportion \\(p\\) by adding and subtracting \\(2\\times \\text{SE}(\\hat{p})\\) from our observed sample proportion \\(\\hat{p}\\).\nUsing probability theory, it can be shown generally that if the sampling distribution of a statistic is symmetric and bell shaped, then approximately 95% of all samples will produce sample statistics that lie within two standard errors of the corresponding population parameter. Such an interval is called an approximate 95% confidence interval for the population parameter.\nApproximate 95% confidence interval: If the sampling distribution of a statistic is symmetric and bell-shaped, a 95% confidence interval for the population parameter is:\n\\[\n\\text{Statistic} \\pm 2\\times \\text{Standard Error},\n\\]\nMore generally, if we want to use a level of confidence that is different than 95%, we can adjust the value we multiply the standard error by. In general, a standard error confidence interval has the form:\n\\[\n\\text{Statistic } \\pm m\\times \\text{Standard Error},\n\\]\nwhere the value of \\(m\\) depends on the desired level of confidence.\nConfidence intervals that are calculated by adding and subtracting a certain number of standard errors from the sample statistic are called standard error confidence intervals. This approach works as long as the sampling distribution is symmetric and bell-shaped. Probability theory tells us that in a symmetric and bell-shaped distribution, approximately 95% of the area lies within two standard errors of the center of the distribution, given by the true parameter value. We will, however, see that this approach will not work in all cases. Not all statistics produce sampling distributions that are symmetric and bell-shaped, and we will need an alternative way to calculate confidence intervals in these situations.\n\n\n\n\n\nImage from https://openintro-ims.netlify.app/foundations-mathematical\n\n\n\n\n\n3.2.1.1 Example: 95% Confidence Interval for \\(p\\)\nWe’ll calculate a 95% confidence interval for the proportion of on-time flights, using our original sample where \\(\\hat{p}\\) = 0.52. The 95% confidence interval is:\n\\[\n\\begin{aligned}\n& \\hat{p} \\pm 2\\times \\text{SE}(\\hat{p}) \\\\\n& = 0.52 \\pm 2(0.056591)\n\\end{aligned}\n\\]\nThe confidence interval is calculated below.\n\nc(p_hat - 2*SE_p_hat, p_hat + 2*SE_p_hat) \n\n[1] 0.406818 0.633182\n\n\nBased on our sample of 75 flights, we can be 95% confident that the true proportion of on-time arrivals among all 2013 flights from New York to Chicago is between 0.407 and 0.633.\n\n\n3.2.1.2 Example: 95% Confidence Interval for \\(\\mu\\)\nLikewise, we calculate a 95% confidence interval for average arrival delay using the formula:\n\\[\n\\begin{aligned}\n\\bar{y} \\pm 2\\times \\text{SE}(\\bar{y}) \\\\\n& = 19.2 \\pm 2(5.5639246)\n\\end{aligned}\n\\]\n\nc(y_bar - 2*SE_y_bar, y_bar + 2*SE_y_bar) \n\n[1]  8.072151 30.327849\n\n\nBased on our sample of 75 flights, we can be 95% confident that the mean arrival delay among all 2013 flights from New York to Chicago is between 8.1 and 30.3 minutes.\nNote that this is a statement about what we think is true of the mean overall flight time, not the time of an individual flight. It would be incorrect to say that we are 95% confident that an individual flight would be expected to have a delay in this interval. You might think about whether the interval for the delay time of an individual flight should be wider or narrower than this. We’ll talk about such an interval later in the term.\n\n\n\n3.2.2 What does 95% Confidence Mean?\nKnowing what we do about the true value of the population parameters \\(p\\) and \\(\\mu\\), we can see that our interval for \\(p\\), which was (0.407, 0.633) does indeed contain the true population value of \\(p=\\) 0.6079841. However, the interval for \\(\\mu\\), which was (8.1, 30.3) does not contain the true value of \\(\\mu=\\) 7.144772.\nDoes this mean we did something wrong when we calculated the interval for \\(\\mu\\), the average flight delay among all flights in the population? The answer is “no”. Notice we claimed to be only “95%” confident that our interval contains the true value of the population parameter \\(\\mu\\). This means that we should expect 5% of samples taken randomly to yield a sample mean \\(\\bar{y}\\) so different from the population mean \\(\\mu\\), that the resulting confidence interval would not contain the true value of \\(\\mu\\). This does not mean we did anything wrong, just that we obtained an unusual sample just by chance. Since our procedure, namely adding and subtracting two standard errors, is designed to work 95% of the time, we can expect such samples to be rare.\nIn a real situation, we won’t know the true value of the population parameter, so we won’t know for sure whether or not our confidence interval contains this true parameter value.\nTo further understand the meaning of “95% confidence”, let’s explore what happens when we calculate confidence intervals based on estimates \\(\\bar{y}\\) obtained from many different samples. For each of our 10,000 different samples taken from our population, we’ll add and subtract two standard errors from the sample proportion \\(\\hat{p}\\) corresponding to that sample.\nThe table below displays the value of \\(\\hat{p}\\), for the first 20 samples we took, along with the lower and upper bounds of the confidence interval, and whether or not the confidence interval contains the true parameter value \\(p\\) (either 1=TRUE or 0=FALSE).\n\nSamples_df_p &lt;- Samples_df %&gt;% mutate(Lower = p_hat_val - 2*SE_p_hat, \n                                     Upper = p_hat_val + 2*SE_p_hat,\n                                     Containsp = p &gt;= Lower & p &lt;= Upper) |&gt;\n                              select(Sample, p_hat_val, Lower, Upper, Containsp)  \nkable(head(Samples_df_p |&gt; round(2), 20))\n\n\n\n\nSample\np_hat_val\nLower\nUpper\nContainsp\n\n\n\n\n1\n0.65\n0.54\n0.77\n1\n\n\n2\n0.53\n0.42\n0.65\n1\n\n\n3\n0.59\n0.47\n0.70\n1\n\n\n4\n0.69\n0.58\n0.81\n1\n\n\n5\n0.59\n0.47\n0.70\n1\n\n\n6\n0.67\n0.55\n0.78\n1\n\n\n7\n0.61\n0.50\n0.73\n1\n\n\n8\n0.59\n0.47\n0.70\n1\n\n\n9\n0.65\n0.54\n0.77\n1\n\n\n10\n0.65\n0.54\n0.77\n1\n\n\n11\n0.61\n0.50\n0.73\n1\n\n\n12\n0.59\n0.47\n0.70\n1\n\n\n13\n0.61\n0.50\n0.73\n1\n\n\n14\n0.48\n0.37\n0.59\n0\n\n\n15\n0.60\n0.49\n0.71\n1\n\n\n16\n0.44\n0.33\n0.55\n0\n\n\n17\n0.49\n0.38\n0.61\n0\n\n\n18\n0.57\n0.46\n0.69\n1\n\n\n19\n0.64\n0.53\n0.75\n1\n\n\n20\n0.56\n0.45\n0.67\n1\n\n\n\n\n\nThe graphic below visualizes the confidence intervals produced using the estimates from the first 100 samples. The green dotted line indicates the true value of \\(p\\). The black dots indicate the value of \\(\\hat{p}\\) for each sample. Intervals that do in fact contain the true value of \\(p\\) are shown in blue, and intervals that do not contain the true value of \\(p\\) are shown in green.\n\nggplot(data=Samples_df_p[1:100,], aes(y=Sample, x=p_hat_val)) +    \n  geom_point() +\n  geom_errorbar(aes(xmin = Lower, xmax = Upper, color=Containsp))  + \n  xlab(\"Confidence Interval\") + \n  ylab(\"Sample\") + \n  geom_vline(xintercept = p, color=\"green\", linetype=\"dotted\", size=2) + \n  ggtitle(\"100 Different Confidence Intervals for p\") + \n  theme_bw() \n\n\n\n\n\n\n\n\nOut of these 100 samples, 93 contain the true value of the population parameter \\(p\\). This is close to the desired 95% confidence level.\nThe picture shows confidence intervals produced by the first 100 samples, but we actually took 10,000 different samples of 75 flights. Let’s calculate how many of these samples produced confidence intervals that contain the true value of \\(p\\).\n\nsum(Samples_df_p$Contains == TRUE)\n\n[1] 9539\n\n\nAgain, notice that close to 95% of the samples produced confidence intervals contain the true population parameter \\(p\\). Note that for the red intervals that do not contain \\(p\\) nothing was done incorrectly. The sample was taken at random, and the confidence interval was calculated using the correct formula. It just happened that by chance, we obtained a sample proportion \\(\\hat{p}\\) that was unusually high or low, leading to an interval that did not capture the true population parameter. This, of course, happens rarely, and approximately 95% of the samples do, in fact, result in intervals that contain the true value of \\(p\\).\n\nSamples_df_mu &lt;- Samples_df %&gt;% mutate(Lower = y_bar_val - 2*SE_y_bar, \n                                     Upper = y_bar_val + 2*SE_y_bar,\n                                     Containsmu = mu &gt;= Lower & mu &lt;= Upper) |&gt;\n                              select(Sample, y_bar_val, Lower, Upper, Containsmu)  \nkable(head(Samples_df_mu |&gt; round(2), 20))\n\n\n\n\nSample\ny_bar_val\nLower\nUpper\nContainsmu\n\n\n\n\n1\n6.16\n-4.97\n17.29\n1\n\n\n2\n10.75\n-0.38\n21.87\n1\n\n\n3\n12.41\n1.29\n23.54\n1\n\n\n4\n2.24\n-8.89\n13.37\n1\n\n\n5\n7.33\n-3.79\n18.46\n1\n\n\n6\n1.28\n-9.85\n12.41\n1\n\n\n7\n3.25\n-7.87\n14.38\n1\n\n\n8\n8.93\n-2.19\n20.06\n1\n\n\n9\n-3.16\n-14.29\n7.97\n1\n\n\n10\n5.73\n-5.39\n16.86\n1\n\n\n11\n12.32\n1.19\n23.45\n1\n\n\n12\n8.79\n-2.34\n19.91\n1\n\n\n13\n5.53\n-5.59\n16.66\n1\n\n\n14\n10.55\n-0.58\n21.67\n1\n\n\n15\n5.40\n-5.73\n16.53\n1\n\n\n16\n24.11\n12.98\n35.23\n0\n\n\n17\n15.93\n4.81\n27.06\n1\n\n\n18\n4.45\n-6.67\n15.58\n1\n\n\n19\n6.76\n-4.37\n17.89\n1\n\n\n20\n3.65\n-7.47\n14.78\n1\n\n\n\n\n\nThe graphic below visualizes the confidence intervals produced using the estimates from the first 100 samples. The green dotted line indicates the true value of \\(p\\). The black dots indicate the value of \\(\\hat{p}\\) for each sample. Intervals that do in fact contain the true value of \\(p\\) are shown in blue, and intervals that do not contain the true value of \\(p\\) are shown in green.\n\nggplot(data=Samples_df_mu[1:100,], aes(y=Sample, x=y_bar_val)) +    \n  geom_point() +\n  geom_errorbar(aes(xmin = Lower, xmax = Upper, color=Containsmu))  + \n  xlab(\"Confidence Interval\") + \n  ylab(\"Sample\") + \n  geom_vline(xintercept = mu, color=\"green\", linetype=\"dotted\", size=2) + \n  ggtitle(expression(paste(\"100 Different Confidence Intervals for \", mu))) + \n  theme_bw() \n\n\n\n\n\n\n\n\nOut of these 100 samples, 92 contain the true value of the population parameter \\(\\mu\\).\nOut of all 10,000 samples, the proportion containing the true population value of \\(\\mu\\) is:\n\nsum(Samples_df_mu$Containsmu == TRUE)\n\n[1] 9570\n\n\nThis brings us back to the question “what does 95% confidence mean?”. An approximate 95% confidence interval means that if we take a large number of samples and calculate confidence intervals from each of them, then approximately 95% of the samples will produce intervals containing the true population parameter. In reality, we’ll only have on sample, and won’t know whether or not our interval contains the true parameter value. Assuming we have taken the sample and calculated the interval correctly, we can rest assured in the knowledge that that 95% of all intervals taken would contain the true parameter value, and hope that ours is among that 95%.\nIt might be tempting to say that “there is approximately a 95% chance” that the population parameter lies within the confidence interval, but this is incorrect. In the statistical framework used here (known as classical, or frequentist statistics), the population parameter is assumed to be a fixed, but (typically) unknown number. It either is within the interval, or it isn’t. We just (typically) don’t know which. There’s nothing random about whether or not the parameter value is in our interval, so it doesn’t make sense to speak of it in terms of chance or randomness. Randomness comes into play due to the fact that we selected a random sample, which will produce a statistic likely to differ from the population parameter due to sampling variability. A different statistical framework, known as Bayesian statistics approaches this differently, and would allow us to use randomness and chance to describe our beliefs about any uncertain quantity, including a population proportion. In this class, however, we’ll stick to the classical frequentist interpretation.\nOf course, you might ask why we needed to calculate a confidence interval for the proportion of on-time flights in the first place, since we actually have data on all 20,591 flights in the population and already know the true proportion of on-time arrivals and mean arrival delay. The answer is that we don’t. But, in most real situations, we will only have data from a single sample, not the entire population, and we won’t know the true population parameter. We’ll be able to build on the ideas of sampling distributions and standard error that we learned about in this section to calculate confidence intervals in those scenarios.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch3.html#bootstrapping",
    "href": "Ch3.html#bootstrapping",
    "title": "3  Simulation-Based Inference",
    "section": "3.3 Bootstrapping",
    "text": "3.3 Bootstrapping\n\n3.3.1 Mercury Concentration in Florida Lakes\nA 2004 study by Lange, T., Royals, H. and Connor, L. examined Mercury accumulation in large-mouth bass, taken from a sample of 53 Florida Lakes. If Mercury accumulation exceeds 0.5 ppm, then there are environmental concerns. In fact, the legal safety limit in Canada is 0.5 ppm, although it is 1 ppm in the United States.\nIn our sample, we have data on 53 lakes, out of more than 30,000 lakes in the the state of Florida. We’ll attempt to draw conclusions about the entire population, consisting of all lakes in Florida, using data from our sample of 53. It is not clear how the lakes in this sample of 53 were selected, or how representative they are of all lakes in the state of Florida. Let’s assume for our purposes that the lakes in the sample can be reasonably thought of as being representative of all lakes in Florida.\n\n\n\n\n\nhttps://www.maine.gov/ifw/fish-wildlife/fisheries/species-information/largemouth-bass.html\n\n\n\n\n\ndata(\"FloridaLakes\")\nglimpse(FloridaLakes)\n\nRows: 53\nColumns: 12\n$ ID                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ Lake              &lt;chr&gt; \"Alligator\", \"Annie\", \"Apopka\", \"Blue Cypress\", \"Bri…\n$ Alkalinity        &lt;dbl&gt; 5.9, 3.5, 116.0, 39.4, 2.5, 19.6, 5.2, 71.4, 26.4, 4…\n$ pH                &lt;dbl&gt; 6.1, 5.1, 9.1, 6.9, 4.6, 7.3, 5.4, 8.1, 5.8, 6.4, 5.…\n$ Calcium           &lt;dbl&gt; 3.0, 1.9, 44.1, 16.4, 2.9, 4.5, 2.8, 55.2, 9.2, 4.6,…\n$ Chlorophyll       &lt;dbl&gt; 0.7, 3.2, 128.3, 3.5, 1.8, 44.1, 3.4, 33.7, 1.6, 22.…\n$ AvgMercury        &lt;dbl&gt; 1.23, 1.33, 0.04, 0.44, 1.20, 0.27, 0.48, 0.19, 0.83…\n$ NumSamples        &lt;int&gt; 5, 7, 6, 12, 12, 14, 10, 12, 24, 12, 12, 12, 7, 43, …\n$ MinMercury        &lt;dbl&gt; 0.85, 0.92, 0.04, 0.13, 0.69, 0.04, 0.30, 0.08, 0.26…\n$ MaxMercury        &lt;dbl&gt; 1.43, 1.90, 0.06, 0.84, 1.50, 0.48, 0.72, 0.38, 1.40…\n$ ThreeYrStdMercury &lt;dbl&gt; 1.53, 1.33, 0.04, 0.44, 1.33, 0.25, 0.45, 0.16, 0.72…\n$ AgeData           &lt;int&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1…\n\n\nWe are interested in whether mercury levels are higher or lower, on average, in Northern Florida compared to Southern Florida.\nWe’ll divide the state along route 50, which runs East-West, passing through Northern Orlando.\n\n\n\n\n\nfrom Google Maps\n\n\n\n\nWe add a variable indicating whether each lake lies in the northern or southern part of the state.\n\nlibrary(Lock5Data)\ndata(FloridaLakes)\n#Location relative to rt. 50\nFloridaLakes$Location &lt;- as.factor(c(\"S\",\"S\",\"N\",\"S\",\"S\",\"N\",\"N\",\"N\",\"N\",\"N\",\"N\",\"S\",\"N\",\"S\",\"N\",\"N\",\"N\",\"N\",\"S\",\"S\",\"N\",\"S\",\"N\",\"S\",\"N\",\"S\",\"N\",\"S\",\"N\",\"N\",\"N\",\"N\",\"N\",\"N\",\"S\",\"N\",\"N\",\"S\",\"S\",\"N\",\"N\",\"N\",\"N\",\"S\",\"N\",\"S\",\"S\",\"S\",\"S\",\"N\",\"N\",\"N\",\"N\"))\nFloridaLakes &lt;- FloridaLakes %&gt;% rename(Mercury = AvgMercury)\n\nOur data come from a sample of 53 lakes, out of more then 30,000 in the entire state of Florida. The mercury levels of the 53 lakes in the sample are shown in the table below.\n\nprint.data.frame(data.frame(FloridaLakes%&gt;% select(Lake, Location, Mercury)), row.names = FALSE)\n\n              Lake Location Mercury\n         Alligator        S    1.23\n             Annie        S    1.33\n            Apopka        N    0.04\n      Blue Cypress        S    0.44\n             Brick        S    1.20\n            Bryant        N    0.27\n            Cherry        N    0.48\n          Crescent        N    0.19\n        Deer Point        N    0.83\n              Dias        N    0.81\n              Dorr        N    0.71\n              Down        S    0.50\n             Eaton        N    0.49\n East Tohopekaliga        S    1.16\n           Farm-13        N    0.05\n            George        N    0.15\n           Griffin        N    0.19\n            Harney        N    0.77\n              Hart        S    1.08\n        Hatchineha        S    0.98\n           Iamonia        N    0.63\n         Istokpoga        S    0.56\n           Jackson        N    0.41\n         Josephine        S    0.73\n          Kingsley        N    0.34\n         Kissimmee        S    0.59\n         Lochloosa        N    0.34\n            Louisa        S    0.84\n        Miccasukee        N    0.50\n          Minneola        N    0.34\n            Monroe        N    0.28\n           Newmans        N    0.34\n        Ocean Pond        N    0.87\n      Ocheese Pond        N    0.56\n        Okeechobee        S    0.17\n            Orange        N    0.18\n       Panasoffkee        N    0.19\n            Parker        S    0.04\n            Placid        S    0.49\n            Puzzle        N    1.10\n            Rodman        N    0.16\n          Rousseau        N    0.10\n           Sampson        N    0.48\n             Shipp        S    0.21\n           Talquin        N    0.86\n            Tarpon        S    0.52\n      Tohopekaliga        S    0.65\n          Trafford        S    0.27\n             Trout        S    0.94\n      Tsala Apopka        N    0.40\n              Weir        N    0.43\n           Wildcat        N    0.25\n              Yale        N    0.27\n\n\nThe histogram shows the distribution of mercury levels in the 53 lakes in the sample. Lakes exceeding the US standard of 1 ppm are shown in red.\n\nLakes_Hist &lt;- ggplot(data=FloridaLakes, aes(x=Mercury)) + \n  geom_histogram(aes(fill=Mercury&lt;=1), color=\"white\", binwidth = 0.1) + \n  ggtitle(\"Mercury Levels in Sample of 53 Florida Lakes\") + \n  xlab(\"Mercury Level\") + ylab(\"Frequency\") + theme_bw()\nLakes_Hist\n\n\n\n\n\n\n\n\nThe proportion of lakes with mercury levels exceeding 1 ppm is calculated below.\n\np_hat &lt;- sum(FloridaLakes$Mercury &gt; 1)/53\np_hat\n\n[1] 0.1132075\n\n\nWe see that in our sample of 53 lakes, approximately 11% have mercury levels exceeding the US standard of 1 ppm. Suppose we want to estimate the proportion of all Florida Lakes whose mercury level exceeds this standard. As we saw in the previous section, we would not expect the population proportion to exactly match the sample, due to random variability between samples. We can use the sample proportion as an estimate (\\(\\hat{p} = 0.1132\\)), and construct a confidence interval for the unknown population proportion \\(p\\).\nIn order to construct the confidence interval, we need to know how much the sample proportion of lakes exceeding 1 ppm \\(\\hat{p}\\) could vary between different samples of size 53. That is, we need to know the standard error of \\(\\hat{p}\\). In the previous section, we calculated the standard error by taking 10,000 different samples of the same size as ours from the population, calculating the proportion for each sample, and then calculating the standard deviation of the proportions obtained from these 10,000 different samples. This procedure will not work here, however, because unlike the previous example where we really did have data on the entire population of all flights from New York to Chicago, we do not have data on all 30,000+ lakes in Florida. We cannot take a lot of different samples of size 53 from the population of all lakes, and thus, cannot obtain the sampling distribution for the the proportion of lakes exceeding 1 ppm, or estimate the standard error of \\(\\hat{p}\\).\n\n\n3.3.2 Bootstrap Sampling\nAll we have is a single sample of 53 lakes. We need to figure out how much the proportion of lakes with mercury levels exceeding 1 ppm would vary between different samples of size 53, using only the information contained in our one sample.\nTo do this, we’ll implement a popular simulation-based strategy, known as bootstrapping.\nLet’s assume our sample is representative of all Florida lakes. Then, we’ll duplicate the sample many times to create a large set that will look like the population of all Florida Lakes. We can then draw samples of 53 from that large population, and record the mean mercury level for each sample of 53.\nAn illustration of the bootstrapping procedure is shown below, using a sample of 12 colored dots, instead of the 53 lakes.\n\n\n\n\n\n\n\n\n\nIn fact, duplicating the sample many times and selecting new samples of size \\(n\\) has the same effect as drawing samples of size \\(n\\) from the original sample, by putting the item drawn back in each time, a procedure called sampling with replacement. Thus, we can skip the step of copying/pasting the sample many times, and instead draw our samples with replacement.\nThis means that in each new sample, some lakes will be drawn multiple times and others not at all. It also ensures that each sample is different, allowing us to estimate variability in the sample mean between the different samples of size 53.\nAn illustration of the concept of bootstrapping, using sampling with replacement is shown below.\n\n\n\n\n\n\n\n\n\nThe variability in sample means in our newly drawn samples is used to approximate the variability in proportion \\(\\hat{p}\\) that would occur between different samples of 53 lakes, drawn from the population of all Florida Lakes.\nThe point of bootstrapping is to observe how much a statistic (in this case the proportion of lakes with Mercury levels exceeding 1 ppm) varies between bootstrap samples. This can act as an estimate of how much that statistic would vary between different samples of size \\(n\\), drawn from the population.\nThe steps of bootstrap sampling can be summarized in the following algorithm.\nBootstrap Algorithm\nFor an original sample of size \\(n\\):\n\nTake a sample size \\(n\\) by randomly sampling from the original, with replacement. Thus, some observations will show up multiple times, and others not at all. This sample is called a bootstrap sample.\nCalculate the statistic of interest in the bootstrap sample (in this case \\(\\hat{p}\\), the proportion of lakes whose mercury levels exceed 1 ppm).\nRepeat steps 1 and 2 many (say 10,000) times, keeping track of the statistic of interest that is calculated in each bootstrap sample.\nLook at the distribution of the statistic across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the statistic of interest.\n\n\n\n3.3.3 Bootstrap Samples of Lakes\nThe sample_n() function samples the specified number rows from a data frame, with or without replacement.\nThe lakes in the first sample are shown below. Notice that some lakes occur multiple times, and others not at all.\nBootstrap Sample 1\n\nBootstrapSample1 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake)\nBootstrapSample1 %&gt;% select(ID, Lake, Mercury) |&gt; kable()\n\n\n\n\nID\nLake\nMercury\n\n\n\n\n3\nApopka\n0.04\n\n\n4\nBlue Cypress\n0.44\n\n\n4\nBlue Cypress\n0.44\n\n\n5\nBrick\n1.20\n\n\n5\nBrick\n1.20\n\n\n5\nBrick\n1.20\n\n\n6\nBryant\n0.27\n\n\n6\nBryant\n0.27\n\n\n7\nCherry\n0.48\n\n\n10\nDias\n0.81\n\n\n10\nDias\n0.81\n\n\n10\nDias\n0.81\n\n\n13\nEaton\n0.49\n\n\n17\nGriffin\n0.19\n\n\n18\nHarney\n0.77\n\n\n18\nHarney\n0.77\n\n\n19\nHart\n1.08\n\n\n20\nHatchineha\n0.98\n\n\n22\nIstokpoga\n0.56\n\n\n22\nIstokpoga\n0.56\n\n\n23\nJackson\n0.41\n\n\n25\nKingsley\n0.34\n\n\n25\nKingsley\n0.34\n\n\n26\nKissimmee\n0.59\n\n\n27\nLochloosa\n0.34\n\n\n30\nMinneola\n0.34\n\n\n32\nNewmans\n0.34\n\n\n33\nOcean Pond\n0.87\n\n\n34\nOcheese Pond\n0.56\n\n\n35\nOkeechobee\n0.17\n\n\n37\nPanasoffkee\n0.19\n\n\n37\nPanasoffkee\n0.19\n\n\n38\nParker\n0.04\n\n\n38\nParker\n0.04\n\n\n40\nPuzzle\n1.10\n\n\n40\nPuzzle\n1.10\n\n\n40\nPuzzle\n1.10\n\n\n41\nRodman\n0.16\n\n\n41\nRodman\n0.16\n\n\n42\nRousseau\n0.10\n\n\n43\nSampson\n0.48\n\n\n43\nSampson\n0.48\n\n\n44\nShipp\n0.21\n\n\n44\nShipp\n0.21\n\n\n45\nTalquin\n0.86\n\n\n51\nTohopekaliga\n0.65\n\n\n51\nTohopekaliga\n0.65\n\n\n51\nTohopekaliga\n0.65\n\n\n48\nTrout\n0.94\n\n\n48\nTrout\n0.94\n\n\n52\nWildcat\n0.25\n\n\n52\nWildcat\n0.25\n\n\n53\nYale\n0.27\n\n\n\n\n\nWe calculate the proportion of lakes with mercury levels exceeding 1 ppm in this bootstrap sample. Note that if a lake shows up more than once in the bootstrap sample, then it is counted however many times it shows up.\n\nsum(BootstrapSample1$Mercury &gt; 1) / 53\n\n[1] 0.1320755\n\n\nBootstrap Sample #2\nWe take a second bootstrap sample. We display only the first 10 lakes, though the bootstrap sample still has 53 lakes.\nNotice that the lakes chosen and omitted differ from the first sample.\n\nBootstrapSample2 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake)\nBootstrapSample2 %&gt;% select(ID, Lake, Mercury)\n\n# A tibble: 53 × 3\n      ID Lake         Mercury\n   &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1     1 Alligator       1.23\n 2     3 Apopka          0.04\n 3     4 Blue Cypress    0.44\n 4     5 Brick           1.2 \n 5     6 Bryant          0.27\n 6     7 Cherry          0.48\n 7     9 Deer Point      0.83\n 8    10 Dias            0.81\n 9    10 Dias            0.81\n10    11 Dorr            0.71\n# ℹ 43 more rows\n\n\nProportion exceeding 1 ppm:\n\nsum(BootstrapSample2$Mercury &gt; 1) / 53\n\n[1] 0.0754717\n\n\nBootstrap Sample #3\nWe’ll take one more bootstrap sample and calculate the proportion of lakes with mercury levels exceeding 1 ppm. The first 10 lakes in this third sample are shown.\n\nBootstrapSample3 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake)\nBootstrapSample3 %&gt;% select(ID, Lake, Mercury)\n\n# A tibble: 53 × 3\n      ID Lake       Mercury\n   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1     1 Alligator     1.23\n 2     3 Apopka        0.04\n 3     3 Apopka        0.04\n 4     5 Brick         1.2 \n 5     5 Brick         1.2 \n 6     7 Cherry        0.48\n 7     7 Cherry        0.48\n 8     8 Crescent      0.19\n 9     9 Deer Point    0.83\n10    10 Dias          0.81\n# ℹ 43 more rows\n\n\nProportion exceeding 1 ppm:\n\nsum(BootstrapSample3$Mercury &gt; 1) / 53\n\n[1] 0.1132075\n\n\n\n\n3.3.4 Bootstrap Distribution\nNow that we have seen how bootstrap sampling works, we’ll take a large number (10,000) different bootstrap samples and examine how the proportion of lakes with mercury levels exceeding 1 ppm varies between samples.\nWe’ll use a for-loop to take many different bootstrap samples and record the observed proportion in a vector called p_hat_b\n\np_hat &lt;- sum(FloridaLakes$Mercury &gt; 1)/53 #calculate sample statistic\nBootstrap_prop &lt;- rep(NA, 10000)   #setup vector to hold bootstrap statistics\n\nfor (i in 1:10000){\nBootstrapSample &lt;- sample_n(FloridaLakes, 53, replace=TRUE) #take bootstrap sample\nBootstrap_prop[i] &lt;- sum(BootstrapSample$Mercury &gt; 1)/53 # calc. prop exceeding 1\n}\nLakes_Bootstrap_Prop &lt;- data.frame(Bootstrap_prop)  #store values in a dataframe\n\nThe distribution of proportions observed in the 10,000 different bootstrap samples is shown below. This distribution is called the bootstrap distribution.\n\nLakes_Bootstrap_Prop_plot &lt;- ggplot(data=Lakes_Bootstrap_Prop, aes(x=Bootstrap_prop)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\", binwidth=0.02) +\n  xlab(\"Prop &gt; 1 in Bootstrap Sample \") + ylab(\"Frequency\") +\n  ggtitle(\"Bootstrap Distribution for Prop. of Lakes Exeeding 1 ppm Hg\") + \n  theme(legend.position = \"none\")\nLakes_Bootstrap_Prop_plot\n\n\n\n\n\n\n\n\nThe bootstrap distribution is meant to approximate the sampling distribution of the statistic of interest (in this case the proportion exceeding 1 ppm). Because it is based on the sample, the bootstrap distribution will be centered at the sample statistic (\\(\\hat{p}\\) in this case) while the sampling distribution would have been centered at the population parameter (\\(p\\)), which is unknown. The important things, however, is that the variability in the bootstrap distribution gives a good approximation of the amount of variability in the sampling distribution, so we can use the standard deviation of the bootstrap distribution (called bootstrap standard error) in our confidence interval calculation.\n\n\n3.3.5 Bootstrap SE Confidence Interval\nWe calculate the standard deviation of this bootstrap distribution, which is an estimate of the standard error of \\(\\hat{p}\\). It measures how much the proportion of lakes exceeding 1 ppm varies between samples of size 53.\nBootstrap Standard Error:\n\nSE_p_hat &lt;- sd(Lakes_Bootstrap_Prop$Bootstrap_prop)\n\nSince the bootstrap distribution is roughly symmetric and bell-shaped, we can calculate a 95% confidence interval for the proportion of all Florida lakes with mercury levels exceeding 1 ppm, using bootstrap standard error confidence interval method.\n\\[\n\\hat{p} \\pm 2\\times\\text{SE}(\\hat{p})\n\\]\n\nc(p_hat - 2*SE_p_hat, p_hat + 2*SE_p_hat)\n\n[1] 0.02684273 0.19957237\n\n\nThe gold bar at the bottom of the bootstrap distribution represents this 95% confidence interval.\n\nLakes_Bootstrap_Prop_plot + \n  geom_segment(aes(x=p_hat - 2*SE_p_hat,xend=p_hat + 2*SE_p_hat, y=50, yend=50),\n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe are 95% confident that the proportion of all Florida lakes with mercury levels exceeding 1 ppm is between 0.0268 and 0.1996.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch3.html#more-bootstrap-examples",
    "href": "Ch3.html#more-bootstrap-examples",
    "title": "3  Simulation-Based Inference",
    "section": "3.4 More Bootstrap Examples",
    "text": "3.4 More Bootstrap Examples\n\n3.4.1 Bootstrapping Other Statistics\nWe’ve seen how to use bootstrapping to calculate confidence intervals for an unknown population parameter \\(p\\), using an estimate \\(\\hat{p}\\), calculated from a sample of size \\(n\\). This procedure can be applied to calculate confidence intervals for a wide range of population parameters, using statistics calculated from a sample.\nFor example, we could calculate confidence intervals any of the following parameters, using the corresponding sample statistic.\n\n\n\n\n\n\n\n\nContext\nParameter\nStatistic\n\n\n\n\nProportion\n\\(p\\)\n\\(\\hat{p}\\)\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard Deviation\n\\(\\sigma\\)\n\\(s\\)\n\n\nMedian\nno common abbreviations\n\n\n\nDifference in Means\n\\(\\mu_2-\\mu_1\\)\n\\(\\bar{x}_2 - \\bar{x}_1\\)\n\n\nRegression Coefficient\n\\(\\beta_j\\)\n\\(b_j\\)\n\n\nEstimated Regression Response\n\\(\\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_px_{ip}\\)\n\\(b_0 + b_1x_{i1} + \\ldots + b_px_{ip}\\)\n\n\n\nWe follow the same algorithm as we did when working with a proportion, and simply calculate whatever statistic we are interested in step 2, in place of \\(\\hat{p}\\), as we did previously.\nThe bootstrap algorithm is given again, below.\nBootstrap Algorithm\nFor an original sample of size \\(n\\):\n\nTake a sample of size \\(n\\) by randomly sampling from the original sample with replacement. (Thus some observations will show up multiple times and others not at all.)\nCalculate the statistic of interest in the bootstrap sample.\nRepeat steps 1 and 2 many (say 10,000) times, keeping track of the statistic of interest that is calculated in each bootstrap sample.\nLook at the distribution of the statistic across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the statistic of interest.\n\nWe’ll now go through examples, calculating bootstrap confidence intervals for each of the parameters listed above.\n\n\n3.4.2 CI for Mean\nThe histogram shows the distribution of mercury levels of the 53 lakes in our sample. The mean and standard deviation in mercury levels for these 53 lakes is shown.\n\nLakes_Hist &lt;- ggplot(data=FloridaLakes, aes(x=Mercury)) + \n  geom_histogram(color=\"white\", fill=\"lightblue\", binwidth = 0.2) + \n  ggtitle(\"Mercury Levels in Sample of Florida Lakes\") + \n  xlab(\"Mercury Level\") + ylab(\"Frequency\") \nLakes_Hist\n\n\n\n\n\n\n\n\nWe’ll calculate the mean and median mercury level for the 53 lakes in the sample.\n\nLakes_Stats &lt;- FloridaLakes %&gt;% summarize(MeanHg = mean(Mercury), \n                           StDevHG = sd(Mercury),\n                           N=n())\nkable(Lakes_Stats)\n\n\n\n\nMeanHg\nStDevHG\nN\n\n\n\n\n0.5271698\n0.3410356\n53\n\n\n\n\n\nWe want to calculate a 95% confidence interval for the mean mercury level among all Florida lakes. We’ll use bootstrapping again, this time using the sample mean, rather than the proportion exceeding 1 ppm, as our statistic of interest.\nBootstrap Steps\n\nTake a sample of 53 lakes by randomly sampling from the original sample of 53 lakes, with replacement.\nCalculate the mean mercury level in the bootstrap sample.\nRepeat steps 1 and 2 many (say 10,000) times, keeping track of the mean mercury level in each bootstrap sample.\nLook at the distribution of the mean across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the mean mercury level.\n\nWe’ll illustrate the procedure on 3 bootstrap samples.\nBootstrap Sample 1\nThe first 10 lakes in the bootstrap sample are shown below. Notice again that some lakes occur multiple times, and others not at all.\n\nBootstrapSample1 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake)\nBootstrapSample1 %&gt;% select(ID, Lake, Mercury)\n\n# A tibble: 53 × 3\n      ID Lake       Mercury\n   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1     1 Alligator     1.23\n 2     2 Annie         1.33\n 3     2 Annie         1.33\n 4     3 Apopka        0.04\n 5     3 Apopka        0.04\n 6     3 Apopka        0.04\n 7     3 Apopka        0.04\n 8     6 Bryant        0.27\n 9     9 Deer Point    0.83\n10     9 Deer Point    0.83\n# ℹ 43 more rows\n\n\nWe calculate the mean mercury level among the lakes in the bootstrap sample.\n\nmean(BootstrapSample1$Mercury)\n\n[1] 0.4790566\n\n\nBootstrap Sample #2\n\nBootstrapSample2 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake)\nBootstrapSample2 %&gt;% select(ID, Lake, Mercury)\n\n# A tibble: 53 × 3\n      ID Lake       Mercury\n   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1     1 Alligator     1.23\n 2     1 Alligator     1.23\n 3     1 Alligator     1.23\n 4     2 Annie         1.33\n 5     5 Brick         1.2 \n 6     6 Bryant        0.27\n 7     8 Crescent      0.19\n 8     8 Crescent      0.19\n 9     9 Deer Point    0.83\n10     9 Deer Point    0.83\n# ℹ 43 more rows\n\n\nMean Mercury Level:\n\nmean(BootstrapSample2$Mercury)\n\n[1] 0.5479245\n\n\nBootstrap Sample #3\n\nBootstrapSample3 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake)\nBootstrapSample3 %&gt;% select(ID, Lake, Mercury)\n\n# A tibble: 53 × 3\n      ID Lake         Mercury\n   &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1     1 Alligator       1.23\n 2     3 Apopka          0.04\n 3     3 Apopka          0.04\n 4     4 Blue Cypress    0.44\n 5     5 Brick           1.2 \n 6     5 Brick           1.2 \n 7     6 Bryant          0.27\n 8     7 Cherry          0.48\n 9     8 Crescent        0.19\n10     9 Deer Point      0.83\n# ℹ 43 more rows\n\n\nMean Mercury Level:\n\nmean(BootstrapSample3$Mercury)\n\n[1] 0.5437736\n\n\nNow, we’ll take 10,000 bootstrap samples, and record the mean mercury concentration in each sample.\n\nmean &lt;- mean(FloridaLakes$Mercury)  #calculate sample statistic\nBootstrap_Mean &lt;- rep(NA, 10000) # setup vector to hold bootstrap statistics\n\nfor (i in 1:10000){\nBootstrapSample &lt;- sample_n(FloridaLakes, 53, replace=TRUE) # take bootstrap sample\nBootstrap_Mean[i] &lt;- mean(BootstrapSample$Mercury) # calculate mean in bootstrap sample\n}\nLakes_Bootstrap_Results_Mean &lt;- data.frame(Bootstrap_Mean)  #store results in data frame\n\nThe bootstrap distribution for the mean mercury level is shown below, along with its standard error.\n\nLakes_Bootstrap_Mean_Plot &lt;- ggplot(data=Lakes_Bootstrap_Results_Mean, \n                                    aes(x=Bootstrap_Mean)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\") +\n  xlab(\"Mean Mercury in Bootstrap Sample \") + ylab(\"Frequency\") +\n  ggtitle(\"Bootstrap Distribution for Sample Mean in Florida Lakes\") + \n  theme(legend.position = \"none\") \nLakes_Bootstrap_Mean_Plot \n\n\n\n\n\n\n\n\nBootstrap Standard Error\nWe’ll calculate the bootstrap standard error of the mean. This is a measure of how much the mean varies between samples of size 53.\n\nSE_mean &lt;- sd(Lakes_Bootstrap_Results_Mean$Bootstrap_Mean)\nSE_mean\n\n[1] 0.04595372\n\n\nNotice that the standard error of the mean is much less than the sample standard deviation of 0.341.\nInterpretations of sample standard deviation and standard error of the mean\n\nThe sample standard deviation measures the amount of variability in mercury levels between the 53 individual lakes in our sample.\nThe standard error of the mean measures the amount of variability in sample mean mercury levels between different samples of size 53.\n\nThere is more variability between mercury levels in individual lakes than there is between average mercury levels in different samples of size 53.\nSince the bootstrap distribution is roughly symmetric and bell-shaped, we can use the bootstrap standard error method to calculate an approximate 95% confidence interval for the mean mercury level among all Florida lakes.\n\\[\n\\text{Statistic} \\pm 2\\times\\text{Standard Error}\n\\]\nIn this case, the statistic of interest is the sample mean \\(\\bar{x}=0.527\\). The confidence interval is\n\\[\n\\begin{aligned}\n& \\bar{x} \\pm 2\\times\\text{SE}(\\bar{x}) \\\\\n& = 0.527 \\pm 2\\times\\text{0.0459537}\n\\end{aligned}\n\\]\n95% Confidence Interval:\n\nc(mean - 2*SE_mean, mean + 2*SE_mean) \n\n[1] 0.4352624 0.6190773\n\n\nThe 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below.\n\nLakes_Bootstrap_Mean_Plot + \n  geom_segment(aes(x=mean - 2*SE_mean,xend=mean + 2*SE_mean, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe are 95% confident that the average mercury level among all Florida lakes is between 0.44 and 0.62 parts per million.\nIt is important to note that we are not saying that we are 95% confident that an individual lake lie in this range, or that 95% of all individual lakes lie in this range. We are only saying that we are confident that the average mercury level among all lakes lies in this range. A confidence interval is a statement about a population parameter (in this case the average mercury level), rather than about individual lakes in the population. Since there is more variability about individual lakes than overall averages, we’ll need to make a wider interval when talking about the mercury level for an individual lake.\n\n\n3.4.3 CI for Standard Deviation\nNow, we’ll calculate a confidence interval for the standard deviation in mercury levels among all Florida lakes. Recall that the sample standard deviation (\\(s\\)) was:\n\nSample_SD &lt;- sd(FloridaLakes$Mercury)\nSample_SD\n\n[1] 0.3410356\n\n\nWe’ll use this estimate to calculate a confidence interval for the population standard deviation \\(\\sigma\\).\nThis time, our statistic of interest is the sample standard deviation \\(s\\).\nBootstrap Steps\n\nTake a sample of 53 lakes by randomly sampling from the original sample of 53 lakes, with replacement.\nCalculate the standard deviation in mercury level in the bootstrap sample.\nRepeat steps 1 and 2 many (say 10,000) times, keeping track of the standard deviation mercury level in each bootstrap sample.\nLook at the distribution of the standard deviations across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the standard deviation in mercury level.\n\nWe’ll illustrate the procedure on 3 bootstrap samples.\nBootstrap Sample 1\n\nBootstrapSample1 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake)\nBootstrapSample1 %&gt;% select(ID, Lake, Mercury)\n\n# A tibble: 53 × 3\n      ID Lake         Mercury\n   &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1     1 Alligator       1.23\n 2     3 Apopka          0.04\n 3     4 Blue Cypress    0.44\n 4     5 Brick           1.2 \n 5     6 Bryant          0.27\n 6     6 Bryant          0.27\n 7     6 Bryant          0.27\n 8     6 Bryant          0.27\n 9     7 Cherry          0.48\n10     8 Crescent        0.19\n# ℹ 43 more rows\n\n\nWe calculate the standard deviation in mercury levels among the lakes in the bootstrap sample.\n\nsd(BootstrapSample1$Mercury)\n\n[1] 0.3157289\n\n\nBootstrap Sample #2\n\nBootstrapSample2 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake)\nBootstrapSample2 %&gt;% select(ID, Lake, Mercury)\n\n# A tibble: 53 × 3\n      ID Lake         Mercury\n   &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1     1 Alligator       1.23\n 2     1 Alligator       1.23\n 3     3 Apopka          0.04\n 4     4 Blue Cypress    0.44\n 5     5 Brick           1.2 \n 6     5 Brick           1.2 \n 7     6 Bryant          0.27\n 8     7 Cherry          0.48\n 9     7 Cherry          0.48\n10     8 Crescent        0.19\n# ℹ 43 more rows\n\n\nStandard Deviation in Mercury Level:\n\nsd(BootstrapSample2$Mercury)\n\n[1] 0.3466327\n\n\nBootstrap Sample #3\n\nBootstrapSample3 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake)\nBootstrapSample3 %&gt;% select(ID, Lake, Mercury)\n\n# A tibble: 53 × 3\n      ID Lake       Mercury\n   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1     2 Annie         1.33\n 2     3 Apopka        0.04\n 3     5 Brick         1.2 \n 4     6 Bryant        0.27\n 5     6 Bryant        0.27\n 6     6 Bryant        0.27\n 7     7 Cherry        0.48\n 8     8 Crescent      0.19\n 9     9 Deer Point    0.83\n10     9 Deer Point    0.83\n# ℹ 43 more rows\n\n\nStandard Deviation Mercury Level:\n\nsd(BootstrapSample3$Mercury)\n\n[1] 0.3294457\n\n\nNow, we’ll take 10,000 bootstrap samples, and record the standard deviation in mercury concentration in each sample.\n\nSample_SD &lt;- sd(FloridaLakes$Mercury)  #calculate sample statistic\nBootstrap_SD &lt;- rep(NA, 10000) # setup vector to hold bootstrap statistics\n\nfor (i in 1:10000){\nBootstrapSample &lt;- sample_n(FloridaLakes, 53, replace=TRUE) # take bootstrap sample\nBootstrap_SD[i] &lt;- sd(BootstrapSample$Mercury) # calculate standard deviation in bootstrap sample\n}\nLakes_Bootstrap_Results_SD &lt;- data.frame(Bootstrap_SD)  #store results in data frame\n\nThe bootstrap distribution for the mean mercury level is shown below, along with its standard error.\n\nLakes_Bootstrap_SD_Plot &lt;- ggplot(data=Lakes_Bootstrap_Results_SD, \n                                    aes(x=Bootstrap_SD)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\") +\n  xlab(\"SD in Mercury in Bootstrap Sample \") + ylab(\"Frequency\") +\n  ggtitle(\"Bootstrap Distribution for Sample SD in Florida Lakes\") + \n  theme(legend.position = \"none\") \nLakes_Bootstrap_SD_Plot \n\n\n\n\n\n\n\n\nBootstrap Standard Error:\nWe’ll calculate the bootstrap standard error of the standard deviation. This is a measure of how much the standard deviation varies between samples.\n\nSE_SD &lt;- sd(Lakes_Bootstrap_Results_SD$Bootstrap_SD)\nSE_SD\n\n[1] 0.02867548\n\n\nSince the bootstrap distribution is roughly symmetric and bell-shaped, we can use the bootstrap standard error method to calculate an approximate 95% confidence interval for the standard deviation in mercury levels among all Florida lakes.\n\\[\n\\text{Statistic} \\pm 2\\times\\text{Standard Error}\n\\]\nIn this case, the statistic of interest is the sample standard deviation \\(s=0.341\\). The confidence interval is\n\\[\n\\begin{aligned}\n& s \\pm 2\\times\\text{SE}(s) \\\\\n& = 0.341 \\pm 2\\times{0.029}\n\\end{aligned}\n\\]\n95% Confidence Interval:\n\nc(Sample_SD - 2*SE_SD, Sample_SD + 2*SE_SD        ) \n\n[1] 0.2836847 0.3983866\n\n\nThe 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below.\n\nLakes_Bootstrap_SD_Plot + \n  geom_segment(aes(x=Sample_SD - 2*SE_SD,xend=Sample_SD + 2*SE_SD, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe are 95% confident that the standard deviation in mercury levels among all Florida lakes is between 0.28 and 0.4 parts per million.\n\n\n3.4.4 CI for Median\nWe already calculated a confidence interval for the mean mercury level among all Florida lakes. We could calculate a bootstrap confidence interval for the median mercury level as well, but since the distribution of mercury levels in the lakes is roughly symmetric, the mean is a reasonable measure of center, and there is not a clear reason for using the median instead.\nWhen a distribution is skewed or contains large outliers, however, the median is a more robust measure of center than the mean. Recall the distribution of 100 Seattle house prices seen in Chapters 1 and 2.\n\nggplot(data=Houses, aes(x=price)) + \n  geom_histogram(fill=\"lightblue\", color=\"white\") + \n  ggtitle(\"Distribution of House Prices\") +\n  xlab(\"Price\") + \n  ylab(\"Frequency\")\n\n\n\n\n\n\n\n\nThese 100 houses are a sample of all houses sold in Seattle in 2014 and 2015, so we can use statistics from our sample to draw conclusions about all houses sold in Seattle in this time period.\nIn this subsection, we’ll use bootstrapping to calculate a 95% confidence interval for the median price among all houses sold in Seattle in this time period.\nWe calculate the sample median price.\n\nSample_Median &lt;- median(Houses$price)\nSample_Median\n\n[1] 507.5\n\n\nBootstrap Steps\n\nTake a sample of 100 houses by randomly sampling from the original sample of 100 houses, with replacement.\nCalculate the median price in the bootstrap sample.\nRepeat steps 1 and 2 many (say 10,000) times, keeping track of the median price in each bootstrap sample.\nLook at the distribution of the median price across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the median price.\n\nWe’ll illustrate the procedure on 3 bootstrap samples.\nBootstrap Sample 1\n\nBootstrapSample1 &lt;- sample_n(Houses, 100, replace=TRUE) %&gt;% arrange(Id)\nBootstrapSample1 %&gt;% select(Id, price)\n\n# A tibble: 100 × 2\n      Id price\n   &lt;int&gt; &lt;dbl&gt;\n 1     2  885.\n 2     3  385.\n 3     4  253.\n 4     4  253.\n 5     5  468.\n 6     6  310.\n 7     8  485.\n 8     9  315.\n 9     9  315.\n10    10  425 \n# ℹ 90 more rows\n\n\nWe calculate the median price among the houses in the bootstrap sample.\n\nmedian(BootstrapSample1$price)\n\n[1] 400\n\n\nBootstrap Sample #2\n\nBootstrapSample2 &lt;- sample_n(Houses, 100, replace=TRUE) %&gt;% arrange(Id)\nBootstrapSample2 %&gt;% select(Id, price)\n\n# A tibble: 100 × 2\n      Id price\n   &lt;int&gt; &lt;dbl&gt;\n 1     1 1225 \n 2     3  385.\n 3     4  253.\n 4     4  253.\n 5     4  253.\n 6     6  310.\n 7     7  550.\n 8     8  485.\n 9     8  485.\n10     9  315.\n# ℹ 90 more rows\n\n\nMedian Price:\n\nmedian(BootstrapSample2$price)\n\n[1] 427.5\n\n\nBootstrap Sample #3\n\nBootstrapSample3 &lt;- sample_n(Houses, 100, replace=TRUE) %&gt;% arrange(Id)\nBootstrapSample3 %&gt;% select(Id, price)\n\n# A tibble: 100 × 2\n      Id price\n   &lt;int&gt; &lt;dbl&gt;\n 1     2  885.\n 2     4  253.\n 3     5  468.\n 4     7  550.\n 5    10  425 \n 6    13 1325 \n 7    16 3075 \n 8    17  438 \n 9    18  688.\n10    19  995.\n# ℹ 90 more rows\n\n\nMedian Price:\n\nmedian(BootstrapSample3$price)\n\n[1] 488\n\n\nNow, we’ll take 10,000 bootstrap samples, and record the median price in each sample.\n\nSample_Med &lt;- median(Houses$price)  #calculate sample median\nBootstrap_Med &lt;- rep(NA, 10000) # setup vector to hold bootstrap statistics\n\nfor (i in 1:10000){\nBootstrapSample &lt;- sample_n(Houses, 100, replace=TRUE) # take bootstrap sample\nBootstrap_Med[i] &lt;- median(BootstrapSample$price) # calculate standard deviation in bootstrap sample\n}\nHouses_Bootstrap_Results_Med &lt;- data.frame(Bootstrap_Med)  #store results in data frame\n\nThe bootstrap distribution for the median price is shown below, along with its standard error.\n\nHouses_Bootstrap_Med_Plot &lt;- ggplot(data=Houses_Bootstrap_Results_Med, \n                                    aes(x=Bootstrap_Med)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\") +\n  xlab(\"Median Price in Bootstrap Sample \") + ylab(\"Frequency\") +\n  ggtitle(\"Bootstrap Distribution for Median Price in Seattle Houses\") + \n  theme(legend.position = \"none\") \nHouses_Bootstrap_Med_Plot \n\n\n\n\n\n\n\n\nBootstrap Standard Error:\nWe’ll calculate the bootstrap standard error of the median. This is a measure of how much the median varies between samples.\n\nSE_Med &lt;- sd(Houses_Bootstrap_Results_Med$Bootstrap_Med)\nSE_Med\n\n[1] 48.11411\n\n\nThe standard error measures the amount of variability in median house price between different samples of size 100.\nNote that this is different than the sample standard deviation, which represents the standard deviation in prices between the 100 different houses in the sample.\nNotice that the bootstrap distribution for the median is not symmetric and bell-shaped. Thus, we cannot be assured that 95% of samples will produce a statistic within two standard errors of the mean, so the standard error confidence interval method is not appropriate here. Instead, we’ll calculate a confidence interval by taking the middle 95% of the values in the bootstrap distribution. A confidence interval calculated this way is called a percentile bootstrap interval.\nWe’ll calculate the 0.025 quantile and the 0.975 quantile of this bootstrap distribution. These are the points below which lie 2.5% and 97.5% of the medians in the bootstrap distribution. Thus, the middle 95% of the medians lie between these values.\n\nq.025 &lt;- quantile(Houses_Bootstrap_Results_Med$Bootstrap_Med, 0.025)\nq.025\n\n2.5% \n 410 \n\n\n\nq.975 &lt;- quantile(Houses_Bootstrap_Results_Med$Bootstrap_Med, 0.975)\nq.975 \n\n 97.5% \n600.05 \n\n\nThe 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below.\n\nHouses_Bootstrap_Med_Plot + \n  geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe are 95% confident that the median price among all houses that sold in Seattle between 2014 and 2015 is between 410 and 600 thousand dollars.\n\n\n3.4.5 CI for Difference in Means\nWe previously calculated a confidence interval for the average mercury level among all lakes in Florida.\nNow, we’ll calculate an interval for the difference in average mercury levels between lakes in Northern Florida, compared to Southern Florida.\nThe boxplot shows and table below describe the distribution of mercury levels for lakes in Northern Florida, compared to Southern Florida.\n\nLakesBP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=Mercury, fill=Location)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + coord_flip()\nLakesBP\n\n\n\n\n\n\n\n\n\nLakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(Mercury), \n                                                  StDevHg=sd(Mercury), \n                                                  N=n())\nLakesTable\n\n# A tibble: 2 × 4\n  Location MeanHg StDevHg     N\n  &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1 N         0.425   0.270    33\n2 S         0.696   0.384    20\n\n\nIn our sample of 33 Northern Lakes and 20 Southern Lakes, we saw a difference of 0.27 ppm. We’ll calculate a confidence interval to estimate how big or small this difference could be among all Florida lakes.\nWe’ll use a statistical model to calculate the average mercury levels in Northern and Southern Florida.\n\\(\\widehat{\\text{Mercury}} = b_0 +b_1\\times{\\text{South}}\\)\n\n\\(b_0\\) represents the mean mercury level for lakes in North Florida, and\n\n\\(b_1\\) represents the mean difference in mercury level for lakes in South Florida, compared to North Florida\n\nThe estimates for corresponding to the original sample are shown below.\n\nM &lt;- lm(data=FloridaLakes, Mercury~Location)\nM\n\n\nCall:\nlm(formula = Mercury ~ Location, data = FloridaLakes)\n\nCoefficients:\n(Intercept)    LocationS  \n     0.4245       0.2720  \n\n\nThus, we can obtain a confidence interval for the difference in average mercury levels by fitting a regression model to each of our bootstrap samples and recording the value of the sample statistic \\(b_1\\), which represents this difference. Alternatively, we could calculate the mean from each group separately and subtract.\nWhen comparing groups, we make one modification in Step #1 of the bootstrap process. Rather than drawing a sample of size \\(n\\) at random, with replacement, we’ll draw the same number of observations from each group as were in the original sample. In this case, we had 33 northern lakes, and 20 southern lakes.\nBootstrap Steps\n\nTake a sample of 33 northern lakes and 20 southern lakes by randomly sampling from the original sample, with replacement.\nFit a regression model with location as the explanatory variable and record the value of \\(b_1\\), representing the difference between the means for each group (South-North).\nRepeat steps 1 and 2 many (say 10,000) times, keeping track of the difference in means in each bootstrap sample.\nLook at the distribution of the differences in means across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the difference in means between mercury levels in Northern and Southern Florida.\n\nWe’ll illustrate the procedure on 3 bootstrap samples.\nBootstrap Sample 1\n\nNLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==\"N\"), 33, replace=TRUE)   ## sample 33 northern lakes\nSLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==\"S\"), 20, replace=TRUE)   ## sample 20 southern lakes\nBootstrapSample1 &lt;- rbind(NLakes, SLakes) %&gt;% arrange(ID) %&gt;% \n  select(ID, Lake, Location, Mercury)   ## combine Northern and Southern Lakes\nBootstrapSample1\n\n# A tibble: 53 × 4\n      ID Lake         Location Mercury\n   &lt;int&gt; &lt;chr&gt;        &lt;fct&gt;      &lt;dbl&gt;\n 1     1 Alligator    S           1.23\n 2     2 Annie        S           1.33\n 3     2 Annie        S           1.33\n 4     3 Apopka       N           0.04\n 5     4 Blue Cypress S           0.44\n 6     4 Blue Cypress S           0.44\n 7     4 Blue Cypress S           0.44\n 8     7 Cherry       N           0.48\n 9     8 Crescent     N           0.19\n10    10 Dias         N           0.81\n# ℹ 43 more rows\n\n\nWe fit a regression model to the bootstrap sample and calculate the regression coefficients. We’re interested in the second coefficient, \\(b_1\\), which represents the mean difference between lakes in Southern and Northern Florida\n\nMb1 &lt;- lm(data=BootstrapSample1, Mercury ~ Location) ## fit linear model\nMb1\n\n\nCall:\nlm(formula = Mercury ~ Location, data = BootstrapSample1)\n\nCoefficients:\n(Intercept)    LocationS  \n     0.3894       0.2471  \n\n\n\nNLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==\"N\"), 33, replace=TRUE)   ## sample 33 northern lakes\nSLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==\"S\"), 20, replace=TRUE)   ## sample 20 southern lakes\nBootstrapSample2 &lt;- rbind(NLakes, SLakes) %&gt;% arrange(ID) %&gt;% \n  select(ID, Lake, Location, Mercury)   ## combine Northern and Southern Lakes\nBootstrapSample2\n\n# A tibble: 53 × 4\n      ID Lake         Location Mercury\n   &lt;int&gt; &lt;chr&gt;        &lt;fct&gt;      &lt;dbl&gt;\n 1     1 Alligator    S           1.23\n 2     1 Alligator    S           1.23\n 3     1 Alligator    S           1.23\n 4     1 Alligator    S           1.23\n 5     3 Apopka       N           0.04\n 6     3 Apopka       N           0.04\n 7     3 Apopka       N           0.04\n 8     4 Blue Cypress S           0.44\n 9     4 Blue Cypress S           0.44\n10     4 Blue Cypress S           0.44\n# ℹ 43 more rows\n\n\nBootstrap Sample 2\n\nMb2 &lt;- lm(data=BootstrapSample2, Mercury ~ Location) ## fit linear model\nMb2\n\n\nCall:\nlm(formula = Mercury ~ Location, data = BootstrapSample2)\n\nCoefficients:\n(Intercept)    LocationS  \n     0.4585       0.2755  \n\n\nBootstrap Sample 3\n\nNLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==\"N\"), 33, replace=TRUE)   ## sample 33 northern lakes\nSLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==\"S\"), 20, replace=TRUE)   ## sample 20 southern lakes\nBootstrapSample3 &lt;- rbind(NLakes, SLakes) %&gt;% arrange(ID) %&gt;% \n  select(ID, Lake, Location, Mercury)   ## combine Northern and Southern Lakes\nBootstrapSample3\n\n# A tibble: 53 × 4\n      ID Lake      Location Mercury\n   &lt;int&gt; &lt;chr&gt;     &lt;fct&gt;      &lt;dbl&gt;\n 1     1 Alligator S           1.23\n 2     1 Alligator S           1.23\n 3     2 Annie     S           1.33\n 4     2 Annie     S           1.33\n 5     2 Annie     S           1.33\n 6     2 Annie     S           1.33\n 7     3 Apopka    N           0.04\n 8     3 Apopka    N           0.04\n 9     6 Bryant    N           0.27\n10     6 Bryant    N           0.27\n# ℹ 43 more rows\n\n\n\nMb3 &lt;- lm(data=BootstrapSample3, Mercury ~ Location) ## fit linear model\nMb3\n\n\nCall:\nlm(formula = Mercury ~ Location, data = BootstrapSample3)\n\nCoefficients:\n(Intercept)    LocationS  \n     0.3342       0.4178  \n\n\nWe’ll now take 10,000 different bootstrap samples and look at the bootstrap distribution for \\(b_1\\), the difference in mean mercury levels between lakes in Southern and Northern Florida\n\nM &lt;- lm(data=FloridaLakes, Mercury~Location) #fit model to original sample\nSample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient)\nBootstrap_b1 &lt;- rep(NA, 10000)  #vector to store b1 values\n\nfor (i in 1:10000){\nNLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==\"N\"), 33, replace=TRUE)   ## sample 33 northern lakes\nSLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==\"S\"), 20, replace=TRUE)   ## sample 20 southern lakes\nBootstrapSample &lt;- rbind(NLakes, SLakes)   ## combine Northern and Southern Lakes\nM &lt;- lm(data=BootstrapSample, Mercury ~ Location) ## fit linear model\nBootstrap_b1[i] &lt;- M$coefficients[2] ## record b1 \n}\nNS_Lakes_Bootstrap_Results &lt;- data.frame(Bootstrap_b1)  #save results as dataframe\n\nThe bootstrap distribution for the difference in means, \\(b_1\\), is shown below, along with the standard error for the difference.\n\nNS_Lakes_Bootstrap_Plot_b1 &lt;- ggplot(data=NS_Lakes_Bootstrap_Results, aes(x=Bootstrap_b1)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\") + \n  xlab(\"Mean Difference (b1) in Bootstrap Sample\") + ylab(\"Frequency\") +\n  ggtitle(\"Northern vs Southern Lakes: Bootstrap Distribution for b1\") \nNS_Lakes_Bootstrap_Plot_b1\n\n\n\n\n\n\n\n\nBootstrap Standard Error:\nWe’ll calculate the bootstrap standard error of the difference in means \\(b_1\\). This is a measure of how much the difference in means varies between samples.\n\nSE_b1 &lt;- sd(NS_Lakes_Bootstrap_Results$Bootstrap_b1)\nSE_b1\n\n[1] 0.09549244\n\n\nThe bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval.\n\\[\n\\begin{aligned}\n& b_1 \\pm 2\\times\\text{SE}(b_1) \\\\\n& = 0.271 \\pm 2\\times{0.095}\n\\end{aligned}\n\\]\n95% Confidence Interval:\n\nc(Sample_b1 - 2*SE_b1, Sample_b1 + 2*SE_b1) \n\n LocationS  LocationS \n0.08096967 0.46293942 \n\n\nThe 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below.\n\nNS_Lakes_Bootstrap_Plot_b1 + \n  geom_segment(aes(x=Sample_b1 - 2*SE_b1,xend=Sample_b1 + 2*SE_b1, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe are 95% confident that the mean mercury level among all lakes in Southern Florida is between 0.08 and 0.46 higher than the mean mercury level among all lakes in Northern Florida.\n\n\n3.4.6 CI for Regression Slope\nNow, we’ll examine the relationship between mercury concentration and pH in Florida lakes. The scatterplot displays these variables, along with the least squares regression line.\n\nggplot(data=FloridaLakes, aes(y=Mercury, x=pH)) + \n  geom_point() + stat_smooth(method=\"lm\", se=FALSE)\n\n\n\n\n\n\n\n\nThe regression equation is\n\\[\n\\widehat{\\text{Mercury}} = b_0 + b_1\\times\\text{pH}\n\\]\nRegression estimates \\(b_0\\) and \\(b_1\\) are shown below.\n\nM &lt;- lm(data=FloridaLakes, Mercury~pH)\nM\n\n\nCall:\nlm(formula = Mercury ~ pH, data = FloridaLakes)\n\nCoefficients:\n(Intercept)           pH  \n     1.5309      -0.1523  \n\n\n\nOn average, lakes with pH level 0 are expected to have a mercury level of 1.53 ppm.\n\nFor each one-unit increase in pH, mercury level is expected to decrease by 0.15 ppm.\n\nThese estimates are sample statistics, calculated from our sample of 53 lakes. We can think of our regression equation estimates \\(b_0\\) and \\(b_1\\) as estimates of parameters \\(\\beta_0\\) and \\(\\beta_1\\), which pertain to the slope and intercept of the regression line pertaining to the entire population of all lakes in Florida. We’ll use \\(b_0\\) and \\(b_1\\) to estimate \\(\\beta_0\\) and \\(\\beta_1\\) in the same way that we used sample proportion \\(\\hat{p}\\) to estimate population proportion \\(p\\) and sample mean \\(\\bar{x}\\) to estimate population mean \\(\\mu\\).\nThe intercept, \\(\\beta_0\\) has little meaning here, but the slope \\(\\beta_1\\) represents the average change in mercury level for each one-unit increase in pH, among all Florida lakes. We’ll use bootstrapping to find a confidence interval for this quantity.\nBootstrap Steps\n\nTake a sample of 53 lakes by randomly sampling from the original sample, with replacement.\nFit a regression model with pH as the explanatory variable and record the value of slope \\(b_1\\).\nRepeat steps 1 and 2 many (say 10,000) times, keeping track of slope of the regression line for each bootstrap sample.\nLook at the distribution of the slopes across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the slope relating mercury and pH levels.\n\nWe’ll illustrate the procedure on 3 bootstrap samples.\nBootstrap Sample 1\n\nBootstrapSample1 &lt;- sample_n(FloridaLakes , 53, replace=TRUE)  %&gt;% arrange(ID) %&gt;% \n  select(ID, Lake, pH, Mercury)   # take bootstrap sample\nBootstrapSample1\n\n# A tibble: 53 × 4\n      ID Lake            pH Mercury\n   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n 1     1 Alligator      6.1    1.23\n 2     1 Alligator      6.1    1.23\n 3     2 Annie          5.1    1.33\n 4     2 Annie          5.1    1.33\n 5     3 Apopka         9.1    0.04\n 6     4 Blue Cypress   6.9    0.44\n 7     5 Brick          4.6    1.2 \n 8     9 Deer Point     5.8    0.83\n 9     9 Deer Point     5.8    0.83\n10    10 Dias           6.4    0.81\n# ℹ 43 more rows\n\n\nWe fit a regression model to the bootstrap sample and calculate the regression coefficients. We’re again interested in the second coefficient, \\(b_1\\), which now represents the slope of the regression line.\n\nMb1 &lt;- lm(data=BootstrapSample1, Mercury ~ pH) # fit linear model\nMb1\n\n\nCall:\nlm(formula = Mercury ~ pH, data = BootstrapSample1)\n\nCoefficients:\n(Intercept)           pH  \n     1.7542      -0.1774  \n\n\nBootstrap Sample 2\n\nBootstrapSample2 &lt;- sample_n(FloridaLakes , 53, replace=TRUE)  %&gt;% arrange(ID) %&gt;% \n  select(ID, Lake, pH, Mercury)\nBootstrapSample2\n\n# A tibble: 53 × 4\n      ID Lake         pH Mercury\n   &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1     1 Alligator   6.1    1.23\n 2     2 Annie       5.1    1.33\n 3     2 Annie       5.1    1.33\n 4     3 Apopka      9.1    0.04\n 5     3 Apopka      9.1    0.04\n 6     5 Brick       4.6    1.2 \n 7     5 Brick       4.6    1.2 \n 8     6 Bryant      7.3    0.27\n 9     6 Bryant      7.3    0.27\n10     7 Cherry      5.4    0.48\n# ℹ 43 more rows\n\n\n\nMb2 &lt;- lm(data=BootstrapSample2, Mercury ~ pH) # fit linear model\nMb2\n\n\nCall:\nlm(formula = Mercury ~ pH, data = BootstrapSample2)\n\nCoefficients:\n(Intercept)           pH  \n      1.752       -0.189  \n\n\nBootstrap Sample 3\n\nBootstrapSample3 &lt;- sample_n(FloridaLakes , 53, replace=TRUE)  %&gt;% arrange(ID) %&gt;% \n  select(ID, Lake, pH, Mercury)\nBootstrapSample3\n\n# A tibble: 53 × 4\n      ID Lake                 pH Mercury\n   &lt;int&gt; &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n 1     2 Annie               5.1    1.33\n 2     5 Brick               4.6    1.2 \n 3     6 Bryant              7.3    0.27\n 4     7 Cherry              5.4    0.48\n 5    10 Dias                6.4    0.81\n 6    11 Dorr                5.4    0.71\n 7    14 East Tohopekaliga   5.8    1.16\n 8    14 East Tohopekaliga   5.8    1.16\n 9    15 Farm-13             7.6    0.05\n10    15 Farm-13             7.6    0.05\n# ℹ 43 more rows\n\n\n\nMb3 &lt;- lm(data=BootstrapSample3, Mercury ~ pH) # fit linear model\nMb3\n\n\nCall:\nlm(formula = Mercury ~ pH, data = BootstrapSample3)\n\nCoefficients:\n(Intercept)           pH  \n     1.2629      -0.1102  \n\n\nWe’ll now take 10,000 different bootstrap samples and look at the bootstrap distribution for \\(b_1\\), the slope of the regression line relating mercury level and pH.\n\nM &lt;- lm(data=FloridaLakes, Mercury~pH) #fit model to original sample\nSample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient)\nBootstrap_b1 &lt;- rep(NA, 10000)  #vector to store b1 values\n\nfor (i in 1:10000){\nBootstrapSample &lt;- sample_n(FloridaLakes , 53, replace=TRUE)   #take bootstrap sample\nM &lt;- lm(data=BootstrapSample, Mercury ~ pH) # fit linear model\nBootstrap_b1[i] &lt;- M$coefficients[2] # record b1 \n}\nLakes_Bootstrap_Slope_Results &lt;- data.frame(Bootstrap_b1)  #save results as dataframe\n\nThe bootstrap distribution for the slopes, \\(b_1\\), is shown below, along with the standard error for the difference.\n\nLakes_Bootstrap_Plot_Slope &lt;- ggplot(data=Lakes_Bootstrap_Slope_Results, aes(x=Bootstrap_b1)) +\n  geom_histogram(color=\"white\", fill=\"lightblue\") + \n  xlab(\"Slope in Bootstrap Sample\") + ylab(\"Frequency\") +\n  ggtitle(\"Bootstrap Distribution for Slope\") \nLakes_Bootstrap_Plot_Slope\n\n\n\n\n\n\n\n\nBootstrap Standard Error:\nWe’ll calculate the bootstrap standard error of the slope \\(b_1\\). This is a measure of how much the slope varies between samples.\n\nSE_b1 &lt;- sd(Lakes_Bootstrap_Slope_Results$Bootstrap_b1)\nSE_b1\n\n[1] 0.02694529\n\n\nThe bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval.\n\\[\n\\begin{aligned}\n& b_1 \\pm 2\\times\\text{SE}(b_1) \\\\\n& = -0.1523 \\pm 2\\times{0.027}\n\\end{aligned}\n\\]\n95% Confidence Interval:\n\nc(Sample_b1 - 2*SE_b1, Sample_b1 + 2*SE_b1) \n\n         pH          pH \n-0.20619145 -0.09841028 \n\n\nThe 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below.\n\nLakes_Bootstrap_Plot_Slope + \n  geom_segment(aes(x=Sample_b1 - 2*SE_b1,xend=Sample_b1 + 2*SE_b1, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe are 95% confident that among all Florida lakes, for each 1 unit increase in pH, mercury level decreases between 0.2 and 0.1, on average.\n\n\n3.4.7 CI for Regression Response\nIn addition to calculating a confidence interval for the slope of the regression line relating mercury and pH levels in a lake, we can also calculate a confidence interval for the average mercury level among all lakes with a given pH.\nWe’ll calculate a confidence interval for the average mercury level among all lakes with a neutral pH level of 7.\nThe regression equation is\n\\[\n\\begin{aligned}\n\\widehat{\\text{Mercury}} & = b_0 + b_1\\times\\text{pH} \\\\\n& = 1.5309 - 0.1523\\times\\text{pH}\n\\end{aligned}\n\\]\nso the expected mercury level among all lakes with \\(\\text{pH} = 7\\) is \\(b_0+7b_1 = 1.5309-0.1523(7)=0.4648\\) ppm.\nThis quantity is a statistic calculated from a sample of 53 lakes, so we would not expect the average mercury level among all lakes in the population to be exactly equal to 0.4648. Again, we’ll use this sample statistic as an estimate of the population parameter, and use bootstrapping to estimate the variability associated with this statistic, in order to make a confidence interval.\nBootstrap Steps\n\nTake a sample of 53 lakes by randomly sampling from the original sample, with replacement.\nFit a regression model with location as the explanatory variable and record the values of \\(b_0\\) and \\(b_1\\). Use these to calculate \\(b_0+7b_1\\).\nRepeat steps 1 and 2 many (say 10,000) times, keeping track of \\(b_0\\) and \\(b_1\\), and calculating \\(b_0+7b_1\\) in each bootstrap sample.\nLook at the distribution of the expected response, \\(b_0 + 7b_1\\), across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the expected mercury level among all lakes with pH level of 7.\n\nWe’ll illustrate the procedure on 3 bootstrap samples.\nBootstrap Sample 1\n\nBootstrapSample1 &lt;- sample_n(FloridaLakes , 53, replace=TRUE)  %&gt;% arrange(ID) %&gt;% \n  select(ID, Lake, pH, Mercury)   # take bootstrap sample\nBootstrapSample1\n\n# A tibble: 53 × 4\n      ID Lake          pH Mercury\n   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1     1 Alligator    6.1    1.23\n 2     3 Apopka       9.1    0.04\n 3     3 Apopka       9.1    0.04\n 4     6 Bryant       7.3    0.27\n 5     7 Cherry       5.4    0.48\n 6     8 Crescent     8.1    0.19\n 7     8 Crescent     8.1    0.19\n 8     9 Deer Point   5.8    0.83\n 9     9 Deer Point   5.8    0.83\n10    10 Dias         6.4    0.81\n# ℹ 43 more rows\n\n\nWe fit a regression model to the bootstrap sample and calculate the regression coefficients. We’re interested in the second coefficient, \\(b_1\\), which represents the mean difference between lakes in Southern and Northern Florida\n\nMb1 &lt;- lm(data=BootstrapSample1, Mercury ~ pH) ## fit linear model\nb0 &lt;- Mb1$coefficients[1] # record value of b0 (first coefficient)\nb1 &lt;- Mb1$coefficients[2] # record value of b1 (second coefficient)\nb0+7*b1 #calculate b0+7*b1\n\n(Intercept) \n  0.4353724 \n\n\nBootstrap Sample 2\n\nBootstrapSample2 &lt;- sample_n(FloridaLakes , 53, replace=TRUE)  %&gt;% arrange(ID) %&gt;% \n  select(ID, Lake, pH, Mercury)\nBootstrapSample2\n\n# A tibble: 53 × 4\n      ID Lake            pH Mercury\n   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n 1     2 Annie          5.1    1.33\n 2     2 Annie          5.1    1.33\n 3     3 Apopka         9.1    0.04\n 4     4 Blue Cypress   6.9    0.44\n 5     6 Bryant         7.3    0.27\n 6     6 Bryant         7.3    0.27\n 7    10 Dias           6.4    0.81\n 8    10 Dias           6.4    0.81\n 9    12 Down           7.2    0.5 \n10    12 Down           7.2    0.5 \n# ℹ 43 more rows\n\n\n\nMb2 &lt;- lm(data=BootstrapSample2, Mercury ~ pH) # fit linear model\nb0 &lt;- Mb2$coefficients[1] # record value of b0 (first coefficient)\nb1 &lt;- Mb2$coefficients[2] # record value of b1 (second coefficient)\nb0+7*b1 #calculate b0+7*b1\n\n(Intercept) \n  0.5738289 \n\n\nBootstrap Sample 3\n\nBootstrapSample3 &lt;- sample_n(FloridaLakes , 53, replace=TRUE)  %&gt;% arrange(ID) %&gt;% \n  select(ID, Lake, pH, Mercury)\nBootstrapSample3\n\n# A tibble: 53 × 4\n      ID Lake        pH Mercury\n   &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     2 Annie      5.1    1.33\n 2     3 Apopka     9.1    0.04\n 3     6 Bryant     7.3    0.27\n 4     6 Bryant     7.3    0.27\n 5     7 Cherry     5.4    0.48\n 6     7 Cherry     5.4    0.48\n 7     8 Crescent   8.1    0.19\n 8    10 Dias       6.4    0.81\n 9    10 Dias       6.4    0.81\n10    10 Dias       6.4    0.81\n# ℹ 43 more rows\n\n\n\nMb3 &lt;- lm(data=BootstrapSample3, Mercury ~ pH) # fit linear model\nb0 &lt;- Mb3$coefficients[1] # record value of b0 (first coefficient)\nb1 &lt;- Mb3$coefficients[2] # record value of b1 (second coefficient)\nb0+7*b1 #calculate b0+7*b1\n\n(Intercept) \n  0.4223812 \n\n\nWe’ll now take 10,000 different bootstrap samples and record the values of \\(b_0\\), \\(b_1\\), which we’ll then use to calculate \\(b_0+7b_1\\).\n\nM &lt;- lm(data=FloridaLakes, Mercury~pH) #fit model to original sample\nSample_b0 &lt;- M$coefficients[1] # record b0 value (second coefficient)\nSample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient)\nSample_Exp7 &lt;- Sample_b0 + 7*Sample_b1 # calculate sample expected mercury when pH=7\nBootstrap_b0 &lt;- rep(NA, 10000)  #vector to store b1 values\nBootstrap_b1 &lt;- rep(NA, 10000)  #vector to store b1 values\n\nfor (i in 1:10000){\nBootstrapSample &lt;- sample_n(FloridaLakes , 53, replace=TRUE)   #take bootstrap sample\nM &lt;- lm(data=BootstrapSample, Mercury ~ pH) # fit linear model\nBootstrap_b0[i] &lt;- M$coefficients[1] # record b0 \nBootstrap_b1[i] &lt;- M$coefficients[2] # record b1 \n}\n\nBootstrap_Exp7 &lt;-  Bootstrap_b0 + 7*Bootstrap_b1 # calcualte expected response for each bootstrap sample\n\nLakes_Bootstrap_Exp7_Results &lt;- data.frame(Bootstrap_b0, Bootstrap_b1, Bootstrap_Exp7)  #save results as dataframe\n\nThe bootstrap distribution for the expected mercury level among all lakes with pH level 7, \\(b_0+7b_1\\), is shown below, along with the standard error for this quantity.\n\nLakes_Bootstrap_Plot_Exp7 &lt;- ggplot(data=Lakes_Bootstrap_Exp7_Results, aes(x=Bootstrap_Exp7)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\") + \n  xlab(\"Expected Mercury Level in Bootstrap Sample\") + ylab(\"Frequency\") +\n  ggtitle( \"Bootstrap Distribution for Exp. Mercury when pH=7\") \nLakes_Bootstrap_Plot_Exp7\n\n\n\n\n\n\n\n\nBootstrap Standard Error:\nWe’ll calculate the bootstrap standard error of expected mercury concentration \\(b_0 + 7b_1\\). This is a measure of how much the estimated expected concentration varies between samples.\n\nSE_Exp7 &lt;- sd(Lakes_Bootstrap_Exp7_Results$Bootstrap_Exp7)\nSE_Exp7\n\n[1] 0.03702503\n\n\nAgain, the bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval.\n\\[\n\\begin{aligned}\n& b_1 \\pm 2\\times\\text{SE}(b_1) \\\\\n& = 0.4648 \\pm 2\\times{0.037}\n\\end{aligned}\n\\]\n95% Confidence Interval:\n\nc(Sample_Exp7 - 2*SE_Exp7, Sample_Exp7 + 2*SE_Exp7) \n\n(Intercept) (Intercept) \n  0.3907626   0.5388627 \n\n\nThe 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below.\n\nLakes_Bootstrap_Plot_Exp7 + \n  geom_segment(aes(x=Sample_Exp7 - 2*SE_Exp7,xend=Sample_Exp7 + 2*SE_Exp7, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe are 95% confident that average mercury level among all Florida lakes with pH level 7 is between 0.4 and 0.5 ppm.\nAgain, we are not saying that we think an individual like with a pH level of 7 will lie in this range, only that the average mercury level among all such lakes lies in this range.\n\n\n3.4.8 More CI’s in Regression\nWe saw in the previous two examples how to calculate a confidence interval for the slope of a regression line, and for an expected response in regression. In fact, we can calculate confidence intervals for any function involving regression coefficients \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\), in a similar manner.\nFor example, let’s consider the model for Seattle house prices that involved square feet, whether or not the house was on the waterfront, and an interaction term between these variables.\nThe model is\n\\[\n\\widehat{Price} = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{waterfront} + b_3\\times\\text{Sq.Ft}\\times\\text{Waterfront}\n\\]\nWe fit the model and obtain the parameter estimates shown below.\n\nM &lt;- lm(data=Houses, price~sqft_living + waterfront +\n          sqft_living:waterfront) #fit model to original sample\nSample_b0 &lt;- M$coefficients[1] # record b0 value (second coefficient)\nSample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient)\nSample_b2 &lt;- M$coefficients[3] # record b1 value (second coefficient)\nSample_b3 &lt;- M$coefficients[4] # record b1 value (second coefficient)\nM\n\n\nCall:\nlm(formula = price ~ sqft_living + waterfront + sqft_living:waterfront, \n    data = Houses)\n\nCoefficients:\n              (Intercept)                sqft_living  \n                  67.3959                     0.2184  \n            waterfrontYes  sqft_living:waterfrontYes  \n                -364.5950                     0.4327  \n\n\nConsider the following quantities that we might be interested in estimating:\n\nThe expected price of a 2,000 square foot waterfront house.\n\nThe expected price of a 1,500 square foot non-waterfront house.\n\nThe difference between the expected price of a house 1,800 square foot house on the waterfront, compared to a house the same size that is not on the waterfront.\n\nThe difference in the rate of change in house prices for each additional 100 square feet for houses on the waterfront, compared to houses not on the waterfront.\n\nEach of these quantities can be expressed as a linear function of our regression coefficients \\(b_0, b_1, b_2, b_3\\). We just need to find the appropriate function of the \\(b_j\\)’s, and then calculate a bootstrap confidence interval for that quantity, using the same steps we’ve seen in the previous examples.\nSubstituting into the regression equation, we see that:\n\nThe expected price of a 2,000 square foot waterfront house is given by \\[b_0 + 2000b_1 + b_2 + 2000b_3\\]\n\nWe calculate this estimate from the model, based on our sample of 100 houses:\n\n2000*Sample_b1 +Sample_b2+2000*Sample_b3 # calculate b0+2000b1+b2+2000b3\n\nsqft_living \n   937.4777 \n\n\nWe estimate that the average price of all 2,000 square foot waterfront houses in Seattle is 937 thousand dollars.\n\nThe expected price of a 1,500 square foot non-waterfront house is given by \\[b_0 + 1500b_1\\]\n\n\nSample_b0 + 1500*Sample_b1 # calculate b0+1500b1+\n\n(Intercept) \n   394.9499 \n\n\nWe estimate that the average price of all 1,500 square foot non-waterfront houses in Seattle is 395 thousand dollars.\n\nThe difference between the expected price of a house 1,800 square foot house on the waterfront, compared to a house the same size that is not on the waterfront is given by:\n\n\\[\n\\begin{aligned}\n& (b_0 + 1800b_1 + b_2 + 1800b_3) - (b_0 + 1800b_1) \\\\\n& = b_2 +1800b_3\n\\end{aligned}\n\\]\n\nSample_b2+1800*Sample_b3 # calculate b2+1800b3\n\nwaterfrontYes \n     414.2058 \n\n\nWe estimate that on average a 1,800 square foot house on the waterfront will cost 414 thousand dollars more than a 1,800 square foot house not on the waterfront.\n\nThe difference in the rate of change in house prices for each additional 100 square feet for houses on the waterfront, compared to houses not on the waterfront.\n\nThis question is asking about the difference in slopes of the regression lines relating price and square feet for houses on the waterfront, compared to those not on the waterfront.\nFor houses on the waterfront, the regression equation is\n\\[ \\widehat{Price} = (b_0 + b_2) + (b_1 +b_3)\\times\\text{Sq. Ft.}, \\]\nso the slope is \\(b_1 + b_3\\).\nFor houses not on the waterfront, the regression equation is\n\\[ \\widehat{Price} = b_0 + b_1 \\times\\text{Sq. Ft.}, \\]\nso the slope is \\(b_1\\).\nThese slope pertain to the expected change in price for each additional 1 square foot. So, for a 100-square foot increase, the price of a waterfront house is expected to increase by \\(100(b_1+b_3)\\), compared to an increase of \\(100b_1\\) for a non-waterfront house. Thus, the difference in the rates of change is \\(100b_3\\).\n\n100*Sample_b3 # calculate 100b3\n\nsqft_living:waterfrontYes \n                 43.26671 \n\n\nWe estimate that the price of waterfront houses increases by 43 thousand dollars more for each additional 100 square feet than the price of non-waterfront houses.\nThese estimates calculated from the sample are statistics, which, like all the other statistics we’ve seen are likely to vary from the true values of the corresponding population parameters, due to variability between samples. We can use bootstrapping to calculate confidence intervals for the relevant population parameters, using these sample statistics (the functions of \\(b_j\\)’s), just as we’ve done for the other statistics we’ve seen.\nBootstrap Steps\n\nTake a sample of 100 houses by randomly sampling from the original sample, with replacement.\nFit a regression model with location as the explanatory variable and record the values of regression coefficients \\(b_0, b_1, b_2, b_3\\). Use these to calculate each of the four desired quantities (i.e. \\(b_0 + 2000b_1 + b_2 +2000b_3\\))\nRepeat steps 1 and 2 many (say 10,000) times, keeping track of the regression coefficients and calculating the desired quantities in each bootstrap sample.\nLook at the distribution of the quantities of interest, across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for each of these quantities.\n\nWe’ll illustrate the procedure on 3 bootstrap samples.\nBootstrap Sample 1\nWe take the first bootstrap sample and fit a model with interaction. For brevity, we won’t list out the houses in each of the bootstrap samples, as the idea should be clear by now. Model coefficients are shown below.\n\nBootstrapSample1 &lt;- sample_n(Houses , 100, replace=TRUE)  %&gt;% arrange(Id) %&gt;% \n  select(Id, price, sqft_living, waterfront)   \n\nMb1 &lt;- lm(data=BootstrapSample1, price ~ sqft_living + waterfront + sqft_living:waterfront) # fit linear model with interaction\nb0 &lt;- Mb1$coefficients[1] # record value of b0 (first coefficient)\nb1 &lt;- Mb1$coefficients[2] # record value of b1 (second coefficient)\nb2 &lt;- Mb1$coefficients[3] # record value of b2 (third coefficient)\nb3 &lt;- Mb1$coefficients[4] # record value of b3 (fourth coefficient)\nMb1\n\n\nCall:\nlm(formula = price ~ sqft_living + waterfront + sqft_living:waterfront, \n    data = BootstrapSample1)\n\nCoefficients:\n              (Intercept)                sqft_living  \n                  69.4180                     0.2308  \n            waterfrontYes  sqft_living:waterfrontYes  \n                -444.1795                     0.4229  \n\n\nWe calculate each of the four desired quantities.\n\nb0+2000*b1 + b2 + 2000*b3\n\n(Intercept) \n   932.7184 \n\n\n\nb0+1500*b1\n\n(Intercept) \n   415.6311 \n\n\n\nb2+1800*b3\n\nwaterfrontYes \n     317.0968 \n\n\n\n100*b3\n\nsqft_living:waterfrontYes \n                 42.29312 \n\n\nBootstrap Sample 2\n\nBootstrapSample2 &lt;- sample_n(Houses , 100, replace=TRUE)  %&gt;% arrange(Id) %&gt;% \n  select(Id, price, sqft_living, waterfront)   \n\nMb2 &lt;- lm(data=BootstrapSample2, price ~ sqft_living + waterfront + sqft_living:waterfront) # fit linear model with interaction\nb0 &lt;- Mb2$coefficients[1] # record value of b0 (first coefficient)\nb1 &lt;- Mb2$coefficients[2] # record value of b1 (second coefficient)\nb2 &lt;- Mb2$coefficients[3] # record value of b2 (third coefficient)\nb3 &lt;- Mb2$coefficients[4] # record value of b3 (fourth coefficient)\nMb2\n\n\nCall:\nlm(formula = price ~ sqft_living + waterfront + sqft_living:waterfront, \n    data = BootstrapSample2)\n\nCoefficients:\n              (Intercept)                sqft_living  \n                 118.2805                     0.1840  \n            waterfrontYes  sqft_living:waterfrontYes  \n                -337.1774                     0.4875  \n\n\nWe calculate each of the four desired quantities.\n\nb0+2000*b1 + b2 + 2000*b3\n\n(Intercept) \n    1124.13 \n\n\n\nb0+1500*b1\n\n(Intercept) \n   394.2376 \n\n\n\nb2+1800*b3\n\nwaterfrontYes \n     540.3978 \n\n\n\n100*b3\n\nsqft_living:waterfrontYes \n                 48.75418 \n\n\nBootstrap Sample 3\n\nBootstrapSample3 &lt;- sample_n(Houses , 100, replace=TRUE)  %&gt;% arrange(Id) %&gt;% \n  select(Id, price, sqft_living, waterfront)   \n\nMb3 &lt;- lm(data=BootstrapSample3, price ~ sqft_living + waterfront + sqft_living:waterfront) # fit linear model with interaction\nb0 &lt;- Mb3$coefficients[1] # record value of b0 (first coefficient)\nb1 &lt;- Mb3$coefficients[2] # record value of b1 (second coefficient)\nb2 &lt;- Mb3$coefficients[3] # record value of b2 (third coefficient)\nb3 &lt;- Mb3$coefficients[4] # record value of b3 (fourth coefficient)\nMb3\n\n\nCall:\nlm(formula = price ~ sqft_living + waterfront + sqft_living:waterfront, \n    data = BootstrapSample3)\n\nCoefficients:\n              (Intercept)                sqft_living  \n                  52.4142                     0.2223  \n            waterfrontYes  sqft_living:waterfrontYes  \n                -260.6892                     0.3865  \n\n\nWe calculate each of the four desired quantities.\n\nb0+2000*b1 + b2 + 2000*b3\n\n(Intercept) \n    1009.22 \n\n\n\nb0+1500*b1\n\n(Intercept) \n    385.811 \n\n\n\nb2+1800*b3\n\nwaterfrontYes \n     434.9802 \n\n\n\n100*b3\n\nsqft_living:waterfrontYes \n                  38.6483 \n\n\nWe’ll now take 10,000 different bootstrap samples and record the values of \\(b_0\\), \\(b_1\\), \\(b_3\\), and \\(b_4\\), which we’ll then use to calculate each of our four desired quantities.\n\nM &lt;- lm(data=Houses, price~sqft_living + waterfront +\n          sqft_living:waterfront) #fit model to original sample\nSample_b0 &lt;- M$coefficients[1] # record b0 value (second coefficient)\nSample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient)\nSample_b2 &lt;- M$coefficients[3] # record b1 value (second coefficient)\nSample_b3 &lt;- M$coefficients[4] # record b1 value (second coefficient)\nSample_Q1 &lt;- Sample_b0 + 2000*Sample_b1 +Sample_b2+2000*Sample_b3 # calculate b0+2000b1+b2+2000b3\nSample_Q2 &lt;- Sample_b0 + 1500*Sample_b1 # calculate b0+1500b1+\nSample_Q3 &lt;- Sample_b2+1800*Sample_b3 # calculate b2+1800b3\nSample_Q4 &lt;- 100*Sample_b3 # calculate 100b3\n\nBootstrap_b0 &lt;- rep(NA, 10000)  #vector to store b0 values\nBootstrap_b1 &lt;- rep(NA, 10000)  #vector to store b1 values\nBootstrap_b2 &lt;- rep(NA, 10000)  #vector to store b2 values\nBootstrap_b3 &lt;- rep(NA, 10000)  #vector to store b3 values\n\n\nfor (i in 1:10000){\nBootstrapSample &lt;- sample_n(Houses, 1000, replace=TRUE)   #take bootstrap sample\nMb &lt;- lm(data=BootstrapSample, price ~ sqft_living + \n           waterfront + sqft_living:waterfront) # fit linear model with interaction\nBootstrap_b0[i] &lt;- Mb$coefficients[1] # record value of b0 (first coefficient)\nBootstrap_b1[i] &lt;- Mb$coefficients[2] # record value of b1 (second coefficient)\nBootstrap_b2[i] &lt;- Mb$coefficients[3] # record value of b2 (third coefficient)\nBootstrap_b3[i] &lt;- Mb$coefficients[4] # record value of b3 (fourth coefficient)\n}\n\nBootstrap_Q1 &lt;-  Bootstrap_b0 + 2000*Bootstrap_b1 + Bootstrap_b2 + 2000*Bootstrap_b3\nBootstrap_Q2 &lt;-  Bootstrap_b0 + 1500*Bootstrap_b1 \nBootstrap_Q3 &lt;-  Bootstrap_b2 + 1800*Bootstrap_b3\nBootstrap_Q4 &lt;-  100*Bootstrap_b3\n\nHouses_Bootstrap_Results &lt;- data.frame(Bootstrap_b0, Bootstrap_b1, Bootstrap_b2, Bootstrap_b3, Bootstrap_Q1, Bootstrap_Q2 , Bootstrap_Q3 , Bootstrap_Q4)  #save results as dataframe\n\nBootstrap Distribution for \\(b_0 + 2000b_1 + b_2 + 2000b_3\\)\n\nHouses_Bootstrap_Plot_Q1 &lt;- ggplot(data=Houses_Bootstrap_Results, \n                                   aes(x=Bootstrap_Q1)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\") + \n  xlab(\"Expected Price of 2000 Sq. Ft. Waterfront House\") + ylab(\"Frequency\") +\n  ggtitle( \"Bootstrap Distribution b0+2000b1+b2+2000b3\") \nHouses_Bootstrap_Plot_Q1\n\n\n\n\n\n\n\n\nStandard Error:\n\nSE_Q1 &lt;- sd(Houses_Bootstrap_Results$Bootstrap_Q1)\nSE_Q1\n\n[1] 40.03112\n\n\nThe bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval.\n95% Confidence Interval:\n\nc(Sample_Q1 - 2*SE_Q1, Sample_Q1 + 2*SE_Q1) \n\n(Intercept) (Intercept) \n   924.8114   1084.9359 \n\n\nThe 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below.\n\nHouses_Bootstrap_Plot_Q1 + \n  geom_segment(aes(x=Sample_Q1 - 2*SE_Q1,xend=Sample_Q1 + 2*SE_Q1, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe are 95% confident that average price among all 2,000 square foot Seattle waterfront houses is between 924.8114379 and 1084.9359281 thousand dollars.\nBootstrap Distribution for \\(b_0 + 1500b_1\\)\n\nHouses_Bootstrap_Plot_Q2 &lt;- ggplot(data=Houses_Bootstrap_Results, \n                                   aes(x=Bootstrap_Q2)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\") + \n  xlab(\"Expected Price of 1500 Sq. Ft. Non-Waterfront House\") + ylab(\"Frequency\") +\n  ggtitle( \"Bootstrap Distribution b0+1500b1\") \nHouses_Bootstrap_Plot_Q2\n\n\n\n\n\n\n\n\nStandard Error:\n\nSE_Q2 &lt;- sd(Houses_Bootstrap_Results$Bootstrap_Q2)\nSE_Q2\n\n[1] 5.812557\n\n\nThe bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval.\n95% Confidence Interval:\n\nc(Sample_Q2 - 2*SE_Q2, Sample_Q2 + 2*SE_Q2) \n\n(Intercept) (Intercept) \n   383.3247    406.5750 \n\n\nThe 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below.\n\nHouses_Bootstrap_Plot_Q2 + \n  geom_segment(aes(x=Sample_Q2 - 2*SE_Q2,xend=Sample_Q2 + 2*SE_Q2, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe are 95% confident that average price among all 1,500 square foot Seattle non-waterfront houses is between 383.3247413 and 406.5749705 thousand dollars.\nBootstrap Distribution for \\(b_2 + 1800b_3\\)\n\nHouses_Bootstrap_Plot_Q3 &lt;- ggplot(data=Houses_Bootstrap_Results, \n                                   aes(x=Bootstrap_Q3)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\") + \n  xlab(\"Expected Price Difference WF vs NWF for 1800 sq. Ft. House\") + ylab(\"Frequency\") +\n  ggtitle( \"Bootstrap Distribution b2+1800b3\") \nHouses_Bootstrap_Plot_Q3\n\n\n\n\n\n\n\n\nStandard Error:\n\nSE_Q3 &lt;- sd(Houses_Bootstrap_Results$Bootstrap_Q3)\nSE_Q3\n\n[1] 40.38204\n\n\nThe bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval.\n95% Confidence Interval:\n\nc(Sample_Q3 - 2*SE_Q3, Sample_Q3 + 2*SE_Q3) \n\nwaterfrontYes waterfrontYes \n     333.4417      494.9698 \n\n\nThe 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below.\n\nHouses_Bootstrap_Plot_Q3 + \n  geom_segment(aes(x=Sample_Q3 - 2*SE_Q3,xend=Sample_Q3 + 2*SE_Q3, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe are 95% confident that the average price among all 1800 square feet waterfront houses in Seattle is between 333.441699 and 494.9698429 thousand dollars more than the average price among all non-waterfront houses of the same size.\nBootstrap Distribution for \\(100b_3\\)\n\nHouses_Bootstrap_Plot_Q4 &lt;- ggplot(data=Houses_Bootstrap_Results, \n                                   aes(x=Bootstrap_Q4)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\") + \n  xlab(\"Expected Difference per 100 square feet\") + ylab(\"Frequency\") +\n  ggtitle( \"Bootstrap Distribution 100b3\") \nHouses_Bootstrap_Plot_Q4\n\n\n\n\n\n\n\n\nBootstrap Standard Error: We’ll calculate the bootstrap standard error of the slope \\(100b_3\\). This is a measure of how much the slope varies between samples.\n\nSE_Q4 &lt;- sd(Houses_Bootstrap_Results$Bootstrap_Q4)\nSE_Q4\n\n[1] 2.175838\n\n\nThe bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval.\n95% Confidence Interval:\n\nc(Sample_Q4 - 2*SE_Q4, Sample_Q4 + 2*SE_Q4) \n\nsqft_living:waterfrontYes sqft_living:waterfrontYes \n                 38.91503                  47.61838 \n\n\nThe 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below.\n\nHouses_Bootstrap_Plot_Q4 + \n  geom_segment(aes(x=Sample_Q4 - 2*SE_Q4,xend=Sample_Q4 + 2*SE_Q4, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe are 95% confident that for each 100 square foot increase, the average price among all waterfront houses increases by between 38.9150334 and 47.6183836 thousand dollars more than the increase in average price among all non-waterfront.\n\n\n3.4.9 Bootstrapping Cautions\nWhile bootstrapping is a popular and robust procedure for calculating confidence intervals, it does have cautions and limitations. We should be sure to use the bootstrap procedure appropriate for our context. A standard-error bootstrap interval is appropriate when the sampling distribution for our statistic is roughly symmetric and bell-shaped. When this is not true, a percentile bootstrap interval can be used as long as there are no gaps or breaks in the bootstrap distribution. In situations where there are gaps and breaks in the bootstrap distribution, then the bootstrap distribution may not be a reasonable approximation of the sampling distribution we are interested in.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch3.html#hypothesis-testing",
    "href": "Ch3.html#hypothesis-testing",
    "title": "3  Simulation-Based Inference",
    "section": "3.5 Hypothesis Testing",
    "text": "3.5 Hypothesis Testing\n\n3.5.1 Mercury Levels in Florida Lakes\nRecall the 2004 study by Lange, T., Royals, H. and Connor, L., which examined Mercury accumulation in large-mouth bass, taken from a sample of 53 Florida Lakes. If Mercury accumulation exceeds 0.5 ppm, then there are environmental concerns. In fact, the legal safety limit in Canada is 0.5 ppm, although it is 1 ppm in the United States.\nIn our sample, we have data on 53 lakes, out of more than 30,000 lakes in the the state of Florida.\nWe are interested in whether mercury levels are higher or lower, on average, in Northern Florida compared to Southern Florida.\nWe’ll divide the state along route 50, which runs East-West, passing through Northern Orlando.\n\n\n\n\n\nfrom Google Maps\n\n\n\n\nWe add a variable indicating whether each lake lies in the northern or southern part of the state.\n\nlibrary(Lock5Data)\ndata(FloridaLakes)\n#Location relative to rt. 50\nFloridaLakes$Location &lt;- as.factor(c(\"S\",\"S\",\"N\",\"S\",\"S\",\"N\",\"N\",\"N\",\"N\",\"N\",\"N\",\"S\",\"N\",\"S\",\"N\",\"N\",\"N\",\"N\",\"S\",\"S\",\"N\",\"S\",\"N\",\"S\",\"N\",\"S\",\"N\",\"S\",\"N\",\"N\",\"N\",\"N\",\"N\",\"N\",\"S\",\"N\",\"N\",\"S\",\"S\",\"N\",\"N\",\"N\",\"N\",\"S\",\"N\",\"S\",\"S\",\"S\",\"S\",\"N\",\"N\",\"N\",\"N\"))\nFloridaLakes &lt;- FloridaLakes %&gt;% rename(Mercury = AvgMercury)\nprint.data.frame(data.frame(FloridaLakes%&gt;% select(Lake, Location, Mercury)), row.names = FALSE)\n\n              Lake Location Mercury\n         Alligator        S    1.23\n             Annie        S    1.33\n            Apopka        N    0.04\n      Blue Cypress        S    0.44\n             Brick        S    1.20\n            Bryant        N    0.27\n            Cherry        N    0.48\n          Crescent        N    0.19\n        Deer Point        N    0.83\n              Dias        N    0.81\n              Dorr        N    0.71\n              Down        S    0.50\n             Eaton        N    0.49\n East Tohopekaliga        S    1.16\n           Farm-13        N    0.05\n            George        N    0.15\n           Griffin        N    0.19\n            Harney        N    0.77\n              Hart        S    1.08\n        Hatchineha        S    0.98\n           Iamonia        N    0.63\n         Istokpoga        S    0.56\n           Jackson        N    0.41\n         Josephine        S    0.73\n          Kingsley        N    0.34\n         Kissimmee        S    0.59\n         Lochloosa        N    0.34\n            Louisa        S    0.84\n        Miccasukee        N    0.50\n          Minneola        N    0.34\n            Monroe        N    0.28\n           Newmans        N    0.34\n        Ocean Pond        N    0.87\n      Ocheese Pond        N    0.56\n        Okeechobee        S    0.17\n            Orange        N    0.18\n       Panasoffkee        N    0.19\n            Parker        S    0.04\n            Placid        S    0.49\n            Puzzle        N    1.10\n            Rodman        N    0.16\n          Rousseau        N    0.10\n           Sampson        N    0.48\n             Shipp        S    0.21\n           Talquin        N    0.86\n            Tarpon        S    0.52\n      Tohopekaliga        S    0.65\n          Trafford        S    0.27\n             Trout        S    0.94\n      Tsala Apopka        N    0.40\n              Weir        N    0.43\n           Wildcat        N    0.25\n              Yale        N    0.27\n\n\nWe are interested in investigating whether average mercury levels are higher in either Northern Florida or Southern Florida than the other.\nThe boxplot and table below show the distribution of mercury levels among the 33 northern and 20 southern lakes in the sample.\n\nLakesBP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=Mercury, fill=Location)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() \nLakesBP\n\n\n\n\n\n\n\n\n\nLakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury),  N=n())\nkable(LakesTable)\n\n\n\n\nLocation\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n0.4245455\n0.2696652\n33\n\n\nS\n0.6965000\n0.3838760\n20\n\n\n\n\n\nWe see that on average mercury levels were higher among the southern lakes than the northern ones, a difference of \\(0.697-0.445= 0.272\\) ppm.\n\n\n3.5.2 Model for Mercury Level\nWe can use a statistical model to estimate a lake’s mercury level, using its location (N or S) as our explanatory variable.\nThe model equation is\n\\(\\widehat{\\text{Hg}} = b_0 +b_1\\times\\text{South}\\)\n\n\\(b_0\\) represents the mean mercury level for lakes in North Florida, and\n\n\\(b_1\\) represents the mean difference in mercury level for lakes in South Florida, compared to North Florida\n\nFitting the model in R, we obtain the estimates for \\(b_0\\) and \\(b_1\\).\n\nLakes_M &lt;- lm(data=FloridaLakes, Mercury ~ Location)\nLakes_M\n\n\nCall:\nlm(formula = Mercury ~ Location, data = FloridaLakes)\n\nCoefficients:\n(Intercept)    LocationS  \n     0.4245       0.2720  \n\n\n\\(\\widehat{\\text{Hg}} = 0.4245455+0.2719545\\times\\text{South}\\)\n\n\\(b_1 = 0.272= 0.6965 - 0.4245\\) is equal to the difference in mean mercury levels between Northern and Southern lakes. (We’ve already seen that for categorical variables, the least-squares estimate is the mean, so this makes sense.)\nWe can use \\(b_1\\) to assess the size of the difference in mean mercury concentration levels.\n\n\n\n3.5.3 Hypotheses and Key Question\nSince the lakes we observed are only a sample of 53 lakes out of more than 30,000, we cannot assume the difference in mercury concentration for all Northern vs Southern Florida lakes is exactly 0.272. Instead, we need to determine whether a difference of this size in our sample is large enough to provide evidence of a difference in average mercury level between all Northern and Southern lakes in Florida.\nOne possible explanation for us getting the results we did in our sample is that there really is no difference in average mercury levels between all lakes in Northern and Southern Florida, and we just happened, by chance, to select more lakes with higher mercury concentrations in Southern Florida than in Northern Florida. A different possible explanation is that there really is a difference in average mercury level between lakes in Northern and Southern Florida.\nIn a statistical investigation, the null hypothesis is the one that says there is no difference between groups , or no relationship between variables in the larger population, and that any difference/relationship observed in our sample occurred merely by chance. The alternative hypothesis contradicts the null hypothesis, stating that there is a difference/relationship.\nStated formally, the hypotheses are:\nNull Hypothesis: There is no difference in average mercury level between all lakes in Northern Florida and all lakes in Southern Florida.\nAlternative Hypothesis: There is a difference in average mercury level between all lakes in Northern Florida and all lakes in Southern Florida.\nA statistician’s job is to determine whether the data provide strong enough evidence to rule out the null hypothesis.\nThe question we need to investigate is:\n*“How likely is it that we would have observed a difference in means (i.e. a value of* \\(b_1\\)) as extreme as 0.6965-0.4245 = 0.272 ppm, merely by chance, if there is really no relationship between location and mercury level?”\n\n\n3.5.4 Permutation Test for Difference in Means\nWe can answer the key question using a procedure known as a permutation test. In a permutation test, we randomly permute (or shuffle) the values of our explanatory variable to simulate a situation where there is no relationship between our explanatory and response variable. We observe whether it is plausible to observe values of a statistic (in this case the difference in means) as extreme or more extreme than what we saw in the actual data.\nWe’ll simulate situations where there is no relationship between location and mercury level, and see how often we observe a difference in means (\\(b_1\\)) as extreme as 0.272.\nProcedure:\n\nRandomly shuffle the locations of the lakes, so that any relationship between location and mercury level is due only to chance.\nCalculate the difference in mean mercury levels (i.e. value of \\(b_1\\)) in “Northern” and “Southern” lakes, using the shuffled data. The statistic used to measure the size of the difference or relationship in the sample is called the test statistic.\nRepeat steps 1 and 2 many (say 10,000) times, recording the test statistic (difference in means, \\(b_1\\)) each time.\nAnalyze the distribution of the test statistic (mean difference), simulated under the assumption that there is no relationship between location and mercury level. Look whether the value of the test statistic we observed in the sample (0.272) is consistent with values simulated under the assumption that the null hypothesis is true.\n\nThis simulation can be performed using this Rossman-Chance App.\n\n\n3.5.5 Five Permutations in R\nWe’ll use R to perform permutation test.\nFirst Permutation\nRecall these groups were randomly assigned, so the only differences in averages are due to random chance.\n\nShuffledLakes &lt;- FloridaLakes    # create copy of dataset\nShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] \n\n\nShuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, \n                         FloridaLakes$Mercury, ShuffledLakes$Location)\nnames(Shuffle1df) &lt;- c(\"Lake\", \"Location\", \"Mercury\", \"Shuffled Location\")\nkable(head(Shuffle1df))\n\n\n\n\nLake\nLocation\nMercury\nShuffled Location\n\n\n\n\nAlligator\nS\n1.23\nS\n\n\nAnnie\nS\n1.33\nN\n\n\nApopka\nN\n0.04\nN\n\n\nBlue Cypress\nS\n0.44\nS\n\n\nBrick\nS\n1.20\nN\n\n\nBryant\nN\n0.27\nS\n\n\n\n\n\nNotice that the locations of the lakes have now been mixed up and assigned randomly. So, any relationship between location and mercury level will have occurred merely by chance.\nWe create a boxplot and calculate the difference in mean mercury levels for the shuffled data.\n\nLakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, \n                                         y=Mercury, fill=`Shuffled Location`)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip()\nLakesPerm\n\n\n\n\n\n\n\n\n\nLakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury),  N=n())\nkable(LakesPermTable)\n\n\n\n\nShuffled Location\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n0.4978788\n0.3536308\n33\n\n\nS\n0.5755000\n0.3220898\n20\n\n\n\n\n\nNotice that the sample means are not identical. We observe a difference of -0.0776212 just by chance associated with the assignment of the lakes to their random location groups.\nThis difference is considerably smaller than the difference of 0.272 that we saw in the actual data, suggesting that perhaps a difference as big as 0.272 would not be likely to occur by chance. Before we can be sure of this, however, we should repeat our simulation many times to get a better sense for how big of a difference we might reasonable expect to occur just by chance.\nSecond Permutation\n\nShuffledLakes &lt;- FloridaLakes    ## create copy of dataset\nShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] \nkable(head(Shuffle1df))\n\n\n\n\nLake\nLocation\nMercury\nShuffled Location\n\n\n\n\nAlligator\nS\n1.23\nS\n\n\nAnnie\nS\n1.33\nN\n\n\nApopka\nN\n0.04\nN\n\n\nBlue Cypress\nS\n0.44\nS\n\n\nBrick\nS\n1.20\nN\n\n\nBryant\nN\n0.27\nS\n\n\n\n\n\n\nShuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$Mercury, ShuffledLakes$Location)\nnames(Shuffle1df) &lt;- c(\"Lake\", \"Location\", \"Mercury\", \"Shuffled Location\")\n\n\nLakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=Mercury, fill=`Shuffled Location`)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip()\nLakesPerm\n\n\n\n\n\n\n\n\n\nLakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury),  N=n())\nkable(LakesPermTable)\n\n\n\n\nShuffled Location\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n0.4866667\n0.3676332\n33\n\n\nS\n0.5940000\n0.2883236\n20\n\n\n\n\n\nThird Permutation\n\nShuffledLakes &lt;- FloridaLakes    ## create copy of dataset\nShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] \nkable(head(Shuffle1df))\n\n\n\n\nLake\nLocation\nMercury\nShuffled Location\n\n\n\n\nAlligator\nS\n1.23\nN\n\n\nAnnie\nS\n1.33\nN\n\n\nApopka\nN\n0.04\nN\n\n\nBlue Cypress\nS\n0.44\nS\n\n\nBrick\nS\n1.20\nS\n\n\nBryant\nN\n0.27\nN\n\n\n\n\n\n\nShuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$Mercury, ShuffledLakes$Location)\nnames(Shuffle1df) &lt;- c(\"Lake\", \"Location\", \"Mercury\", \"Shuffled Location\")\n\n\nLakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=Mercury, fill=`Shuffled Location`)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip()\nLakesPerm\n\n\n\n\n\n\n\n\n\nLakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury),  N=n())\nkable(LakesPermTable)\n\n\n\n\nShuffled Location\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n0.4760606\n0.3406514\n33\n\n\nS\n0.6115000\n0.3329339\n20\n\n\n\n\n\nFourth Permutation\n\nShuffledLakes &lt;- FloridaLakes    ## create copy of dataset\nShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] \nkable(head(Shuffle1df))\n\n\n\n\nLake\nLocation\nMercury\nShuffled Location\n\n\n\n\nAlligator\nS\n1.23\nS\n\n\nAnnie\nS\n1.33\nS\n\n\nApopka\nN\n0.04\nN\n\n\nBlue Cypress\nS\n0.44\nS\n\n\nBrick\nS\n1.20\nN\n\n\nBryant\nN\n0.27\nN\n\n\n\n\n\n\nShuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$Mercury, ShuffledLakes$Location)\nnames(Shuffle1df) &lt;- c(\"Lake\", \"Location\", \"Mercury\", \"Shuffled Location\")\n\n\nLakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=Mercury, fill=`Shuffled Location`)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip()\nLakesPerm\n\n\n\n\n\n\n\n\n\nLakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury),  N=n())\nkable(LakesPermTable)\n\n\n\n\nShuffled Location\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n0.5315152\n0.3750343\n33\n\n\nS\n0.5200000\n0.2851961\n20\n\n\n\n\n\nFifth Permutation\n\nShuffledLakes &lt;- FloridaLakes    ## create copy of dataset\nShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] \nkable(head(Shuffle1df))\n\n\n\n\nLake\nLocation\nMercury\nShuffled Location\n\n\n\n\nAlligator\nS\n1.23\nN\n\n\nAnnie\nS\n1.33\nN\n\n\nApopka\nN\n0.04\nN\n\n\nBlue Cypress\nS\n0.44\nN\n\n\nBrick\nS\n1.20\nN\n\n\nBryant\nN\n0.27\nN\n\n\n\n\n\n\nShuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$Mercury, ShuffledLakes$Location)\nnames(Shuffle1df) &lt;- c(\"Lake\", \"Location\", \"Mercury\", \"Shuffled Location\")\n\n\nLakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=Mercury, fill=`Shuffled Location`)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip()\nLakesPerm\n\n\n\n\n\n\n\n\n\nLakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury),  N=n())\nkable(LakesPermTable)\n\n\n\n\nShuffled Location\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n0.5054545\n0.3339357\n33\n\n\nS\n0.5630000\n0.3582281\n20\n\n\n\n\n\n\n\n3.5.6 R Code for Permutation Test\nWe’ll write a for loop to perform 10,000 permutations and record the value of \\(b_1\\) (the difference in sample means) for each simulation.\n\nb1 &lt;- Lakes_M$coef[2] ## record value of b1 from actual data\n\n## perform simulation\nb1Sim &lt;- rep(NA, 10000)          ## vector to hold results\nShuffledLakes &lt;- FloridaLakes    ## create copy of dataset\nfor (i in 1:10000){\n  #randomly shuffle locations\nShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] \nShuffledLakes_M&lt;- lm(data=ShuffledLakes, Mercury ~ Location)   #fit model to shuffled data\nb1Sim[i] &lt;- ShuffledLakes_M$coef[2]  ## record b1 from shuffled model\n}\nNSLakes_SimulationResults &lt;- data.frame(b1Sim)  #save results in dataframe\n\nThe histogram shows the distribution of differences in the group means observed in our simulation. The red lines indicate the difference we actually observed in the data (0.272), as well as an equally large difference in the opposite direction (-0.272).\n\nNSLakes_SimulationResultsPlot &lt;- ggplot(data=NSLakes_SimulationResults, \n                                        aes(x=b1Sim)) + \n  geom_histogram(fill=\"lightblue\", color=\"white\") + \n  geom_vline(xintercept=c(b1, -1*b1), color=\"red\") + \n  xlab(\"Lakes: Simulated Value of b1\") + ylab(\"Frequency\") + \n  ggtitle(\"Distribution of b1 under assumption of no relationship\")\nNSLakes_SimulationResultsPlot\n\n\n\n\n\n\n\n\nThe red lines are quite extreme, relative to the simulated values shown in the histogram. Based on the simulation, it is rare to obtain a difference as extreme as the 0.272 value we saw in the actual data, by chance when there is actually no difference in average mercury levels between Northern and Southern Florida lakes.\nWe calculate the precise number of simulations (out of 10,000) resulting in difference in means more extreme than 0.27195.\n\nsum(abs(b1Sim) &gt; abs(b1))\n\n[1] 39\n\n\nThe proportion of simulations resulting in difference in means more extreme than 0.272 is:\n\nsum(abs(b1Sim) &gt; abs(b1))/10000\n\n[1] 0.0039\n\n\nWe only observed a difference between the groups as extreme or more extreme than the 0.272 difference we saw in the sample in a proportion of 0.0039 of our simulations (less than 1%).\nThe probability of getting a difference in means as extreme or more extreme than 0.272 ppm by chance, when there is no relationship between location and mercury level is about 0.0039. In other words, it is very unlikely that we would have observed a result like we did by chance alone. Thus, we have strong evidence that there is a difference in average mercury level between lakes in Northern and Southern Florida. In this case, there is strong evidence that mercury level is higher in Southern Florida lakes than Northern Florida lakes.\nRecall that in the previous chapter, we found that we could be 95% confident that the mean mercury level among all lakes in Southern Florida is between 0.08 and 0.46 higher than the mean mercury level among all lakes in Northern Florida.\n\n\n3.5.7 p-values\nThe p-value represents the probability of getting a test statistic as extreme or more extreme than we did in our sample when the null hypothesis is true.\nIn this situation, the p-value represents the probability of observing a difference in sample means as extreme or more extreme than 0.272 if there is actually no difference in average mercury level among all lakes in Northern Florida, compared to Southern Florida.\nIn our study, the p-value was 0.0039, which is very low. This provides strong evidence against the null hypothesis that there is no difference in average mercury levels between all Northern and Southern Florida lakes.\nA low p-value tells us that the difference in average Mercury levels that we saw in our sample is unlikely to have occurred by chance, providing evidence that there is indeed a difference in average Mercury levels between Northern and Southern lakes.\nThe p-value does not tell us anything about the size of the difference! If the difference is really small (say 0.001 ppm), perhaps there is no need to worry about it. It is possible to get a small p-value even when the true difference is very small (especially when our sample size is large). In addition to a p-value, we should consider whether a difference is big enough to be meaningful in a practical way, before making any policy decisions.\nFor now, we can use the difference in sample means of 0.272 ppm as an estimate of the size of the difference. Based on our limited knowledge of mercury levels, this does seem big enough to merit further investigation, and possible action.\nAt this point, a reasonable question is “how small must a p-value be in order to provide evidence against the null hypothesis?” While it is sometimes common to establish strict cutoffs for what counts as a small p-value (such as \\(&lt;0.05\\)), the American Statistical Association does not recommend this. In reality, a p-value of 0.04 is practically no different than a p-value of 0.06. Rather than using strict cutoffs for what counts as small, it is better to interpreting p-values on a sliding scale, as illustrated in the diagram below. A p-value of 0.10 or less provides at least some evidence against a null hypothesis, and the smaller the p-value is, the stronger the evidence gets.\n\nknitr::include_graphics(\"pvals.png\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch3.html#more-hypothesis-test-examples",
    "href": "Ch3.html#more-hypothesis-test-examples",
    "title": "3  Simulation-Based Inference",
    "section": "3.6 More Hypothesis Test Examples",
    "text": "3.6 More Hypothesis Test Examples\n\n3.6.1 Other Test Statistics\nThe permutation test procedure can be used to test hypotheses involving lots of different test statistics, in addition to testing for a difference in means, as we saw seen in the previous section. For example we could test whether there is evidence of:\n\na difference in the median mercury level between lakes in Northern Florida, compared to southern Florida\na difference in the amount of variability in mercury levels between lakes in Northern Florida, compared to southern Florida\n\na difference in the proportion of lakes whose mercury level exceeds 1 ppm between lakes in Northern Florida, compared to southern Florida\n\na difference in mean price between King County houses in very good, good, and average or below conditions\na relationship between mercury level and pH level among all Florida lakes\n\nFor each of these investigations, the null hypothesis will be that there is no difference or relationship among all lakes (that is, whatever difference or relationship occurred in the sample occurred just by random chance). We’ll need to find a test statistic that measures the quantity we’re interested in (for example, difference in means). Then, we use the permutation procedure to simulate a scenario where our null hypothesis is true, and see if test statistic we saw in our data is consistent with the ones we simulate under the null hypothesis.\n\n\n3.6.2 General Permutation Test Procedure\nProcedure:\n\nRandomly shuffle the values or categories of the explanatory variable, so that any relationship between the explanatory and response variable occurs just by chance.\nCalculate the test statistic on the shuffled data.\nRepeat steps 1 and 2 many (say 10,000) times, recording the test statistic each time.\nAnalyze the distribution of the test statistic, simulated under the assumption that the null hypothesis is true. Look whether the value of the test statistic we observed in the sample is consistent with values simulated under the assumption that the null hypothesis is true. (We might calculate a p-value, which represents the proportion of simulations in which we observed a test statistic as extreme or more extreme than the one we saw in our actual sample.)\n\nNext, we’ll apply these steps to questions 2, 4, and 5 from the previous subsection.\n\n\n3.6.3 Difference in Standard Deviation\nWe’ll test whether there is evidence of a difference in variability between lakes in Northern Florida, compared to Southern Florida. Since standard deviation is a measure of variability, we’ll use the difference in standard deviation in Northern vs Southern lakes as our test statistic.\nRecall that the standard deviation among the 53 Northern Florida Lakes in our sample was 0.270 ppm, which is lower than the 0.384 ppm in Southern Florida.\n\nkable(LakesTable)\n\n\n\n\nLocation\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n0.4245455\n0.2696652\n33\n\n\nS\n0.6965000\n0.3838760\n20\n\n\n\n\n\nThe test statistic we observe in our sample is \\(0.2696-0.3839 = -0.1142\\) ppm.\nWe need to determine whether a difference this large could have plausibly occurred in our sample, just by chance, if there is really no difference in standard deviation among all lakes in Northern Florida, compared to Southern Florida.\nNull Hypothesis: There is no difference in standard deviation of mercury levels between all lakes in Northern Florida and all lakes in Southern Florida.\nAlternative Hypothesis: There is a difference in standard deviation of mercury levels between all lakes in Northern Florida and all lakes in Southern Florida.\nWe’ll apply the general hypothesis testing procedure, using standard deviation as our test statistic.\nProcedure:\n\nRandomly shuffle the locations of the lakes, so that any relationship between the location and mercury level occurs just by chance.\nCalculate the difference in standard deviation between lakes in the two samples of the shuffled data.\nRepeat steps 1 and 2 many (say 10,000) times, recording the difference in standard deviations each time.\nAnalyze the distribution of difference in standard deviations, simulated under the assumption that there is no difference in standard deviations between North and South. Look whether the value of the test statistic we observed in the sample is consistent with values simulated under the assumption that there is no difference in standard deviations.\n\nR Code for Permutation Test\nWe’ll write a for loop to perform 10,000 permutations and record the value of \\(b_1\\) (the difference in sample means) for each simulation.\n\nSDTab &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(SD=sd(Mercury))\nDiffSD &lt;- SDTab$SD[2] - SDTab$SD[1] \n\n## perform simulation\nDiffSim &lt;- rep(NA, 10000)          ## vector to hold results\nShuffledLakes &lt;- FloridaLakes    ## create copy of dataset\nfor (i in 1:10000){\n  #randomly shuffle locations\nShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] \nSDTabSim &lt;- ShuffledLakes %&gt;% group_by(Location) %&gt;% summarize(SD=sd(Mercury))\nDiffSim[i] &lt;- SDTabSim$SD[2] - SDTabSim$SD[1] #record difference in SD for simulated data\n}\nNSLakes_SDSimResults &lt;- data.frame(DiffSim)  #save results in dataframe\n\nThe distribution of the simulated differences in standard deviation is shown below. Recall that these were simulated assuming that the null hypothesis, that there is no difference in standard deviation of mercury levels among all lakes in Northern Florida, compared to Southern Florida is true.\nThe red lines represent differences as extreme as -0.1142 that we saw in our sample.\n\nNSLakes_SDSimResultsPlot &lt;- ggplot(data=NSLakes_SDSimResults, aes(x=DiffSim)) + \n  geom_histogram(fill=\"lightblue\", color=\"white\") + \n  geom_vline(xintercept=c(DiffSD, -1*DiffSD), color=\"red\") + \n  xlab(\"Simulated Difference in SD's\") + ylab(\"Frequency\") + \n  ggtitle(\"Distribution of Difference in SD under assumption of no relationship\")\nNSLakes_SDSimResultsPlot\n\n\n\n\n\n\n\n\nWe calculate the number of simulations (out of 10,000) resulting in standard deviations greater the 0.1142.\n\nsum(abs(DiffSim) &gt; abs(DiffSD))\n\n[1] 614\n\n\np-value: Proportion of simulations (out of 10,000) resulting in difference in standard deviations greater the 0.1142.\n\nmean(abs(DiffSim) &gt; abs(DiffSD))\n\n[1] 0.0614\n\n\nThis p-value represents the probability of observing a difference in sample standard deviations as extreme as 0.1142 in a samples of size 33 and 20 by chance, if in fact, the standard deviation in mercury concentration levels is the same for lakes in Northern Florida as in Southern Florida.\nSince the p-value is small, it is unlikely that we would observe a difference in standard deviations as extreme as 0.1142 by chance. There is evidence that lakes in Southern Florida exhibit more variability in mercury levels than lakes in Northern Florida (though the evidence is not as strong as it was when we were testing for a difference in means).\nNote that we have avoided the fallacy of using 0.05 as a strict cutoff for rejecting the null hypothesis.\nAlthough the difference in standard deviations is statistically discernible, it is hard to say whether it is practically meaningful. Without knowing a lot about mercury levels, and their impact on the ecosystem, it’s harder to tell whether an estimated difference in standard deviations of 0.11 ppm is meaningful or not. It would be good to consult a biologist before making any decisions based on these results.\n\n\n3.6.4 Slope of Regression Line\nIn addition to the mercury levels of the Florida lakes, we have data on the pH level of each lake. pH level measures the acidity of a lake, ranging from 0 to 14, with 7 being neutral, and lower levels indicating more acidity. We plot the pH level against the mercury level in our sample of 53 lakes.\n\nggplot(data=FloridaLakes, aes(y=Mercury, x=pH)) + \n  geom_point() + stat_smooth(method=\"lm\", se=FALSE) + \n  xlim(c(3, 10)) + ylim(c(0,1.5))\n\n\n\n\n\n\n\n\nThe regression equation is\n\\[\n\\widehat{\\text{Mercury}} = b_0 + b_1\\times\\text{pH}\n\\]\nRegression estimates \\(b_0\\) and \\(b_1\\) are shown below.\n\nLakes_M_pH &lt;- lm(data=FloridaLakes, Mercury~pH)\nLakes_M_pH\n\n\nCall:\nlm(formula = Mercury ~ pH, data = FloridaLakes)\n\nCoefficients:\n(Intercept)           pH  \n     1.5309      -0.1523  \n\n\nWe can use the slope of the regression line \\(b_1\\) to measure the strength relationship between Mercury and pH. Based on our sample, each one-unit increase in pH, mercury level is expected to decrease by 0.15 ppm.\nIf there was really no relationship, then the slope among all lakes would be 0. But, of course, we would not expect the slope in our sample to exactly match the slope for all lakes. Our question of interest is whether it is plausible that we could have randomly selected a sample resulting in a slope as extreme as 0.15 by chance, when there is actually no relationship between mercury and pH levels, among all lakes. In other words, could we plausible have drawn the sample of 53 lakes shown in blue from a population like the one in red, shown below?\n\n\n\n\n\n\n\n\n\nKey Question:\n\nHow likely is it that we would have observed a slope (i.e. a value of \\(b_1\\)) as extreme as 0.15 by chance, if there is really no relationship between mercury level and pH?\n\nNull Hypothesis: Among all Florida lakes, there is no relationship between mercury level and pH.\nAlternative Hypothesis: Among all Florida lakes, there is a relationship between mercury level and pH.\nProcedure:\n\nRandomly shuffle the pH values, so that any relationship between acceleration mercury and pH is due only to chance.\nFit a regression line to the shuffled data and record the slope of the regression line.\nRepeat steps 1 and 2 many (say 10,000) times, recording the slope (i.e. value of \\(b_1\\)) each time.\nAnalyze the distribution of slopes, simulated under the assumption that there is no relationship between mercury and pH. Look whether the actual slope we observed is consistent with the simulation results.\n\nWe’ll illustrate the first three permutations.\nFirst Permutation\n\nShuffledLakes &lt;- FloridaLakes    ## create copy of dataset\nShuffledLakes$pH &lt;- ShuffledLakes$pH[sample(1:nrow(ShuffledLakes))] \n\n\nShuffle1df &lt;- data.frame(ShuffledLakes$Lake, FloridaLakes$Mercury, FloridaLakes$pH, ShuffledLakes$pH)\nnames(Shuffle1df) &lt;- c(\"Lake\", \"Mercury\", \"pH\", \"Shuffled_pH\")\nkable(head(Shuffle1df))\n\n\n\n\nLake\nMercury\npH\nShuffled_pH\n\n\n\n\nAlligator\n1.23\n6.1\n8.4\n\n\nAnnie\n1.33\n5.1\n7.1\n\n\nApopka\n0.04\n9.1\n6.9\n\n\nBlue Cypress\n0.44\n6.9\n4.6\n\n\nBrick\n1.20\n4.6\n5.8\n\n\nBryant\n0.27\n7.3\n7.8\n\n\n\n\n\nThe red line indicates the slope of the regression line fit to the shuffled data. The blue line indicates the regression line for the actual lakes in the sampe, which has a slope of -0.15.\n\nggplot(data=Shuffle1df, aes(x=Shuffled_pH, y=Mercury)) + \n  geom_point() + stat_smooth(method=\"lm\", se=FALSE, color=\"red\") + \n  xlim(c(3, 10)) + ylim(c(0,1.5)) + \n  geom_abline(slope=-0.1523, intercept=1.5309, color=\"blue\")\n\n\n\n\n\n\n\n\nSlope of regression line from permuted data:\n\nM_Lakes_Shuffle &lt;- lm(data=Shuffle1df, Mercury~Shuffled_pH)\nsummary(M_Lakes_Shuffle)$coef[2]\n\n[1] 0.03934635\n\n\nSecond Permutation\n\nShuffledLakes &lt;- FloridaLakes    ## create copy of dataset\nShuffledLakes$pH &lt;- ShuffledLakes$pH[sample(1:nrow(ShuffledLakes))] \n\n\nShuffle2df &lt;- data.frame(ShuffledLakes$Lake, FloridaLakes$Mercury, FloridaLakes$pH, ShuffledLakes$pH)\nnames(Shuffle2df) &lt;- c(\"Lake\", \"Mercury\", \"pH\", \"Shuffled_pH\")\nkable(head(Shuffle2df))\n\n\n\n\nLake\nMercury\npH\nShuffled_pH\n\n\n\n\nAlligator\n1.23\n6.1\n4.4\n\n\nAnnie\n1.33\n5.1\n7.3\n\n\nApopka\n0.04\n9.1\n7.5\n\n\nBlue Cypress\n0.44\n6.9\n5.8\n\n\nBrick\n1.20\n4.6\n7.2\n\n\nBryant\n0.27\n7.3\n6.7\n\n\n\n\n\n\nggplot(data=Shuffle2df, aes(x=Shuffled_pH, y=Mercury)) + \n  geom_point() + stat_smooth(method=\"lm\", se=FALSE, color=\"red\") + \n  xlim(c(3, 10)) + ylim(c(0,1.5)) + \n  geom_abline(slope=-0.1523, intercept=1.5309, color=\"blue\")\n\n\n\n\n\n\n\n\nSlope of regression line from permuted data:\n\nM_Lakes_Shuffle &lt;- lm(data=Shuffle2df, Mercury~Shuffled_pH)\nsummary(M_Lakes_Shuffle)$coef[2]\n\n[1] 0.009030783\n\n\n\nShuffledLakes &lt;- FloridaLakes    ## create copy of dataset\nShuffledLakes$pH &lt;- ShuffledLakes$pH[sample(1:nrow(ShuffledLakes))] \n\n\nShuffle3df &lt;- data.frame(ShuffledLakes$Lake, FloridaLakes$Mercury, FloridaLakes$pH, ShuffledLakes$pH)\nnames(Shuffle3df) &lt;- c(\"Lake\", \"Mercury\", \"pH\", \"Shuffled_pH\")\nkable(head(Shuffle3df))\n\n\n\n\nLake\nMercury\npH\nShuffled_pH\n\n\n\n\nAlligator\n1.23\n6.1\n7.0\n\n\nAnnie\n1.33\n5.1\n6.2\n\n\nApopka\n0.04\n9.1\n8.3\n\n\nBlue Cypress\n0.44\n6.9\n7.3\n\n\nBrick\n1.20\n4.6\n6.8\n\n\nBryant\n0.27\n7.3\n7.2\n\n\n\n\n\n\nggplot(data=Shuffle3df, aes(x=Shuffled_pH, y=Mercury)) + \n  geom_point() + stat_smooth(method=\"lm\", se=FALSE, color=\"red\") + \n  xlim(c(3, 10)) + ylim(c(0,1.5)) + \n  geom_abline(slope=-0.1523, intercept=1.5309, color=\"blue\")\n\n\n\n\n\n\n\n\nSlope of regression line from permuted data:\n\nM_Lakes_Shuffle &lt;- lm(data=Shuffle3df, Mercury~Shuffled_pH)\nsummary(M_Lakes_Shuffle)$coef[2]\n\n[1] 0.003748437\n\n\nNone of our three simulations resulted in a slope near as extreme as the -0.15 that we saw in the actual data. This seems to suggest that it is unlikely that we would have observed a slope as extreme as -0.15 if there is actually no relationship between mercury and pH among all lakes.\nThat said, we should repeat the simulation many more times to see whether getting a slope as extreme as -0.15 is plausible.\n\nb1 &lt;- Lakes_M_pH$coef[2] ## record value of b1 from actual data\n\n## perform simulation\nb1Sim &lt;- rep(NA, 10000)          ## vector to hold results\nShuffledLakes &lt;- FloridaLakes    ## create copy of dataset\nfor (i in 1:10000){\n  #randomly shuffle acceleration times\nShuffledLakes$pH &lt;- ShuffledLakes$pH[sample(1:nrow(ShuffledLakes))] \nShuffledLakes_M&lt;- lm(data=ShuffledLakes, Mercury ~ pH)   #fit model to shuffled data\nb1Sim[i] &lt;- ShuffledLakes_M$coef[2]  ## record b1 from shuffled model\n}\nLakes_pHSimulationResults &lt;- data.frame(b1Sim)  #save results in dataframe\n\n\nb1 &lt;- Lakes_M_pH$coef[2] ## record value of b1 from actual data\nLakes_pHSimulationResultsPlot &lt;- ggplot(data=Lakes_pHSimulationResults, aes(x=b1Sim)) + \n  geom_histogram(fill=\"lightblue\", color=\"white\") + \n  geom_vline(xintercept=c(b1, -1*b1), color=\"red\") + \n  xlab(\"Simulated Value of b1\") + ylab(\"Frequency\") + \n  ggtitle(\"Distribution of b1 under assumption of no relationship\")\nLakes_pHSimulationResultsPlot\n\n\n\n\n\n\n\n\np-value: Proportion of simulations resulting in value of \\(b_1\\) more extreme than -0.15\n\nmean(abs(b1Sim) &gt; abs(b1))\n\n[1] 0\n\n\nThe p-value represents the probability of observing a slope as extreme or more extreme than -0.15 by chance when there is actually no relationship between mercury level and pH.\nIt is extremely unlikely that we would observe a value of \\(b_1\\) as extreme as -0.15 by chance, if there is really no relationship between mercury level and pH. In fact, this never happened in any of our 10,000 simulations!\nThere is very strong evidence of a relationship mercury level and pH.\nA low p-value tells us only that there is evidence of a relationship, not that it is practically meaningful. We have seen that for each one-unit increase in pH, mercury level is expected to decrease by 0.15 ppm on average, which seems like a pretty meaningful decrease, especially considering that mercury levels typically stay between 0 and 1.\nWe used the slope as our test statistic to measure the evidence of the relationship between the explanatory and response variables. In fact, we could have also used the correlation coefficient \\(r\\) as our test statistic, and we would have gotten the same p-value. Either slope or correlation may be used for a hypothesis test involving two quantitative variables, but we will use slope in this class.\n\n\n3.6.5 F-Statistic\nRecall when we examined the prices of houses in King County, WA, whose conditions were rated as either very good, good, or average or below. Suppose we want to test the hypotheses:\nNull Hypothesis: There is no difference in average prices between houses of the three different conditions, among all houses in King County, WA.\nAlternative Hypothesis: There is a difference in average prices between houses of the three different conditions, among all houses in King County, WA.\nComparative boxplots are shown below.\n\nggplot(data=Houses, aes(x=condition, y=price, fill=condition)) + \n  geom_boxplot(outlier.shape = NA) + geom_jitter(alpha=0.5) + coord_flip() + ggtitle(\"Houses\")\n\n\n\n\n\n\n\n\n\nCond_Tab &lt;- Houses %&gt;% group_by(condition) %&gt;% summarize(Mean_Price = mean(price), \n                                             SD_Price= sd (price), \n                                             N= n())\nkable(Cond_Tab)\n\n\n\n\ncondition\nMean_Price\nSD_Price\nN\n\n\n\n\naverage or below\n700.6349\n768.1179\n61\n\n\ngood\n861.0000\n1048.9521\n30\n\n\nvery_good\n551.8361\n332.8597\n9\n\n\n\n\n\nWe notice differences in price. Surprisingly, houses in good condition cost more than 300 thousand dollars more than those in very good condition, on average.\nIf we were only comparing two groups, we could use the difference in average price between them as a test statistic. But since we’re comparing three, we need a statistic that can measure the size of differences between all three groups. An F-statistic can do this, so we’ll use the F-statistic as our test statistic here.\nWe calculated the F-statistic in Chapter 2.\n\nM_House_Cond &lt;- lm(data=Houses, price~condition)\nM0_House &lt;- lm(data=Houses, price~1)\nanova(M_House_Cond, M0_House)$F[2]\n\n[1] 0.6046888\n\n\nOur question of interest is “How likely is it to observe an F-statistic as extreme or more extreme than 0.605 if there is actually no difference in average price between houses of the three conditions?”\nWe’ll use a permutation-based hypothesis test to investigate this question.\nProcedure:\n\nRandomly shuffle the conditions of the houses, so that any relationship between condition and price is due only to chance.\nUsing the shuffled data, calculate an F-statistic for a predicting price, comparing a full model that uses condition as an explanatory variable, to a reduced model with no explanatory variables.\nRepeat steps 1 and 2 many (say 10,000) times, recording the F-statistic each time.\nAnalyze the distribution of F-statistics, simulated under the assumption that there is no relationship between condition and price. Look whether the actual F-statistic we observed is consistent with the simulation results.\n\nWe’ll illustrate the first three permutations.\nFirst Permutation\n\nShuffledHouses &lt;- Houses    ## create copy of dataset\nShuffledHouses$condition &lt;- ShuffledHouses$condition[sample(1:nrow(ShuffledHouses))] \n\n\nShuffle1df &lt;- data.frame(Houses$Id, Houses$price, Houses$condition, ShuffledHouses$condition)\nnames(Shuffle1df) &lt;- c(\"Id\", \"price\", \"condition\", \"Shuffled_Condition\")\nkable(head(Shuffle1df))\n\n\n\n\nId\nprice\ncondition\nShuffled_Condition\n\n\n\n\n1\n1225.0\naverage or below\naverage or below\n\n\n2\n885.0\naverage or below\naverage or below\n\n\n3\n385.0\ngood\naverage or below\n\n\n4\n252.7\naverage or below\ngood\n\n\n5\n468.0\ngood\ngood\n\n\n6\n310.0\ngood\naverage or below\n\n\n\n\n\n\nggplot(data=ShuffledHouses, aes(x=condition, y=price, fill=condition)) + \n  geom_boxplot(outlier.shape = NA) + geom_jitter(alpha=0.5) + coord_flip() + ggtitle(\"Shuffled Houses\")\n\n\n\n\n\n\n\n\nWe fit a model to predict price using condition, and compare it to one that predicts price without using condition, and calculate the F-statistic.\n\nM1_Shuffled_Houses &lt;- lm(data=ShuffledHouses, price~condition)\nM0_Shuffled_Houses &lt;- lm(data=ShuffledHouses, price~1)\nanova(M1_Shuffled_Houses, M0_Shuffled_Houses)$F[2]\n\n[1] 1.831639\n\n\nSecond Permutation\n\nShuffledHouses &lt;- Houses    ## create copy of dataset\nShuffledHouses$condition &lt;- ShuffledHouses$condition[sample(1:nrow(ShuffledHouses))] \n\n\nShuffle2df &lt;- data.frame(Houses$Id, Houses$price, Houses$condition, ShuffledHouses$condition)\nnames(Shuffle2df) &lt;- c(\"Id\", \"price\", \"condition\", \"Shuffled_Condition\")\nkable(head(Shuffle2df))\n\n\n\n\nId\nprice\ncondition\nShuffled_Condition\n\n\n\n\n1\n1225.0\naverage or below\ngood\n\n\n2\n885.0\naverage or below\naverage or below\n\n\n3\n385.0\ngood\ngood\n\n\n4\n252.7\naverage or below\naverage or below\n\n\n5\n468.0\ngood\naverage or below\n\n\n6\n310.0\ngood\naverage or below\n\n\n\n\n\n\nggplot(data=ShuffledHouses, aes(x=condition, y=price, fill=condition)) + \n  geom_boxplot(outlier.shape = NA) + geom_jitter(alpha=0.5) + coord_flip() + ggtitle(\"Shuffled Houses\")\n\n\n\n\n\n\n\n\nWe fit a model to predict price using condition, and compare it to one that predicts price without using condition, and calculate the F-statistic.\n\nM1_Shuffled_Houses &lt;- lm(data=ShuffledHouses, price~condition)\nM0_Shuffled_Houses &lt;- lm(data=ShuffledHouses, price~1)\nanova(M1_Shuffled_Houses, M0_Shuffled_Houses)$F[2]\n\n[1] 0.3999225\n\n\nThird Permutation\n\nShuffledHouses &lt;- Houses    ## create copy of dataset\nShuffledHouses$condition &lt;- ShuffledHouses$condition[sample(1:nrow(ShuffledHouses))] \n\n\nShuffle3df &lt;- data.frame(Houses$Id, Houses$price, Houses$condition, ShuffledHouses$condition)\nnames(Shuffle3df) &lt;- c(\"Id\", \"price\", \"condition\", \"Shuffled_Condition\")\nkable(head(Shuffle1df))\n\n\n\n\nId\nprice\ncondition\nShuffled_Condition\n\n\n\n\n1\n1225.0\naverage or below\naverage or below\n\n\n2\n885.0\naverage or below\naverage or below\n\n\n3\n385.0\ngood\naverage or below\n\n\n4\n252.7\naverage or below\ngood\n\n\n5\n468.0\ngood\ngood\n\n\n6\n310.0\ngood\naverage or below\n\n\n\n\n\n\nggplot(data=ShuffledHouses, aes(x=condition, y=price, fill=condition)) + \n  geom_boxplot(outlier.shape = NA) + geom_jitter(alpha=0.5) + coord_flip() + ggtitle(\"Shuffled Houses\")\n\n\n\n\n\n\n\n\nWe fit a model to predict price using condition, and compare it to one that predicts price without using condition, and calculate the F-statistic.\n\nM1_Shuffled_Houses &lt;- lm(data=ShuffledHouses, price~condition)\nM0_Shuffled_Houses &lt;- lm(data=ShuffledHouses, price~1)\nanova(M1_Shuffled_Houses, M0_Shuffled_Houses)$F[2]\n\n[1] 2.55569\n\n\nWe’ll simulate 10,000 permutations and record the F-statistic for each set of permuted data.\n\nFstat &lt;- anova(M_House_Cond, M0_House)$F[2] ## record value of F-statistic from actual data\n\n## perform simulation\nFSim &lt;- rep(NA, 10000)          ## vector to hold results\nShuffledHouses &lt;- Houses    ## create copy of dataset\nfor (i in 1:10000){\n  #randomly shuffle acceleration times\nShuffledHouses$condition &lt;- ShuffledHouses$condition[sample(1:nrow(ShuffledHouses))] \nShuffledHouses_M1&lt;- lm(data=ShuffledHouses, price ~ condition)  #fit full model to shuffled data\nShuffledHouses_M0&lt;- lm(data=ShuffledHouses, price ~ 1)  #fit reduced model to shuffled data\nFSim[i] &lt;- anova(ShuffledHouses_M1, ShuffledHouses_M0)$F[2]  ## record F from shuffled model\n}\nHouse_Cond_SimulationResults &lt;- data.frame(FSim)  #save results in dataframe\n\nThe distribution of the F-statistics is shown below. Recall that these are simulated under the assumption that there is no difference in average price between houses of the three different conditions, i.e. no relationship between price and condition.\nThe red line shows the location of the F-statistic we saw in our data (0.60). Since F-statistics cannot be negative, we don’t need to worry about finding an F-statistic as extreme in the opposite direction.\n\nHouse_Cond_SimulationResults_Plot &lt;- ggplot(data=House_Cond_SimulationResults, \n                                            aes(x=FSim)) + \n  geom_histogram(fill=\"lightblue\", color=\"white\") +  geom_vline(xintercept=c(Fstat), color=\"red\") + \n  xlab(\"Simulated Value of F\") + ylab(\"Frequency\") +  ggtitle(\"Distribution of F under assumption of no relationship\")\nHouse_Cond_SimulationResults_Plot\n\n\n\n\n\n\n\n\nThe F-statistic in our actual we observed does not appear to be very extreme.\np-value: Proportion of simulations resulting in value of F more extreme than 0.60.\n\nmean(FSim &gt; Fstat)\n\n[1] 0.5548\n\n\nThe p-value represents the probability of observing an F-statistic as extreme as 0.60 by chance, in samples of size 61, 30, and 9, if in fact there is no relationship between price and size of car.\nMore than half of our simulations resulted in an F-statistic as extreme or more extreme than the one we saw in our actual data, even though the simulation was performed in a situation where there was no relationship between price and condition. Thus, it is very plausible that we would observe an F-statistic as extreme or more extreme than we saw in our data, even if there is no relationship between price and condition (or no difference in average price between the conditions), among all houses.\nSince the p-value is large, we cannot reject the null hypothesis. We do not have evidence to say that average price differs between houses of the different condition types.\nIt is important to note that we are not saying that we believe the average price is the same for each condition. Recall that the average prices among the conditions in our sample differed by more than 300 thousand dollars! It’s just that given the size of our samples, and the amount of variability in our data, we cannot rule out the possibility that this difference occurred purely by chance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch3.html#responsible-hypothesis-testing",
    "href": "Ch3.html#responsible-hypothesis-testing",
    "title": "3  Simulation-Based Inference",
    "section": "3.7 Responsible Hypothesis Testing",
    "text": "3.7 Responsible Hypothesis Testing\nWhile hypothesis tests are a powerful tool in statistics, they are also one that has been widely misused, to the detriment of scientific research. The hard caused by these misuses caused the American Statistical Association to release a 2016 statement, intended to provide guidance and clarification to scientists who use hypothesis testing and p-values in their research.\nThe statement provides the following six principles for responsible use of hypothesis tests and p-values.\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\nThe statement provides important guidance for us to consider as we work with hypothesis testing in this class, as well as in future classes and potentially in our own research.\n\nA hypothesis test can only tell us the strength of evidence against the null hypothesis. The absence of evidence against the null hypothesis should not be interpreted as evidence for the null hypothesis.\nWe should never say that the data support/prove/confirm the null hypothesis.\nWe can only say that the data do not provide evidence against the null hypothesis.\n\nWhat to conclude from p-values and what not to:\n\nA low p-value provides evidence against the null hypothesis. It suggests the test statistic we observed is inconsistent with the null hypothesis.\nA low p-value does not tell us that the difference or relationship we observed is meaningful in a practical sense. Researchers should look at the size of the difference or strength of the relationship in the sample before deciding whether it merits being acted upon.\nA high p-value means that the data could have plausibly been obtained when the null hypothesis is true. The test statistic we observed is consistent with what we would have expected to see when the null hypothesis is true, and thus we cannot rule out the null hypothesis.\nA high p-value does not mean that the null hypothesis is true or probably true. A p-value can only tell us the strength of evidence against the null hypothesis, and should never be interpreted as support for the null hypothesis.\n\nJust because our result is consistent with the null hypothesis does not mean that we should believe that null hypothesis is true. Lack of evidence against a claim does not necessarily mean that the claim is true.\nIn this scenario, we got a small p-value, but we should also be aware of what we should conclude if the p-value is large. Remember that the p-value only measures the strength of evidence against the null hypothesis. A large p-value means we lack evidence against the null hypothesis. This does not mean, however, that we have evidence supporting null hypothesis.\nA hypothesis test can be thought of as being analogous to a courtroom trial, where the null hypothesis is that the defendant did not commit the crime. Suppose that after each side presents evidence, the jury remains unsure whether the defendant committed the crime. Since the jury does not have enough evidence to be sure, they must, under the law of the United States find the defendant “not guilty.” This does not mean that the jury thinks the defendant is innocent, only that they do not have enough evidence to be sure they are guilty. Similarly in a hypothesis test, a large p-value indicates a lack of evidence against the null hypothesis, rather than evidence supporting it. As such, we should avoid statements suggesting we “support”, “accept”, or “believe” the null hypothesis, and simply state that we lack evidence against it.\nThings to say when the p-value is large:\n\nThe data are consistent with the null hypothesis.\n\nWe do not have enough evidence against the null hypothesis.\n\nWe cannot reject the null hypothesis.\n\nThe null hypothesis is plausible.\n\nThings NOT to say when the p-value is large:\n\nThe data support the null hypothesis.\n\nThe data provide evidence for the null hypothesis.\n\nWe accept the null hypothesis.\n\nWe conclude that the null hypothesis is true.\n\nThus, if we had obtained a large p-value in the comparison in mercury levels between Northern and Southern lakes, the appropriate conclusion would be\n“We do not have evidence that the average mercury level differs between lakes in Northern Florida, compared to Southern Florida.”\nEven if we got a large p-value it would be incorrect to say “There is no difference in average mercury levels between lakes in Northern Florida and Southern Florida.”\nWe would just be saying that given the size of our sample and the amount of variability on the date, we cannot rule out the possibility of observing a difference like we did by chance, when there really is no difference.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch4.html",
    "href": "Ch4.html",
    "title": "4  Inference from Models",
    "section": "",
    "text": "4.1 The Normal Error Regression Model\nYou’ve probably noticed that most of the sampling distributions of statistics we’ve seen were symmetric and bell-shaped in nature. When working with statistics that have symmetric and bell-shaped distributions and know standard error formulas, it is possible to use well-known probability facts to obtain confidence intervals and perform hypothesis tests without actually performing simulation.\nIn this chapter, we’ll examine a set of assumptions that, if true, would ensure that statistics like means and differences in means, and regression coefficients follow symmetric and bell-shaped distributions. We’ll learn how to use facts from probability to calculate confidence intervals and p-values without actually performing simulation in these instances.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#the-normal-error-regression-model",
    "href": "Ch4.html#the-normal-error-regression-model",
    "title": "4  Inference from Models",
    "section": "",
    "text": "4.1.1 Example: Ice Cream dispenser\n\n\n\n\n\n\n\n\n\nSuppose an ice cream machine is manufactured to dispense 2 oz. of ice cream per second, on average. If 15 people used the ice cream machine, holding the dispenser for different amounts of time, and each person got exactly 2 oz. per second, the relationship between time holding the dispenser and amount dispensed would look like this:\n\n\n\n\n\n\n\n\n\nIn reality, however, the actual amount dispensed each time it is used will vary due to unknown factors like:\n\nforce applied to dispenser\n\ntemperature\n\nbuild-up of ice cream\n\nother unknown factors\n\nThus, if 15 real people held the dispenser and recorded the amount of ice cream they got, the scatter plot we would see would look something like this:\n\n\n\n\n\n\n\n\n\nWe can think of the amount of ice cream a person receives as being a result of two separate components, often referred to as signal and noise.\nSignal represents the average amount of ice cream a person is expected to receive based on the amount of time holding the dispenser. In this case, signal is given by the function \\(\\text{Expected Amount} = 2\\times\\text{Time}\\). Everyone who holds the dispenser for \\(t\\) seconds is expected to receive \\(2t\\) ounces of ice cream.\nNoise represents how much each person’s actual amount of ice cream deviates from their expected amount. For example, a person who holds the dispenser for 1.5 seconds and receives 3.58 oz. of ice cream will have received 0.58 ounces more than expected due to noise (i.e. factors beyond time holding the dispenser).\nIn a statistical model, we assume that the response value of a response variable we observe is the sum of the signal, or expected response, which is a function of the explanatory variables in the model, and noise, which results from deviations due to factors beyond those accounted for in the model.\n\n\n4.1.2 Normal Distribution\nIt is common to model noise using a symmetric, bell-shaped distribution, known as a normal distribution.\n\n\n\n\n\n\n\n\n\nWe can think of the error term as a random draw from somewhere in the area below the bell-curve. For example, in the above illustration, most of the area under the curve lies between \\(-1\\leq x\\leq 1\\). If this curve represented the noise term in the ice cream example, it would mean that most people’s actual amount of ice cream dispensed would be within \\(\\pm 1\\) ounce of their expected amount (or signal). Notice that the normal distribution is centered at 0, indicating that on average, a person would be expected to get an amount exactly equal to their signal, but that they might deviate above or below this amount by unexplained factors, which can be modeled by random chance.\nA normal distribution is defined by two parameters:\n- \\(\\mu\\) representing the center of the distribution\n- \\(\\sigma\\) representing the standard deviation\nThis distribution is denoted \\(\\mathcal{N}(\\mu, \\sigma)\\).\n\n\n\n\n\n\n\n\n\nWhen the standard deviation is small, such as for the blue curve, noise tends to be close to 0, meaning the observed values will be close to their expectation. On the other hand, the green curve, which has higher standard deviation, would often produce noise values as extreme as \\(\\pm 2\\) or more.\nNote that the square of the standard deviation \\(\\sigma^2\\) is called the variance. Some books denote the normal distribution as \\(\\mathcal{N}(\\mu, \\sigma^2)\\), instead of \\(\\mathcal{N}(\\mu,\\sigma)\\). We will use the \\(\\mathcal{N}(\\mu,\\sigma)\\) here, which is consistent with R.\n\n\n4.1.3 Modeling Ice Cream Dispenser\nIn this example, we’ll simulate the amount of ice cream dispensed for each person by adding a random number from a normal distribution with mean 0 and standard deviation 0.5 to the expected amount dispensed, which is given by \\(2x\\), where \\(x\\) represents time pressing the dispenser. We’ll let \\(\\epsilon_i\\) represent the random noise term for the \\(i\\)th person.\nThus, amount dispensed (\\(Y_i\\)) for person \\(i\\) is given by\n\\[\nY_i = 2x_i+\\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, 0.5)\n\\]\nIllustration\n\n\n\n\n\nThe amount dispensed follows a normal distribution with mean equal to twice the time holding the dispensor and standard deviation 0.5\n\n\n\n\nWe simulate the amount dispensed for a sample of 15 people below. The rnorm(n, \\(\\mu\\), \\(\\sigma\\)) function generates \\(n\\) random numbers from a normal distribution with mean \\(\\mu\\), and standard deviation \\(\\sigma\\). The code below simulates 15 random numbers and adds them to the expected amount dispensed for 15 people who hold the dispenser for the times shown.\n\nset.seed(10082020)\n# set times \ntime &lt;- c(1, 1.2, 1.5, 1.8, 2.1, 2.1, 2.3, 2.5, 2.6, 2.8, 2.9, 2.9, 3.1, 3.2, 3.6)\nexpected &lt;- 2*time  # expected amount\nnoise &lt;-rnorm(15, 0, 0.5) %&gt;% round(2)  #generate noise from normal distribution\namount &lt;- 2*time + noise  # calculate observed amounts\nIcecream &lt;- data.frame(time, signal, noise, amount) # set up data table\nkable((Icecream)) #display table\n\n\n\n\ntime\nsignal\nnoise\namount\n\n\n\n\n1.0\n2.0\n0.23\n2.23\n\n\n1.2\n2.4\n-0.49\n1.91\n\n\n1.5\n3.0\n0.58\n3.58\n\n\n1.8\n3.6\n-0.03\n3.57\n\n\n2.1\n4.2\n0.17\n4.37\n\n\n2.1\n4.2\n-0.93\n3.27\n\n\n2.3\n4.6\n0.05\n4.65\n\n\n2.5\n5.0\n-0.37\n4.63\n\n\n2.6\n5.2\n-0.46\n4.74\n\n\n2.8\n5.6\n0.17\n5.77\n\n\n2.9\n5.8\n-0.59\n5.21\n\n\n2.9\n5.8\n0.12\n5.92\n\n\n3.1\n6.2\n0.00\n6.20\n\n\n3.2\n6.4\n0.67\n7.07\n\n\n3.6\n7.2\n0.05\n7.25\n\n\n\n\n\nThe scatterplot displays the amount dispensed, compared to the time pressing the dispenser. The red line indicates the line \\(y=2x\\). If there was no random noise, then each person’s amount dispensed would lie exactly on this line.\n\nggplot(data=Icecream1, aes(x=time, y=amount)) + geom_point() + ggtitle(\"Icecream Dispensed\") + xlab(\"Time Pressing dispenser\") + ylab(\"Amount Dispensed\") + geom_abline(slope=2, intercept=0, color=\"red\") + \n  annotate(\"text\", label=\"y=2x\", x= 3.5, y=6.5, size=10, color=\"red\")\n\n\n\n\n\n\n\n\nEstimating Regression Line\nIn a real situation, we would not see the signal and noise columns in the table or the red line on the graph. We would only see the time and amount, and points on the scatter plot. From these, we would need to estimate the location of the red line by fitting a least squares regression line to the data, as we’ve done before.\nThe blue line represents the location of the least squares regression line fit to the time and amounts observed.\n\nggplot(data=Icecream1, aes(x=time, y=amount)) + geom_point() + ggtitle(\"Icecream Dispensed\") + xlab(\"Time Pressing dispenser\") + ylab(\"Amount Dispensed\") + stat_smooth(method=\"lm\", se=FALSE) + geom_abline(slope=2, intercept=0, color=\"red\") + \n  annotate(\"text\", label=\"y=2x\", x= 3.5, y=6.5, size=10, color=\"red\")\n\n\n\n\n\n\n\n\nThe blue line is close, but not identical to the red line, representing the true (usually unknown) signal.\nThe slope and intercept of the blue line are given by:\n\nIC_Model &lt;- lm(data=Icecream1, lm(amount~time))\nIC_Model\n\n\nCall:\nlm(formula = lm(amount ~ time), data = Icecream1)\n\nCoefficients:\n(Intercept)         time  \n    -0.1299       2.0312  \n\n\nNotice that these estimates are close, but not identical to the intercept and slope of the red line, which are 0 and 2, respectively.\nThe equation of the red line is given by:\n\\(Y_i = \\beta_0 + \\beta_1X_{i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\),\nwhere \\(Y_i\\) represents amount dispensed, and \\(X_i\\) represents time. \\(\\beta_0, \\beta_1,\\), and \\(\\sigma\\) are the unknown model parameters associated with the ice cream machine’s process.\nUsing the values of \\(b_0\\) and \\(b_1\\) obtained by fitting a model to our observed data as estimates of \\(\\beta_0\\) and \\(\\beta_1\\), our estimated regression equation is\n[Y_i = b_0 + b_1X_i + _i = -0.1299087 + 2.0312489X_i + _i ]\nwhere \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\).\nAn estimate for \\(\\sigma\\) is given by\n\\(s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\).\nWe calculate \\(s\\) , using R.\n\ns &lt;- sqrt(sum(IC_Model$residuals^2)/(15-2))\ns\n\n[1] 0.4527185\n\n\nIn statistics it is common to use Greek letters like \\(\\beta_j\\) and \\(\\sigma\\), to represent unknown model parameters, pertaining to a population or process (such as the ice cream dispenser), and English letters like \\(b_j\\) and \\(s\\) to represent statistics calculated from data.\nThe estimates of \\(b_0 = -0.1299087\\), \\(b_1=2.0312489\\), and \\(s = 0.4527185\\) are reasonably close estimates to the values \\(\\beta_0=0, \\beta_1=2\\), and \\(\\sigma = 0.5\\), that we used to generate the data.\nIn a real situation, we’ll have only statistics \\(b_0\\), \\(b_1\\), and \\(s\\), and we’ll need to use them to draw conclusions about parameters \\(\\beta_0=0, \\beta_1=2\\), and \\(\\sigma = 0.5\\).\n\n\n4.1.4 Normal Error Regression Model\nIn the ice cream example, the relationship between expected amount and time holding the dispenser was given by a linear equation involving a single numeric explanatory variable. We can generalize this to situations with multiple explanatory variables, which might be numeric or categorical.\nIndividual observations are then assumed to vary from their expectation in accordance with a normal distribution, representing random noise (or error).\nThe mathematical form of a normal error linear regression model is\n\\(Y_i = \\beta_0 + \\beta_1X_{i1}+ \\ldots + \\beta_pX_{ip} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\).\nNote that in place of \\(X_{ip}\\), we could have indicators for categories, or functions of \\(X_{ip}\\), such as \\(X_{ip}^2\\), \\(\\text{log}(X_{ip})\\), or \\(\\text{sin}(X_{ip})\\).\n\nThe quantities \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are parameters, pertaining to the true but unknown data generating mechanism.\nThe estimates \\(b_0, b_1, \\ldots, b_p\\), are statistics, calculated from our observed data.\n\nWe use statistics \\(b_0, b_1, \\ldots, b_p\\) to obtain confidence intervals and hypothesis tests to make statements about parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\).\n\nAssumptions of Normal Error Regression Model\nWhen generating data for the ice cream machine, we assumed four things:\n\nLinearity - the expected amount of ice cream dispensed (i.e the signal) is a linear function of time the dispenser was pressed.\nConstant Variance - individual amounts dispensed varied from their expected amount with equal variability, regardless of the amount of time. That is, the amount of variability in individual amounts dispensed was the same for people who held the dispenser for 1 s. as for people who held it for 2 s. or 3 s., etc.\nNormality - individual amounts dispensed varied from their expected amount in accordance with a normal distribution.\nIndependence - the amount of ice cream dispensed for one person was not affected by the amount dispensed for anyone else.\n\nMore generally, the normal error regression model assumes:\n\nLinearity - the expected response is a linear function of the explanatory variable(s).\nConstant Variance - the variance between individual values of the response variable are the same for any values/categories of the explanatory variable(s)\nNormality - for any values/categories of the explanatory variable(s) individual response values vary from their expectation in accordance with a normal distribution.\nIndependence - individual response values are not affected by one another\n\n\n\n4.1.5 Examples of Normal Error Regression Model\nWe can formulate all of the examples we’ve worked with so far in terms of the normal error regression model.\nIn the house price example, consider the following models:\nModel 1:\n\\(\\text{Price}_i = \\beta_0 + \\beta_1\\text{Sq.Ft.}_{i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\).\nIf we use this model, we’re saying that we believe the expected price of a house is a linear function of its size, and that for any given size, the distribution of actual prices are normally distributed around their expected value of \\(\\beta_0 + \\beta_1\\text{Sq.Ft.}_{i}\\). Furthermore, we’re saying that the amount of variability in hourse prices is the same for houses of any size.\nModel 2:\n\\(\\text{Price}_i = \\beta_0 + \\beta_2\\text{Waterfront}_{i}+ \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\).\nIf we use this model, we’re saying that we believe the expected price of a house depends only on whether or not it is on the waterfront, and that prices of both waterfront and non-waterfront houses follow normal distributions. Though these distributions may have different means (\\(\\beta_0\\) for non-waterfront houses, and \\(\\beta_1\\) for waterfront houses), the amount of variability in prices should be the same for waterfront as non-waterfront houses.\nModel 3:\n\\(\\text{Price}_i = \\beta_0 + \\beta_1\\text{Sq.Ft.}_{i}+ \\beta_2\\text{Waterfront}_{i}+ \\beta_3\\times\\text{Sq.Ft.}_i\\times\\text{Waterfront}_{i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\).\nand\nModel 4:\n\\(\\text{Price}_i = \\beta_0 + \\beta_1\\text{Sq.Ft.}_{i}+ \\beta_2\\text{Waterfront}_{i}+ \\beta_3\\times\\text{Sq.Ft.}_i\\times\\text{Waterfront}_{i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\).\nBoth of Models 3 and 4 assume that actual prices of houses with the same size and waterfront status are normally distributed, and that the mean of the normal distribution is a linear function of its size. Model 3 allows for the intercept of the lines to differ between waterfront and non-waterfront houses, while Model 4 allows both the intercept and slope to differ. Both assume that the amount of variability among houses of the same size and waterfront status is the same.\n\n\n4.1.6 Implications of Normal Error Regression Model\nIf we really believe that data come about as the normal error regression model describes, then probability theory tells us that regression coefficients \\(b_j\\)’s, representing differences between categories for categorical variables and rates of change for quantitative variables, follow symmetric and bell-shaped distributions. We can use this fact to create confidence intervals and perform hypothesis tests, without needing to perform simulation. This is, in fact what R does in it’s model summary output.\nThese methods are only valid, however, if data can reasonably be thought of as having come from a process consistent with the assumptions of the normal error regression model process. If we don’t believe that our observed data can be reasonably thought of as having come from such a process, then the confidence intervals and p-values produced by R, and other places that rely on probability-based methods will not be reliable.\nWe close the section with a philosophical question:\nDo data really come about from processes like the normal error regression model? That is, do you think it is reasonable to believe that data we see in the real world (perhaps the amount of ice cream dispensed by an ice cream machine) represent independent outcomes of a process in which expected outcomes are a linear function of explanatory variables, and deviate from their expectation according to a normal distribution with constant variance?\nWe won’t attempt to answer that question here, but it is worth thinking about. After all, it is an assumption on which many frequently employed methods of statistical inference depends.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#standard-error-and-confidence-intervals",
    "href": "Ch4.html#standard-error-and-confidence-intervals",
    "title": "4  Inference from Models",
    "section": "4.2 Standard Error and Confidence Intervals",
    "text": "4.2 Standard Error and Confidence Intervals\n\n4.2.1 Common Standard Error Formulas\nIn the normal error regression model, \\(\\sigma\\) is an unknown model parameter representing the standard deviation in response values among cases with the same values/categories of explanatory variable(s). We estimate \\(\\sigma\\) using the statistic\n\\(s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\).\n\\(s\\) is not the same as the standard errors of statistics we estimated using bootstrapping in Chapter 3. It is, however, related to these standard errors. In fact, standard errors for common statistics like sample means, differences in means, and regression coefficients can be approximated using formulas involving \\(s\\).\nSo far, we’ve used simulation (permutation tests and bootstrap intervals) to determine the amount of variability associated with a test statistic or estimate, in order to perform hypotheses tests and create confidence intervals. In special situations, there are mathematical formulas, based on probability theory, that can be used to approximate these standard errors without having to perform the simulations.\nTheory-Based Standard Error Formulas\n\n\n\n\n\n\n\nStatistic\nStandard Error\n\n\n\n\nSingle Mean\n\\(SE(b_0)=\\frac{s}{\\sqrt{n}}\\)\n\n\nDifference in Means\n\\(SE(b_j)=s\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\)\n\n\nSingle Proportion\n\\(SE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\)\n\n\nDifference in Proportions\n\\(SE(\\hat{p}) = \\sqrt{\\left(\\frac{\\hat{p_1}(1-\\hat{p}_1)}{n_1}+\\frac{\\hat{p_2}(1-\\hat{p_2})}{n_2}\\right)}\\)\n\n\nIntercept in Simple Linear Regression\n\\(SE(b_0)=s\\sqrt{\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}}\\)\n\n\nSlope in Simple Linear Regression\n\\(SE(b_1)=\\sqrt{\\frac{s^2}{\\sum(x_i-\\bar{x})^2}}=\\sqrt{\\frac{1}{n-2}\\frac{{\\sum(\\hat{y}_i-y_i)^2}}{\\sum(x_i-\\bar{x})^2}}\\)\n\n\n\n\n\\(s=\\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}} = \\sqrt{\\frac{\\text{SSR}}{(n-(p+1))}}\\), (p is number of regression coefficients not including \\(b_0\\)). \\(s\\) is an estimate of the variability in the response variable among cases where the explanatory variable(s) are the same. Note that in the one-sample case, this simplifies to the standard deviation formula we’ve seen previously.\nIn the 2nd formula, the standard error estimate \\(s\\sqrt{\\frac{1}{n_1+n_2}}\\) is called a “pooled” estimate since it combines information from all groups. When there is reason to believe standard deviation differs between groups, we often use an “unpooled” standard error estimate of \\(\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\), where \\(s_1, s_2\\) represent the standard deviation for groups 1 and 2. This can be used in situations where the explanatory variable is categorical and we have doubts about the constant variance assumption.\n\nThere is no theory-based formula for standard error associated with the median or standard deviation. For these, and many other statistics, we rely on simulation to estimate variability between samples.\nThere are formulas for standard errors associated with coefficients in multiple regression, but these require mathematics beyond what is assumed in this class. They involve linear algebra and matrix inversion, which you can read about here if you are interested.\nWhen the sampling distribution is symmetric and bell-shaped, approximate 95% confidence intervals can be calculated using the formula,\n\\[\n\\text{Statistic} \\pm 2\\times \\text{Standard Error},\n\\]\nwhere the standard error is estimated using a formula, rather than through bootstrapping.\nWe’ll go through some examples to illustrate how to calculate and interpret \\(s\\) and \\(SE(b_j)\\).\n\n\n4.2.2 Example: Difference in Means\nWe’ll use the normal error regression model to predict a lake’s mercury level, using location (N vs S) as the explanatory variable.\nThe regression model is:\n\\[\n\\text{Mercury} = \\beta_0 +\\beta_1 \\times\\text{Location}_{\\text{South}} + \\epsilon_i, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nThis model assumes:\n\nLinearity - there is no linearity assumption when the explanatory variable is categorical.\nConstant Variance - the variance between mercury levels of individual lakes is the same for Northern Florida, as for Southern Florida.\nNormality - mercury levels are normally distributed in Northern Florida and also in Southern Florida.\nIndependence - mercury levels of individual lakes are not affected by those of other lakes.\n\nThe lm summary command in R returns information pertaining to the model.\n\nLakes_M &lt;- lm(data=FloridaLakes, Mercury ~ Location)\nsummary(Lakes_M)\n\n\nCall:\nlm(formula = Mercury ~ Location, data = FloridaLakes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.65650 -0.23455 -0.08455  0.24350  0.67545 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)  0.42455    0.05519   7.692 0.000000000441 ***\nLocationS    0.27195    0.08985   3.027        0.00387 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3171 on 51 degrees of freedom\nMultiple R-squared:  0.1523,    Adjusted R-squared:  0.1357 \nF-statistic: 9.162 on 1 and 51 DF,  p-value: 0.003868\n\n\nThe estimates column returns the estimates of \\(b_0\\) and \\(b_1\\). The estimated regression equation is\n\\[\n\\widehat{\\text{Mercury}} = 0.42455 + 0.27185\\times\\text{Location}_{\\text{South}}\n\\]\nEstimating \\(\\sigma\\)\nThe Residual Standard Error in the output gives the value of \\(s\\), and estimate of \\(\\sigma\\).\n\\(s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\).\nSSR is:\n\nsum(Lakes_M$residuals^2)\n\n[1] 5.126873\n\n\nSince LocationSouth is our only explanatory variable, \\(p=1\\), and since we have 53 lakes in the sample, \\(n=53\\).\nThus,\n\\(s =\\sqrt{\\frac{5.126873}{53-(1+1)}} = 0.317\\).\nThe standard deviation in mercury concentrations among lakes in the same part of the state is estimated to be 0.317 ppm.\nTHe second column in the coefficients table gives standard errors associated with \\(b_0\\) and \\(b_1\\). These tell us how much these statistics are expected to vary between samples of the given size.\nEstimating \\(\\text{SE}(b_0)\\) and \\(\\text{SE}(b_1)\\)\nWe’ll use the theory-based formulas to calculate the standard errors for \\(b_0\\) and \\(b_1\\).\nIn this case, \\(b_0\\) represents a single mean, the mean mercury level for lakes in Northern Florida. Since there are 33 such lakes, the calculation is:\n\\(SE(b_0)=\\frac{s}{\\sqrt{n}} = \\frac{0.317}{\\sqrt{33}} \\approx 0.055\\)\nThe standard error of intercept \\(b_0\\) is 0.055. This represents the variability in average mercury level in different samples of 33 lakes from Northern Florida.\n\\(SE(b_1)=s\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}} = 0.317\\sqrt{\\frac{1}{20}+{1}{33}} = 0.0898\\)\nThe standard error of slope \\(b_1\\) is 0.0898. This represents the variability in average difference in mercury levels between samples of 33 lakes from Northern Florida and 20 lakes from Southern Florida.\nThese numbers match the values in the Std. Error column of the coefficients table of the lm summary output.\nA 95% confidence interval for \\(\\beta_0\\) is given by\n\\[\n\\begin{aligned}\n& b_0 \\pm 2\\times\\text{SE}(b_0) \\\\\n& = 0.42455 \\pm 2\\times{0.055} \\\\\n& = (0.314, 0.535)\n\\end{aligned}\n\\]\nA 95% confidence interval for \\(\\beta_1\\) is given by\n\\[\n\\begin{aligned}\n& b_1 \\pm 2\\times\\text{SE}(b_1) \\\\\n& = 0.272 \\pm 2\\times{0.0898} \\\\\n& = (0.09, 0.45)\n\\end{aligned}\n\\]\nWe are 95% confident that the average mercury level in Southern Lakes is between 0.09 ppm and 0.45 ppm higher than in Northern Florida.\nThese intervals can be obtained directly in R using the confint command.\n\nconfint(Lakes_M, level=0.95)\n\n                 2.5 %    97.5 %\n(Intercept) 0.31374083 0.5353501\nLocationS   0.09157768 0.4523314\n\n\nComparison to Bootstrap\nWe previously calculated \\(\\text{SE}(b_1)\\) and a 95% confidence interval for \\(\\beta_1\\), which are shown below.\nBootstrap Standard Error\n\nSE_b1 &lt;- sd(NS_Lakes_Bootstrap_Results$Bootstrap_b1)\nSE_b1\n\n[1] 0.09549244\n\n\nBootstrap 95% Confidence Interval:\n\nSample_b1 &lt;- Lakes_M$coefficients[2]\nc(Sample_b1 - 2*SE_b1, Sample_b1 + 2*SE_b1) \n\n LocationS  LocationS \n0.08096967 0.46293942 \n\n\n\nNS_Lakes_Bootstrap_Plot_b1 + \n  geom_segment(aes(x=Sample_b1 - 2*SE_b1,xend=Sample_b1 + 2*SE_b1, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nThe formula-based standard error and confidence interval and those produced by bootstrapping are both approximations, so they are not expected to be identical, but should be close.\n\n\n4.2.3 Example: Regression Slope and Intercept\nWe’ll use the normal error regression model to predict a lake’s mercury level, using pH as the explanatory variable.\nThe regression model is:\n\\[\n\\text{Mercury} = \\beta_0 +\\beta_1 \\times\\text{pH} + \\epsilon_i, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nThis model assumes:\n\nLinearity - the mercury level is a linear function of pH.\nConstant Variance - the variance between mercury levels of individual lakes is the same for each pH level.\nNormality - for each pH, mercury levels are normally distributed.\nIndependence - mercury levels of individual lakes are not affected by those of other lakes.\n\nWe might have doubts about some of these assumptions. For example, if we believe there might be more variability in mercury levels among lakes with higher pH levels than lower ones (a violation of the constant variance assumption), or that lakes closer together are likely to have similar mercury levels (a violation of the independence assumption) then the results of the model might not be reliable.\nLater in this chapter, we’ll learn ways to check the appropriateness of these assumptions. For now, we’ll assume that the model is a reasonable enough approximation of reality and use it accordingly.\nThe lm summary command in R returns information pertaining to the model.\n\nM_pH &lt;- lm(data=FloridaLakes, Mercury ~ pH)\nsummary(M_pH)\n\n\nCall:\nlm(formula = Mercury ~ pH, data = FloridaLakes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.48895 -0.19188 -0.05774  0.09456  0.71134 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)  1.53092    0.20349   7.523 0.000000000814 ***\npH          -0.15230    0.03031  -5.024 0.000006572811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2816 on 51 degrees of freedom\nMultiple R-squared:  0.3311,    Adjusted R-squared:  0.318 \nF-statistic: 25.24 on 1 and 51 DF,  p-value: 0.000006573\n\n\nThe estimated regression equation is\n\\[\n\\widehat{\\text{Mercury}} = 1.53 - 0.15 \\times\\text{pH}\n\\]\nEstimating \\(\\sigma\\)\nAs we saw in the ice cream example, an estimate for \\(\\sigma\\) is given by\n\\(s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\).\nWe calculate \\(s\\) , using R.\nSSR is:\n\nsum(M_pH$residuals^2)\n\n[1] 4.045513\n\n\nSince pH is our only explanatory variable, \\(p=1\\), and since we have 53 lakes in the sample, \\(n=53\\).\nThus,\n\\(s =\\sqrt{\\frac{4.045513}{53-(1+1)}} = 0.2816\\).\nThe standard deviation in mercury concentrations among lakes with the same pH is estimated to be 0.2816 ppm.\nThe residual standard error in the summary output returns the value of \\(s\\).\nEstimating \\(\\text{SE}(b_0)\\) and \\(\\text{SE}(b_1)\\)\nNow, we’ll use the theory-based formulas to calculate standard error associated with the intercept and slope of the regression line relating mercury level and pH in Florida lakes.\nWe calculate \\(\\bar{x}\\), the mean pH, and \\(\\sum(x_i-\\bar{x})^2\\).\n\nxbar &lt;- mean(FloridaLakes$pH)\nxbar\n\n[1] 6.590566\n\n\n\nSxx &lt;- sum((FloridaLakes$pH  - xbar)^2)\nSxx\n\n[1] 86.32528\n\n\n\\(SE(b_0)=s\\sqrt{\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}} = 0.2816\\sqrt{\\frac{1}{53}+\\frac{6.59^2}{86.325}}\\approx 0.2034\\)\nThe standard error of intercept \\(b_0\\) is 0.2034. This represents the variability in estimated mercury level in lakes with pH of 0, between different samples of size 53. Since none of the lakes have pH levels of 0, this is not a meaningful statistic.\n\\(SE(b_1)=\\sqrt{\\frac{s^2}{\\sum(x_i-\\bar{x})^2}}=\\sqrt{\\frac{0.2816^2}{86.325}} \\approx 0.0303\\)\nThe standard error of slope \\(b_1\\) is 0.0303. This represents the variability in estimated rate of change in mercury level per one unit increase in pH, between different samples of size 53.\nThese numbers match the values in the Std. Error column of the coefficients table of the lm summary output.\nA 95% confidence interval for \\(\\beta_1\\) is given by\n\\[\n\\begin{aligned}\n& b_1 \\pm 2\\times\\text{SE}(b_1) \\\\\n& = -0.15 \\pm 2\\times{0.0303} \\\\\n& = (-0.09, -0.21)\n\\end{aligned}\n\\]\nWe are 95% confident that for each one unit increase in pH, mercury concentration is expected to decrease between 0.09 and 0.21 ppm, on average.\nWe can also use the confint() command to obtain these intervals.\n\nconfint(M_pH, level=0.95)\n\n                 2.5 %      97.5 %\n(Intercept)  1.1223897  1.93944769\npH          -0.2131573 -0.09144445\n\n\nComparison to Bootstrap\nWe previously calculated \\(\\text{SE}(b_1)\\) and a 95% confidence interval for \\(\\beta_1\\), which are shown below.\nBootstrap Standard Error for \\(b_1\\):\n\nSample_b1 &lt;- M_pH$coefficients[2]\nSE_b1 &lt;- sd(Lakes_Bootstrap_Slope_Results$Bootstrap_b1)\nSE_b1\n\n[1] 0.02694529\n\n\nBootstrap 95% Confidence Interval for \\(\\beta_1\\):\n\nc(Sample_b1 - 2*SE_b1, Sample_b1 + 2*SE_b1) \n\n         pH          pH \n-0.20619145 -0.09841028 \n\n\nThe formula-based standard error and confidence interval and those produced by bootstrapping are both approximations, so they are not expected to be identical, but should be close.\n\n\n4.2.4 Example: Single Mean\nFinally, we’ll use a model to estimate the average mercury level among all lakes in Florida. For this, we use an “Intercept-only” model, with no explanatory variables.\nThe model equation is:\nThe regression model is:\n\\[\n\\text{Mercury} = \\beta_0  + \\epsilon_i, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nThis model assumes:\n\nLinearity - there is no linearity assumption since there are no explanatory variables.\nConstant Variance - there is no linearity assumption since there are no explanatory variables.\nNormality - mercury levels among all lakes in Florida are normally distributed.\nIndependence - mercury levels of individual lakes are not affected by those of other lakes.\n\nThe lm summary command in R returns information pertaining to the model.\n\nLakes_M0 &lt;- lm(data=FloridaLakes, Mercury ~ 1)\nsummary(Lakes_M0)\n\n\nCall:\nlm(formula = Mercury ~ 1, data = FloridaLakes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.48717 -0.25717 -0.04717  0.24283  0.80283 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  0.52717    0.04684   11.25 0.00000000000000151 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.341 on 52 degrees of freedom\n\n\nThe model estimates that the average mercury level is 0.527, which matches the sample mean, as we know it should.\nEstimating \\(\\sigma\\)\nAn estimate for \\(\\sigma\\) is given by\n\\(s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\).\nWe calculate \\(s\\) , using R.\nSSR is:\n\nsum(Lakes_M0$residuals^2)\n\n[1] 6.047875\n\n\nSince there are no explanatory variables, \\(p=0\\), and since we have 53 lakes in the sample, \\(n=53\\).\nWhen there are no explanatory variables, \\(s\\) is simply an estimate of the standard deviation of individual observations of the response variable.\nThus,\n\\(s =\\sqrt{\\frac{6.047875}{53-(0+1)}} = 0.341\\).\nThe standard deviation in mercury concentrations among lakes in Florida is estimated to be 0.341 ppm.\nThis matches residual standard error in the summary output returns the value of \\(s\\).\nEstimating \\(\\text{SE}(b_0)\\)\nNow, we’ll use the theory-based formulas to calculate standard error associated with \\(b_0\\), the average mercury level among all lakes in Florida.\n\\(SE(b_0)=\\frac{s}{\\sqrt{n}}=\\frac{0.341}{\\sqrt{53}}\\approx0.04684\\)\nThe standard error of mean \\(b_0\\) is 0.0484. This represents the variability in mean mercury level among samples of 53 lakes.\nNotice that the standard error of the mean is less than the standard deviation of individual observations, and that the standard error of the mean decreases as sample size increases, as we’ve seen previously.\nA 95% confidence interval for \\(\\beta_0\\), is:\n\\[\n\\begin{aligned}\n& b_0 \\pm 2\\times\\text{SE}(b_0) \\\\\n& = 0.527 \\pm 2\\times{0.0484} \\\\\n& = (0.43, 0.62)\n\\end{aligned}\n\\]\nWe are 95% confident that the average mercury level among all lakes in Florida is between 0.43 and 0.62 ppm.\nWe can also obtain this interval using the confint() command.\n\nconfint(Lakes_M0, level=0.95)\n\n                2.5 %    97.5 %\n(Intercept) 0.4331688 0.6211709\n\n\nComparison to Bootstrap\nHere are the bootstrap standard error and confidence interval we calculated previously.\nBootstrap Standard Error\n\nSE_mean &lt;- sd(Lakes_Bootstrap_Results_Mean$Bootstrap_Mean)\nSE_mean\n\n[1] 0.04595372\n\n\nBootstrap 95% Confidence Interval:\n\nc(mean - 2*SE_mean, mean + 2*SE_mean) \n\n[1] 0.4352624 0.6190773\n\n\n\nLakes_Bootstrap_Mean_Plot + \n  geom_segment(aes(x=mean - 2*SE_mean,xend=mean + 2*SE_mean, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nThe standard error and interval are very close to those obtained using the approximation formulas.\n\n\n4.2.5 CI Method Comparison\nWe’ve now seen 3 different ways to obtain confidence intervals based on statistics calculated from data.\nThe table below tells us what must be true of the sampling distribution for a statistic in order to use each technique.\n\n\n\n\n\n\n\n\n\nTechnique\nNo Gaps\nBell-Shaped\nKnown Formula for SE\n\n\n\n\nBootstrap Percentile\nx\n\n\n\n\nBootstrap Standard Error\nx\nx\n\n\n\nTheory-Based\nx\nx\nx",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#t-tests",
    "href": "Ch4.html#t-tests",
    "title": "4  Inference from Models",
    "section": "4.3 t-tests",
    "text": "4.3 t-tests\n\n4.3.1 t-Distribution\nA t-distribution is a symmetric, bell-shaped curve. The t-distribution depends on a parameter called degrees of freedom, which determines the thickness of the distribution’s tails.\nWhen the sampling distribution for a statistic is symmetric and bell-shaped, it can be approximated by a t-distribution. As the sample size increases, so do the degrees of freedom in the t-distribution.\nAs the degrees of freedom grow large, we see that the t-distributions get closer together, converging to a bell-shaped curve. This distribution is called a standard normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 t-tests\nWhen the sampling distribution of a statistic is symmetric and bell-shaped, then the ratio\n\\[\nt= \\frac{{\\text{Statistic}}}{\\text{SE}(\\text{Statistic})}  \n\\]\napproximately follows a t-distribution.\nThe statistic \\(t\\) is called a t-statistic.\nWe’ll use this t-statistic as the test statistic in our hypothesis test.\nThe degrees of freedom are given by \\(n-(p+1)\\), where \\(p\\) represents the number of terms in the model, not including the intercept.\nTo find a p-value, we use a t-distribution to find the probability of obtaining a t-statistic as or more extreme than the one calculated from our data.\nA hypothesis test based on the t-statistic and t-distribution is called a t-test.\n\n\n4.3.3 t-test Examples\n\n4.3.3.1 Example 1: N vs S Lakes\nThe equation of the model is:\n\\[\n\\widehat{\\text{Mercury}} = b_0+b_1\\times\\text{South}\n\\]\nWe fit the model in R and display its summary output below.\n\nLakes_M &lt;- lm(data=FloridaLakes, Mercury~Location)\nsummary(Lakes_M)\n\n\nCall:\nlm(formula = Mercury ~ Location, data = FloridaLakes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.65650 -0.23455 -0.08455  0.24350  0.67545 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)  0.42455    0.05519   7.692 0.000000000441 ***\nLocationS    0.27195    0.08985   3.027        0.00387 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3171 on 51 degrees of freedom\nMultiple R-squared:  0.1523,    Adjusted R-squared:  0.1357 \nF-statistic: 9.162 on 1 and 51 DF,  p-value: 0.003868\n\n\nLooking at the coefficients table, we’ve already seen where the first two columns, Estimate and Std. Error come from. The last two columns, labeled t value and “Pr(&gt;|t|)” represent the t-statistic and p-value for the t-test associated with the that the regression parameter on that line is zero. (i.e. \\(\\beta_j=0\\)).\nt-test for line LocationS\nTo test whether there is evidence of a difference in mercury levels between Northern and Southern Florida, we’ll test the null hypothesis (\\(\\beta_1=0\\)). This corresponds to the locationS line of the R output.\nNull Hypothesis: There is no difference in average mercury levels between Northern and Southern Florida (\\(\\beta_1=0\\)).\nAlternative Hypothesis: There is a difference in average mercury levels in Northern and Southern Florida (\\(\\beta_1\\neq 0\\)).\nIn the previous section, we calculated the standard error for \\(b_1\\) to be 0.0898, respectively.\nTest Statistic: \\(t=\\frac{{b_j}}{\\text{SE}(b_j)} = \\frac{0.27195}{0.0898} = 3.027\\)\nTo calculate the p-value, we plot the t-statistic of 3.027 that we observed in our data and observe where it lies on a t-distribution.\n\nts=3.027\ngf_dist(\"t\", df=51, geom = \"area\", fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=\"red\")  + xlab(\"t\")\n\n\n\n\n\n\n\n\nThe pt function returns the probability of getting a t-statistic higher than 3.027. We multiply by 2 to also get the area in the left tail.\np-value:\n\n2*pt(-abs(ts), df=51)\n\n[1] 0.003866374\n\n\nThe low p-value gives us strong evidence of a difference in average mercury levels between lakes in Northern and Southern Florida.\nComparison to Permutation Test\nRecall the permutation test we previously performed.\n\nNSLakes_SimulationResultsPlot\n\n\n\n\n\n\n\n\np-value:\n\nb1 &lt;- Lakes_M$coef[2] ## record value of b1 from actual data\n\nmean(abs(NSLakes_SimulationResults$b1Sim) &gt; abs(b1))\n\n[1] 0.0039\n\n\nThe p-value from the permutation test is similar to the one from the t-test. Both lead us to the same conclusion.\nHypothesis Test for line (intercept)\nNotice that there is also a t-statistic and p-value on the line (intercept). This is tesing the following hypotheses.\nNull Hypothesis: The average mercury level among all lakes in North Florida is 0 (\\(\\beta_0=0\\)).\nAlternative Hypothesis: The average mercury level among all lakes in Northern Florida is not 0 (\\(\\beta_0\\neq 0\\)).\nWe could carry out this test by dividing the estimate by its standard error and finding the p-value using a t-distribution, just like we did for \\(\\beta_1\\). However, we already know that the average mercury level among all lakes in North Florida is certainly not 0, so this is a silly test, and we shouldn’t conclude much from it.\nNot every p-value that R provides is actually meaningful or informative.\n\n\n4.3.3.2 Example 2: Regression Slope\nWe revisit the model predicting a lake’s mercury level, using pH as the explanatory variable.\n\nM_pH &lt;- lm(data=FloridaLakes, Mercury~pH)\nsummary(M_pH)\n\n\nCall:\nlm(formula = Mercury ~ pH, data = FloridaLakes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.48895 -0.19188 -0.05774  0.09456  0.71134 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)  1.53092    0.20349   7.523 0.000000000814 ***\npH          -0.15230    0.03031  -5.024 0.000006572811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2816 on 51 degrees of freedom\nMultiple R-squared:  0.3311,    Adjusted R-squared:  0.318 \nF-statistic: 25.24 on 1 and 51 DF,  p-value: 0.000006573\n\n\nThe estimated regression equation is\n\\[\n\\text{Mercury} = 1.53 - 0.15 \\times\\text{pH}, \\text{where } \\epsilon\\_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nWe can use \\(b_1\\) to test whether there is evidence of a relationship between mercury concentration and pH level.\nNull Hypothesis: There is no relationship between mercury and pH level among all Florida lakes. (\\(\\beta_1=0\\)).\nAlternative Hypothesis: There is a relationship between mercury and pH level among all Florida lakes. (\\(\\beta_1 \\neq 0\\)).\nIn the last section, we calculated \\(SE(b_1)\\) to be 0.0303.\nTest Statistic: \\(t=\\frac{{b_j}}{\\text{SE}(b_j)} = \\frac{-0.15230}{0.03031} = -5.024\\)\n\nts=5.024\ngf_dist(\"t\", df=51, geom = \"area\", fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=\"red\")  + xlab(\"t\")\n\n\n\n\n\n\n\n\np-value:\n\n2*pt(-abs(ts), df=51)\n\n[1] 0.000006578117\n\n\nThe p-value is extremely small, providing strong evidence of a relationship between pH level and mercury concentration.\nHypothesis Test for Intercept Line\nNull Hypothesis: The average mercury level among all Florida lakes with pH = 0 is 0. (\\(\\beta_0=0\\)).\nAlternative Hypothesis: The average mercury level among all Florida lakes with pH = 0 not 0. (\\(\\beta_0 \\neq 0\\)).\nSince there are no lakes with pH level 0, this is not a meaningful test.\n\n\n\n4.3.4 Limitations of Model-Based Inference\nWe’ve seen that in situations where the sampling distribution for a regression coefficient \\(b_j\\) is symmetric and bell-shaped, we can create confidence intervals and perform hypothesis tests using the t-distribution without performing permutation for hypothesis tests, or bootstrapping for confidence intervals.\nThere are, however, limitations to this approach, which underscore the importance of the simulation-based approaches seen in Chapters 3 and 4.\nThese include:\n\nThere are lots of statistics, like medians and standard deviations, that do not have known standard error formulas, and do not follow symmetric bell-shaped distributions. In more advanced and complicated models, it is common to encounter statistics of interest with unknown sampling distributions. In these cases, we can estimate p-values and build confidence intervals via simulation, even if we cannot identify the distribution by name.\nEven for statistics with known standard error formulas, the t-test is only appropriate when the sampling distribution for \\(b_j\\) is symmetric and bell-shaped. While there is probability theory that shows this will happen when the sample size is “large enough,” there is no set sample size that guarantees this. Datasets with heavier skewness in the response variable will require larger sample sizes than datasets with less skewness in the response.\nThe simulation-based approaches provide valuable insight to the logic behind hypothesis tests. When we permute values of an explanatory variable in a hypothesis test it is clear that we are simulating a situation where the null hypothesis is true. Likewise, when we simulate taking many samples in bootstrapping, it is clear that we are assessing the variability in a statistic across samples. Simply jumping to the t-based approximations of these distributions makes it easy to lose our sense of what they actually represent, and thus increases the likelihood of interpreting them incorrectly.\n\nIn fact prominent statistician R.A. Fisher wrote of simulation-based methods in 1936:\n“Actually, the statistician does not carry out this very simple and very tedious process, but his conclusions have no justification beyond the fact that they agree with those which could have been arrived at by this elementary method.”\nFisher’s comment emphasizes the fact that probability-based tests, like the t-test are simply approximations to what we would obtain via simulation-based approaches, which were not possible in his day, but are now.\nProponents of simulation-based inference include Tim Hesterberg, Senior Statistician at Instacart, and former Senior Statistician at Google, which heavily used simulation-based tests associated with computer experiments associated with their search settings. Hesterberg wrote a 2015 paper, arguing for the use and teaching of simulation-based techniques.\nWe will move forward by using probability-based inference where appropriate, while understanding that we are merely approximating what we would obtain via simulation. Meanwhile, we’ll continue to employ simulation-based approaches where probability-based techniques are inappropriate or unavailable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#f-tests",
    "href": "Ch4.html#f-tests",
    "title": "4  Inference from Models",
    "section": "4.4 F-tests",
    "text": "4.4 F-tests\nJust as we’ve seen that the ratio of a regression statistic to its standard error follows a t-distribution when can be thought of as having come from a process that can be approximated with the normal error regression model, F-statistics also follow a known probability distribution under this assumption.\n\n4.4.1 F-Distribution\nAn F distribution is a right-skewed distribution. It is defined by two parameters, \\(\\nu_1, \\nu_2\\), called numerator and denominator degrees of freedom.\n\n\n\n\n\n\n\n\n\nUnder certain conditions (which we’ll examine in the next chapter), the F-statistic\n\\[\nF=\\frac{\\frac{\\text{Unexplained Variability in Reduced Model}-\\text{Unexplained Variability in Full Model}}{p-q}}{\\frac{\\text{Unexplained Variability in Full Model}}{n-(p+1)}}\n\\]\nfollows an F-distribution.\nThe numerator and denominator degrees of freedom are given by \\(p-q\\) and \\(n-(p+1)\\), respectively. These are the same values we divided by when computing the F-statistic.\n\n\n4.4.2 House Condition Example\nRecall the F-statistic for comparing prices of houses in either very good, good, or average condition, and the p-value associated with the simulation-based F-test we performed previously.\nNull Hypothesis: There is no difference in average prices between houses of the three different conditions, among all houses in King County, WA.\nAlternative Hypothesis: There is a difference in average prices between houses of the three different conditions, among all houses in King County, WA.\nReduced Model: \\(\\text{Price}= \\beta_0 + \\epsilon_i , \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\)\nFull Model: \\(\\text{Price}= \\beta_0+ \\beta_1 \\times\\text{good condition}+ \\beta_2\\times\\text{very good condition} + \\epsilon_i , \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\)\n\\[\n\\begin{aligned}\nF &= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\\n&=\\frac{\\frac{69,045,634-68,195,387}{2-0}}{\\frac{68,195,387}{100-(2+1)}} \\\\\n\\end{aligned}\n\\]\n\n((SST - SSR_cond)/(2-0))/(SSR_cond/(100-(2+1)))\n\n[1] 0.6046888\n\n\nThe results of the simulation-based hypothesis test are shown below.\n\nHouse_Cond_SimulationResults_Plot\n\n\n\n\n\n\n\n\nsimulation-based p-value:\n\nmean(FSim &gt; Fstat)\n\n[1] 0.5548\n\n\nNow, we calculate the p-value using the probability-based F-distribution.\n\nts=0.605\ngf_dist(\"f\", df1=2, df2=97, geom = \"area\", fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts), color=\"red\")  + xlab(\"F\")\n\n\n\n\n\n\n\n\np-value:\n\n1-pf(ts, df1=2, df2=97)\n\n[1] 0.5481219\n\n\nThe p-value we obtained is very similar to the one we obtained using the simulation-based test.\nWe can obtain this p-value directly using the anova command.\n\nM_cond &lt;- lm(data=Houses, price ~ condition)\nM0 &lt;- lm(data=Houses, price ~ 1)\nanova(M0, M_cond)\n\nAnalysis of Variance Table\n\nModel 1: price ~ 1\nModel 2: price ~ condition\n  Res.Df      RSS Df Sum of Sq      F Pr(&gt;F)\n1     99 69045634                           \n2     97 68195387  2    850247 0.6047 0.5483\n\n\nSince the p-value is large, we do not have enough evidence to say that average price differs between conditions of the houses.\n\n\n4.4.3 Interaction Example\nWe can also use an F-test to compare a model predicting house prices with an interaction term to one without one.\nReduced Model: \\(\\text{Price}= \\beta_0+ \\beta_1 \\times\\text{sqft\\_living} + \\beta_2\\times\\text{Waterfront} + \\epsilon_i , \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\)\nFull Model: \\(\\text{Price}= \\beta_0+ \\beta_1 \\times\\text{sqft\\_living}+ \\beta_2\\times\\text{Waterfront} + \\beta_3\\times\\text{sqft\\_living}\\times\\text{Waterfront} + \\epsilon_i , \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\)\n\\[\n\\begin{aligned}\nF &= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\\n&=\\frac{\\frac{16,521,296-10,139,974}{3-2}}{\\frac{10,139,974}{100-(3+1)}} \\\\\n\\end{aligned}\n\\]\n\n((SSR_wf_sqft-SSR_int)/(3-2))/((SSR_int)/(100-(3+1)))\n\n[1] 60.41505\n\n\n\nts=60.41505\ngf_dist(\"f\", df1=1, df2=96, geom = \"area\", fill = ~ (abs(x)&gt;! abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts), color=\"red\")  + xlab(\"F\")\n\n\n\n\n\n\n\n\np-value:\n\n1-pf(ts, df1=1, df2=96)\n\n[1] 0.000000000008572476\n\n\nThe probability-based F-test is used in the anova command.\n\nM_wf_SqFt &lt;- lm(data=Houses, price~sqft_living + waterfront)\nM_House_Int &lt;- lm(data=Houses, price~sqft_living * waterfront)\nanova(M_wf_SqFt, M_House_Int)\n\nAnalysis of Variance Table\n\nModel 1: price ~ sqft_living + waterfront\nModel 2: price ~ sqft_living * waterfront\n  Res.Df      RSS Df Sum of Sq      F            Pr(&gt;F)    \n1     97 16521296                                          \n2     96 10139974  1   6381323 60.415 0.000000000008572 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince the p-value is small, we have enough evidence to say that there is evidence of an interaction between size and whether or not the house is on the waterfront.\nNotice that this p-value is identical to the one we obtained in the previous section, using the t-test for interaction associated with the lm command.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#lm-summary-in-r",
    "href": "Ch4.html#lm-summary-in-r",
    "title": "4  Inference from Models",
    "section": "4.5 lm Summary in R",
    "text": "4.5 lm Summary in R\nWe’ve now seen how to obtain all of the quantities shown in the lm summary output in R. This section does not provide any new information, but brings together all of the information from the previous sections and chapters to show, in one place, how the lm summary output is obtained.\n\n4.5.1 lm summary Output\nThe summary command for a linear model in R displays a table including 4 columns.\nLinear Model summary() Output in R\n\nEstimate gives the least-squares estimates \\(b_0, b_1, \\ldots, b_p\\)\nStandard Error gives estimates of the standard deviation in the sampling distribution for estimate. It tells us how the estimate is expected to vary between different samples of the given size. These are computed using the formulas in Section 4.2.\nt value is the estimate divided by its standard error.\nPr(&gt;|t|) is a p-value for the hypothesis test associated with the null hypothesis \\(\\beta_j = 0\\), where \\(\\beta_j\\) is the regression coefficient pertaining to the given line. Note that \\(\\beta_j\\) is the unknown population parameter estimated by \\(b_j\\).\nThe Residual Standard Error is \\(s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\). This is an estimate of \\(\\sigma\\), which represents the standard deviation in the distribution of the response variable for given value(s) or category(ies) of explanatory variable(s). It tells us how much variability is expected in the response variable between different individuals with the same values/categories of the explanatory variables.\nThe degrees of freedom are \\(n-(p+1)\\).\nThe Multiple R-Squared value is the \\(R^2\\) value seen in Chapter 2. \\(R^2 = \\frac{\\text{SST} -\\text{SSR}}{\\text{SST}} = \\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y}_i)^2}\\)\nWe know that \\(R^2\\) can never decrease when additional variables are added to a model. The Adjusted-R^2 value is an alternate version of \\(R^2\\) that is designed to penalize adding variables that do little to explain variation in the response.\nThe F-statistic on the bottom line of the R-output corresponds to an F-test of the given model against a reduced model that include no explanatory variables. The p-value on this line is associated with the test of the null hypothesis that there is no relationship between the response variable and any of the explanatory variables. Since SSR for this reduced model is equal to SST, the F-statistic calculation simplifies to:\n\n\\[ F=\\frac{\\frac{SST - SSR}{p}}{\\frac{SSR}{n-(p+1)}} \\]\nThe degrees of freedom associated with the F-statistic are given by \\(p\\) and \\((n-(p+1))\\).\nExample: Northern vs Southern Florida Lakes\nRecall our linear model for mercury levels of lakes in Northern Florida, compared to Southern Florida.\nThe equation of the model is:\n\\[\\text{Mercury} = \\beta\\_0+\\beta_1\\times\\text{South} + \\epsilon_i, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\]\nWe fit the model in R and display its summary output below.\n\nLakes_M &lt;- lm(data=FloridaLakes, Mercury~Location)\nsummary(Lakes_M)\n\n\nCall:\nlm(formula = Mercury ~ Location, data = FloridaLakes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.65650 -0.23455 -0.08455  0.24350  0.67545 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)  0.42455    0.05519   7.692 0.000000000441 ***\nLocationS    0.27195    0.08985   3.027        0.00387 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3171 on 51 degrees of freedom\nMultiple R-squared:  0.1523,    Adjusted R-squared:  0.1357 \nF-statistic: 9.162 on 1 and 51 DF,  p-value: 0.003868\n\n\nThe estimated regression equation is\n\\[ \\text{Mercury} = 0.42455+0.27195\\times\\text{South}, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma) \\]\n\nSSR is:\n\n\nsum(Lakes_M$residuals^2)\n\n[1] 5.126873\n\n\n\nSST is:\n\n\nsum((FloridaLakes$Mercury - mean(FloridaLakes$Mercury))^2)\n\n[1] 6.047875\n\n\n\nThe residual standard error \\(s\\) is our estimate of the standard deviation among lakes in the same location (either Northern or Southern Florida).\n\n\\[ s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{5.126873}{53-(1+1)}}=0.3171 \\]\nThe degrees of freedom associated with this estimate is \\(53-(1+1) = 51\\).\n\nThe Multiple R-Squared is:\n\n\\[ R^2 = \\frac{6.047875 - 5.126873}{6.047875} = 0.1523 \\]\n\nThe F-statistic is\n\n\\[ F=\\frac{\\frac{SST - SSR}{p}}{\\frac{SSR}{n-(p+1)}} = \\frac{\\frac{6.047875 - 5.126873}{1}}{\\frac{5.126873}{53-(1+1)}} = 9.162 \\]\nThis F-statistic is associated with 1 and 51 degrees of freedom.\nUsing formulas in Section 4.2, we obtain the standard error estimates for \\(b_0\\) and \\(b_1\\), given in the second column of the table.\n\\[SE(b_0) = SE(\\bar{x}\\_N)=s\\frac{1}{\\sqrt{n_{\\text{North}}}} = \\frac{0.3171}{\\sqrt{33}} =0.0552 \\]\n\\(SE(b_0)\\) represents the variability in average mercury levels between different samples of 33 Northern Florida lakes.\n\\[\nSE(b_1) = SE(\\bar{x}*{South}-*\\bar{x}{North})=s\\sqrt{\\frac{1}{n_{North}}+\\frac{1}{n_{South}}} = 0.3171\\sqrt{\\frac{1}{20} + \\frac{1}{33}} =0.0898\n\\]\n\\(SE(b_1)\\) represents the variability in average difference in mercury levels between northern and southern lakes between different samples of 33 Northern Florida lakes and 20 Southern Florida lakes.\nThe last column, labeled “Pr(&gt;|t|)” is, in fact a p-value associated with associated with the null hypothesis that the regression parameter on that line is zero. (i.e. \\(\\beta_j=0\\)).\nHypothesis Test for line (intercept)\nNull Hypothesis: The average mercury level among all lakes in North Florida is 0 (\\(\\beta_0=0\\)).\nAlternative Hypothesis: The average mercury level among all lakes in Northern Florida is not 0 (\\(\\beta_0\\neq 0\\)).\nWe already know the average mercury level among all lakes in North Florida is not 0, so this is a silly test.\nHypothesis Test for line LocationS\nNull Hypothesis: There is no difference in average mercury levels between Northern and Southern Florida (\\(\\beta_1=0\\)).\nAlternative Hypothesis: There is a difference in average mercury levels in Northern and Southern Florida (\\(\\beta_1\\neq 0\\)).\n\n\n4.5.2 Difference in Means Example\nRecall the hypothesis test we performed to investigate whether there is a difference in average mercury level between lakes in Northern Florida and Southern Florida.\nNull Hypothesis: There is no difference in average mercury levels between Northern and Southern Florida (\\(\\beta_1=0\\)).\nAlternative Hypothesis: There is a difference in average mercury levels in Northern and Southern Florida (\\(\\beta_1\\neq 0\\)).\nTest Statistic: \\(t=\\frac{{b_j}}{\\text{SE}(b_j)} = \\frac{0.27195}{0.08985} = 3.027\\)\nKey Question: What is the probability of getting a t-statistic as extreme as 3.027 if \\(\\beta_1=0\\) (i.e. there is no difference in mercury levels between northern and southern lakes).\nWe plot the t-statistic of 3.027 that we observed in our data and observe where it lies on a t-distribution.\n\nts=3.027\ngf_dist(\"t\", df=51, geom = \"area\", fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=\"red\")  + xlab(\"t\")\n\n\n\n\n\n\n\n\n\n2*pt(-abs(ts), df=51)\n\n[1] 0.003866374\n\n\nThe low p-value gives us strong evidence of a difference in average mercury levels between lakes in Northern and Southern Florida.\nThis is the p-value reported in R’s lm summary() output.\nA t-statistic more extreme than \\(\\pm 2\\) will roughly correspond to a p-value less than 0.05.\nPermutation Test\nLet’s compare these results to those given by the permutation test.\n\nNSLakes_SimulationResultsPlot\n\n\n\n\n\n\n\n\np-value:\n\nb1 &lt;- Lakes_M$coef[2] ## record value of b1 from actual data\n\nmean(abs(NSLakes_SimulationResults$b1Sim) &gt; abs(b1))\n\n[1] 0.0039\n\n\n\n\n4.5.3 Simple Linear Regression Example\nWe examine the model summary output for the model predicting a lake’s mercury level, using pH as the explanatory variable.\n\nM_pH &lt;- lm(data=FloridaLakes, Mercury~pH)\nsummary(M_pH)\n\n\nCall:\nlm(formula = Mercury ~ pH, data = FloridaLakes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.48895 -0.19188 -0.05774  0.09456  0.71134 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)  1.53092    0.20349   7.523 0.000000000814 ***\npH          -0.15230    0.03031  -5.024 0.000006572811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2816 on 51 degrees of freedom\nMultiple R-squared:  0.3311,    Adjusted R-squared:  0.318 \nF-statistic: 25.24 on 1 and 51 DF,  p-value: 0.000006573\n\n\nThe estimated regression equation is\n\\[ \\text{Mercury} = 1.53 - 0.15 \\times\\text{pH}, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma) \\]\n\nSSR is:\n\n\nsum(M_pH$residuals^2)\n\n[1] 4.045513\n\n\n\nSST is:\n\n\nsum((FloridaLakes$Mercury - mean(FloridaLakes$Mercury))^2)\n\n[1] 6.047875\n\n\n\nThe residual standard error \\(s\\) is our estimate of \\(\\sigma\\), the standard deviation among lakes with the same pH.\n\n\\[ s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{4.045513}{53-(1+1)}}=0.2816 \\]\nThe degrees of freedom associated with this estimate is \\(53-(1+1) = 51\\).\n\nThe Multiple R-Squared is:\n\n\\[ R^2 = \\frac{6.047875 - 4.045513}{6.047875} = 0.3311 \\]\n\nThe F-statistic is\n\n\\[ F=\\frac{\\frac{SST - SSR}{p}}{\\frac{SSR}{n-(p+1)}} = \\frac{\\frac{6.047875 - 4.045513}{1}}{\\frac{4.045513}{53-(1+1)}} = 25.24 \\]\nThis F-statistic is associated with 1 and 51 degrees of freedom.\nUsing formulas in Section 4.2, we obtain the standard error estimates for \\(b_0\\) and \\(b_1\\), given in the second column of the table.\nTo do this, we need to calculate \\(\\bar{x}\\) and \\(\\sum(x_i-\\bar{x})^2\\), where \\(x\\) represents the explanatory variable, \\(pH\\).\n\nmean(FloridaLakes$pH)\n\n[1] 6.590566\n\n\n\nsum((FloridaLakes$pH-mean(FloridaLakes$pH))^2)\n\n[1] 86.32528\n\n\n\\[ SE(b_0)=s\\sqrt{\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}} = 0.2816\\sqrt{\\frac{1}{53} + \\frac{6.59^2}{86.32528} } = 0.2034 \\]\n\\(SE(b_0)\\) represents the variability in mercury levels among lakes with pH of 0 between different samples of size 53. Since we don’t have any lakes with pH of 0, this is not a meaningful calculation.\n\\[ SE(b_1)=\\sqrt{\\frac{s^2}{\\sum(x_i-\\bar{x})^2}}=\\sqrt{\\frac{0.2816^2}{86.32528}} = 0.0303 \\]\n\\(SE(b_1)\\) represents the variability in rate of change in mercury level for each additional one unit increase in pH, between different samples of size 53.\nHypothesis Test for Intercept Line\nNull Hypothesis: The average mercury level among all Florida lakes with pH = 0 is 0. (\\(\\beta_0=0\\)).\nAlternative Hypothesis: The average mercury level among all Florida lakes with pH = 0 not 0. (\\(\\beta_0 \\neq 0\\)).\nSince there are no lakes with pH level 0, this is not a meaningful test.\nHypothesis Test for pH Line\nNull Hypothesis: There is no relationship between mercury and pH level among all Florida lakes. (\\(\\beta_1=0\\)).\nAlternative Hypothesis: There is a relationship between mercury and pH level among all Florida lakes. (\\(\\beta_1 \\neq 0\\)).\nTest Statistic: \\(t=\\frac{{b_j}}{\\text{SE}(b_j)} = \\frac{-0.15230}{0.03031} = -5.024\\)\n\nts=5.024\ngf_dist(\"t\", df=51, geom = \"area\", fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=\"red\")  + xlab(\"t\")\n\n\n\n\n\n\n\n\n\n2*pt(-abs(ts), df=51)\n\n[1] 0.000006578117\n\n\nThe p-value is extremely small, just as the simulation-based p-value we saw in Chapter 3.\n\n\n4.5.4 Multiple Regression Example\nWe perform hypothesis tests on a model predicting house price using square feet and waterfront status as explanatory variables.\n\nM_wf_sqft &lt;- lm(data=Houses, price~sqft_living+waterfront)\nsummary(M_wf_sqft)\n\n\nCall:\nlm(formula = price ~ sqft_living + waterfront, data = Houses)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1363.79  -251.55    59.28   177.58  1599.72 \n\nCoefficients:\n               Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)   -407.6549    86.2868  -4.724        0.00000779668 ***\nsqft_living      0.4457     0.0353  12.626 &lt; 0.0000000000000002 ***\nwaterfrontYes  814.3613   124.8546   6.522        0.00000000313 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 412.7 on 97 degrees of freedom\nMultiple R-squared:  0.7607,    Adjusted R-squared:  0.7558 \nF-statistic: 154.2 on 2 and 97 DF,  p-value: &lt; 0.00000000000000022\n\n\nWe won’t go through the standard error calculations here, though the details are given in the link provided in Section 4.2.\nIntercept Line:\nNull Hypothesis The average price among all non-waterfront houses with 0 square feet is 0 dollars. (\\(\\beta_0=0\\))\nThis is not a sensible hypothesis to test.\nsqft_living Line:\nNull Hypothesis There is no relationship between price and square feet in a house, after accounting for waterfront status. (\\(\\beta_1=0\\))\nThe large t-statistic (12.626) and small p-value provide strong evidence against this null hypothesis.\nWe know that a small p-value alone does not provide evidence of a relationship that is practically meaningful, but since our model estimates an expected 45 thousand dollar increase for each additional 100 square feet, this seems like a meaningful relationship.\nwaterfrontYes Line:\nNull Hypothesis On average, there is no difference between average price of waterfront and non-waterfront houses, assuming they are the same size. (\\(\\beta_2=0\\))\nThe large t-statistic (6.522) and small p-value provide strong evidence against this null hypothesis. Waterfront houses are estimated to cost 814 thousand dollars more, on average, than non-waterfront houses of the same size.\n\n\n4.5.5 MR with Interaction Example\n\nM_House_Int &lt;- lm(data=Houses, price ~ sqft_living * waterfront)\nsummary(M_House_Int)\n\n\nCall:\nlm(formula = price ~ sqft_living * waterfront, data = Houses)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1559.34  -114.93   -30.24   131.09  1266.58 \n\nCoefficients:\n                            Estimate Std. Error t value         Pr(&gt;|t|)    \n(Intercept)                 67.39594   91.39267   0.737           0.4627    \nsqft_living                  0.21837    0.04035   5.412 0.00000045752269 ***\nwaterfrontYes             -364.59498  180.75875  -2.017           0.0465 *  \nsqft_living:waterfrontYes    0.43267    0.05566   7.773 0.00000000000857 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 325 on 96 degrees of freedom\nMultiple R-squared:  0.8531,    Adjusted R-squared:  0.8486 \nF-statistic: 185.9 on 3 and 96 DF,  p-value: &lt; 0.00000000000000022\n\n\nIntercept Line:\nNull Hypothesis The average price among all non-waterfront houses with 0 square feet is 0 dollars. (\\(\\beta_0=0\\))\nThis is not a sensible hypothesis to test.\nsqft_living Line:\nNull Hypothesis There is no relationship between price and square feet among non-waterfront houses. (\\(\\beta_1=0\\))\nThe large t-statistic (5.412) and small p-value provide strong evidence against this null hypothesis.\nwaterfrontYes Line:\nNull Hypothesis On average, there is no difference between average price of waterfront and non-waterfront houses with 0 square feet. (\\(\\beta_2=0\\))\nThis is not a sensible hypothesis to test.\nsqft_living:waterfrontYes\nNull Hypothesis: There is no interaction between square feet and waterfront. (\\(\\beta_3=0\\)) (That is, the effect of size on price is the same for waterfront and non-waterfront houses).\nThe large t-statistic (7.773) and small p-value provide strong evidence against this null hypothesis. It appears there really is evidence of an interaction between price and waterfront status, as we previously suspected, based on graphical representation and background knowledge.\nNote that if the interaction term had yielded a large p-value, indicating a lack of evidence of an interaction, we might have wanted to drop the interaction term from the model, in order to make the interpretations of the other estimates and hypothesis tests simpler.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#intervals-for-expected-response",
    "href": "Ch4.html#intervals-for-expected-response",
    "title": "4  Inference from Models",
    "section": "4.6 Intervals for Expected Response",
    "text": "4.6 Intervals for Expected Response\n\n4.6.1 Parameter Values and Expected Responses\nRecall that in Chapter 4, we saw two different types of confidence intervals. One type was for regression parameters, \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\), using estimate \\(b_0, b_1, \\ldots, b_p\\). The other type was for expected responses, which involved estimating linear functions of these parameters, for example \\(\\beta_0 + 7\\beta_1\\).\nUnder the assumptions of a normal error regression model, we an approximate 95% confidence interval for regression parameter \\(\\beta_j\\) is given by\n\\[ b_j + \\pm t^*\\text{SE}(b_j), \\]\nwhere \\(t^*\\approx 2\\).\nWe’ve seen that in R, confidence intervals for regression parameters can be obtained through the confint() command.\nA 95% confidence interval for an expected response \\(E(Y_i|X_{i1}=x_{i1}, \\ldots X_{ip}=x_{ip}) = \\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_px_{ip}\\) is estimated by\n\\[ b_0 + b_1x_{i1} + \\ldots + b_px_{ip} \\pm t^*\\text{SE}(b_0 + b_1x_{i1} + \\ldots + b_px_{ip}), \\]\nIn this section, we’ll look more into confidence intervals for expected responses and also another kind of interval involving expected responses, called a prediction interval.\nWell sometimes write \\(E(Y_i|X_{i1}=x_{i1}, \\ldots X_{ip}=x_{ip})\\) as \\(E(Y|X)\\).\n\n\n4.6.2 Estimation and Prediction\nRecall the ice cream dispenser that is known to dispense ice cream at a rate of 2 oz. per second on average, with individual amounts varying according to a normal distribution with mean 0 and standard deviation 0.5\nConsider the following two questions:\n\nOn average, how much ice cream will be dispensed for people who press the dispenser for 1.5 seconds?\nIf a single person presses the dispenser for 1.5 seconds, how much ice cream will be dispensed?\n\nThe first question is one of estimation. The second pertains to prediction.\nWhen estimating expected responses and making predictions on new observations, there are two sources of variability we must consider.\n\nWe are using data to estimate \\(\\beta_0\\) and \\(\\beta_1\\), which introduces sampling variability.\n\nEven if we did know \\(\\beta_0\\) and \\(\\beta_1\\), there is variability in individual observations, which follows a \\(\\mathcal{N}(0, \\sigma)\\) distribution.\n\nIn an estimation problem, we only need to think about (1). When predicting the value of a single new observation, we need to think about both (1) and (2).\nThus, intervals for predictions of individual observations carry more uncertainty and are wider than confidence intervals for \\(E(Y|X)\\).\n\n\n\n\n\n\n\n\n\n\n4.6.2.1 Example: Ice Cream Machine\nIn the estimation setting, we are trying to determine the location of the regression line for the entire population. Uncertainty comes from the fact that we only have data from a sample.\nIn the ice cream example, we can see that the blue line, fit to our data, is a good approximation of the “true” regression line that pertains to the mechanism from which the data were generated. It does, however, vary from the red line slightly due to sampling variability.\n\nggplot(data=Icecream1, aes(x=time, y=amount)) + geom_point() + ggtitle(\"Icecream Dispensed\") + xlab(\"Time Pressing dispenser\") + ylab(\"Amount Dispensed\") + geom_abline(slope=2, intercept=0, color=\"red\") + stat_smooth(method=\"lm\")\n\n\n\n\n\n\n\n\n\nsummary(IC_Model)\n\n\nCall:\nlm(formula = lm(amount ~ time), data = Icecream1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8645 -0.3553  0.0685  0.2252  0.6963 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)  -0.1299     0.3968  -0.327        0.749    \ntime          2.0312     0.1598  12.714 0.0000000104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4527 on 13 degrees of freedom\nMultiple R-squared:  0.9256,    Adjusted R-squared:  0.9198 \nF-statistic: 161.6 on 1 and 13 DF,  p-value: 0.00000001042\n\n\n\nb0 &lt;- IC_Model$coefficients[1]\nb1 &lt;- IC_Model$coefficients[2]\ns &lt;- sigma(IC_Model)\n\nThe first question:\n“On average, how much ice cream will be dispensed for people who press the dispenser for 1.5 seconds?”\nis a question of estimation. It is of the form, for a given \\(X\\), on average what do we expect to be true of \\(Y\\).\nIn the ice cream question, we can answer this exactly, since we know \\(\\beta_0\\) and \\(\\beta_1\\).\nIn a real situation, we don’t know these and have to estimate them from the data, which introduces uncertainty.\nConfidence interval for \\(E(Y | (X=x))\\):\n\\[\n\\begin{aligned}\n& b_0+b_1x^* \\pm t^*SE(\\hat{Y}|X=x^*) \\\\\n& b_0+b_1x^* \\pm t^*\\sqrt{\\widehat{Var}(\\hat{Y}|X=x^*)}\n\\end{aligned}\n\\]\nThe second question is a question of prediction. Even if we knew the true values of \\(beta_0\\) and \\(\\beta_1\\), we would not be able to given the exact amount dispensed for an individual user, since this varies between users.\nPrediction interval for \\(E(Y | (X=x))\\):\n\\[\n\\begin{aligned}\n& b_0+b_1x^* \\pm t^*\\sqrt{\\widehat{Var}(\\hat{Y}|X=x^*) + s^2}\n\\end{aligned}\n\\]\nThe extra \\(s^2\\) in the calculation of prediction variance comes from the uncertainty associated with individual observations.\n\n\n\n4.6.3 Intervals in R\nIn R, we can obtain confidence intervals for an expected response and prediction intervals for an individual response using the predict command, with either interval=\"confidence\" or interval=\"prediction\".\n\npredict(IC_Model, newdata=data.frame(time=1.5), interval = \"confidence\", level=0.95)\n\n       fit      lwr      upr\n1 2.916965 2.523728 3.310201\n\n\nWe are 95% confident that the mean amount of ice cream dispensed when the dispenser is held for 1.5 seconds is between 2.52 and 3.31 oz.\n\npredict(IC_Model, newdata=data.frame(time=1.5), interval = \"prediction\", level=0.95)\n\n       fit      lwr      upr\n1 2.916965 1.862832 3.971097\n\n\nWe are 95% confident that in individual who holds the dispenser for 1.5 seconds will get between 1.86 and 3.97 oz of ice cream.\n\n\n\n\n\n\n\n\n\nThe prediction interval (in red) is wider than the confidence interval (in blue), since it must account for variability between individuals, in addition to sampling variability.\n\n\n\n\n\n\n\n\n\n\n\n4.6.4 SLR Calculations (Optional)\nIn simple linear regression,\n\\[\n\\begin{aligned}\nSE(\\hat{Y}|X=x^*) = \\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}}\n\\end{aligned}\n\\]\nThus a confidence interval for \\(E(Y | (X=x))\\) is:\n\\[\n\\begin{aligned}\n& b_0+b_1x^* \\pm t^*SE(\\hat{Y}|X=x^*) \\\\\n& = b_0+b_1x^* \\pm t^*s\\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}}  \\\n\\end{aligned}\n\\]\nA prediction interval for \\(E(Y | (X=x))\\) is:\n\\[\n\\beta_0 + \\beta_1x^* \\pm t^* s\\sqrt{\\left(\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}\\right) + 1}\n\\]\nCalculations in Ice cream example\nFor \\(x=1.5\\), a confidence interval is:\n\\[\n\\begin{aligned}\n& b_0+b_1x^* \\pm t^*SE(\\hat{Y}|X=x^*) \\\\\n& = b_0+b_1x^* \\pm 2s\\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}}  \\\\\n& = -0.1299087 + 2.0312489 \\pm 20.4527185 \\sqrt{\\frac{1}{15}+ \\frac{(1.5-2.3733)^2}{8.02933}}\n\\end{aligned}\n\\]\nA prediction interval is:\n\\[\n\\begin{aligned}\n& b_0+b_1x^* \\pm t^*SE(\\hat{Y}|X=x^*) \\\\\n& = b_0+b_1x^* \\pm 2s\\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}}  \\\\\n& = -0.1299087 + 2.0312489 \\pm 20.4527185 \\sqrt{\\left(\\frac{1}{15}+ \\frac{(1.5-2.3733)^2}{8.02933}\\right)+1}\n\\end{aligned}\n\\]\n\n\n4.6.5 Car Price and Acceleration Time\nWe consider data from the Kelly Blue Book, pertaining to new cars, released in 2015. We’ll investigate the relationship between price, length, and time it takes to accelerate from 0 to 60 mph.\nPrice represents the price of a standard (non-luxury) model of a car. Acc060 represents time it takes to accelerate from 0 to 60 mph.\n\nCarsA060 &lt;- ggplot(data=Cars2015, aes(x=Acc060, y=Price)) + geom_point() \nCarsA060 + stat_smooth(method=\"lm\", se=FALSE)\n\n\n\n\n\n\n\n\n\\(Price = \\beta_0 + \\beta_1\\times\\text{Acc. Time} , \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\)\nThe model assumes expected price is a linear function of acceleration time.\nParameter Interpretations:\n\\(\\beta_0\\) represents intercept of regression line, i.e. expected price of a car that can accelerate from 0 to 60 mph in no time. This is not a meaningful interpretation in context.\n\\(\\beta_1\\) represents slope of regression line, i.e. expected change in price for each additional second it takes to accelerate from 0 to 60 mph.\n\nCars_M_A060 &lt;- lm(data=Cars2015, Price~Acc060)\nsummary(Cars_M_A060)\n\n\nCall:\nlm(formula = Price ~ Acc060, data = Cars2015)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.512  -6.544  -1.265   4.759  27.195 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  89.9036     5.0523   17.79 &lt;0.0000000000000002 ***\nAcc060       -7.1933     0.6234  -11.54 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.71 on 108 degrees of freedom\nMultiple R-squared:  0.5521,    Adjusted R-squared:  0.548 \nF-statistic: 133.1 on 1 and 108 DF,  p-value: &lt; 0.00000000000000022\n\n\n** Model Interpretations**\n\\(\\widehat{Price} = b_0 + b_1\\times\\text{Acc. Time}\\)\n\\(\\widehat{Price} = 89.90 - 7.193\\times\\text{Acc. Time}\\)\n\nIntercept \\(b_0\\) might be interpreted as the price of a car that can accelerate from 0 to 60 in no time, but this is not a meaningful interpretation since there are no such cars.\n\\(b_1=-7.1933\\) tells us that on average, the price of a car is expected to decrease by 7.19 thousand dollars for each additional second it takes to accelerate from 0 to 60 mph.\n\\(R^2 = 0.5521\\) tells us that 55% of the variation in price is explained by the linear model using acceleration time as the explanatory variable.\n\n\nWhat is a reasonable range for the average price of all new 2015 cars that can accelerate from 0 to 60 mph in 7 seconds?\nIf a car I am looking to buy can accelerate from 0 to 60 mph in 7 seconds, what price range should I expect?\n\nWhat is a reasonable range for the average price of all new 2015 cars that can accelerate from 0 to 60 mph in 7 seconds?\n\npredict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=\"confidence\", level=0.95)\n\n      fit      lwr      upr\n1 39.5502 37.21856 41.88184\n\n\nWe are 95% confident that the average price of new 2015 cars that accelerate from 0 to 60 mph in 7 seconds is between 37.2 and 41.9 thousand dollars.\nNote: this is a confidence interval for \\(\\beta_0 -7\\beta_1\\).\nIf a car I am looking to buy can accelerate from 0 to 60 mph in 7 seconds, what price range should I expect?\n\npredict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=\"prediction\", level=0.95)\n\n      fit      lwr      upr\n1 39.5502 18.19826 60.90215\n\n\nWe are 95% confident that a single new 2015 car that accelerates from 0 to 60 mph in 7 seconds will cost between 18.2 and 60.9 thousand dollars.\n\n\n\n\n\n\n\n\n\n\n\n4.6.6 Florida Lakes Est. and Pred.\n\nCalculate an interval that we are 95% confident contains the mean mercury concentration for all lakes in Northern Florida. Do the same for Southern Florida.\nCalculate an interval that we are 95% confident contains the mean mercury concentration for an individual lake in Northern Florida. Do the same for a lake in Southern Florida.\n\n\npredict(Lakes_M, newdata=data.frame(Location=c(\"N\", \"S\")), interval=\"confidence\", level=0.95)\n\n        fit       lwr       upr\n1 0.4245455 0.3137408 0.5353501\n2 0.6965000 0.5541689 0.8388311\n\n\nWe are 95% confident that the mean mercury level in North Florida is between 0.31 and 0.54 ppm.\nWe are 95% confident that the mean mercury level in South Florida is between 0.55 and 0.84 ppm.\nNote: these are confidence intervals for \\(\\beta_0\\), and \\(\\beta_0 + \\beta_1\\), respectively.\n\npredict(Lakes_M, newdata=data.frame(Location=c(\"N\", \"S\")), interval=\"prediction\", level=0.95)\n\n        fit         lwr      upr\n1 0.4245455 -0.22155101 1.070642\n2 0.6965000  0.04425685 1.348743\n\n\nWe are 95% confident that an individual lake in North Florida will have mercury level between 0 and 1.07 ppm.\nWe are 95% confident that the mean mercury level in South Florida is between 0.04 and 1.35 ppm.\nNote that the normality assumption, which allows for negative mercury levels leads to a somewhat nonsensical result.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#the-regression-effect",
    "href": "Ch4.html#the-regression-effect",
    "title": "4  Inference from Models",
    "section": "4.7 The Regression Effect",
    "text": "4.7 The Regression Effect\n\n4.7.1 The Regression Effect\nYou might be wondering how regression gets its name. It comes from a well known phenomenon, known as “regression to the mean”, or the “regression effect”. While the word “regression” is often construed with a negative context (i.e. getting worse), it could also refer to movement in the positive direction.\nExam 1 vs Exam 2 scores for intro stat students at another college\n\n\n\n\n\n\n\n\n\nWhat is the relationship between scores on the two exams?\n\n\n4.7.2 The Regression Effect\nExam 1 vs Exam 2 scores for intro stat students at another college\n\n\n\n\n\n\n\n\n\nHow many of the 6 students who scored below 70 on Exam 1 improved their scores on Exam 2?\nHow many of the 7 students who scored above 90 improved on Exam 2?\n\n\n4.7.3 The Regression Effect\nA low score on an exam is often the result of both poor preparation and bad luck.\nA high score often results from both good preparation and good luck.\nWhile changes in study habits and preparation likely explain some improvement in low scores, we would also expect the lowest performers to improve simply because of better luck.\nLikewise, some of the highest performers may simply not be as lucky on exam 2, so a small dropoff should not be interpreted as weaker understanding of the exam material.\n\n\n4.7.4 Simulating Regression Effect\n\nset.seed(110322018)\nUnderstanding &lt;-rnorm(25, 80, 10)\nScore1 &lt;- Understanding + rnorm(25, 0, 5)\nScore2 &lt;- Understanding + rnorm(25, 0, 5)\nUnderstanding &lt;- round(Understanding,0)\nTestSim &lt;- data.frame(Understanding, Score1, Score2)\nggplot(data=TestSim, aes(y=Score2, x=Score1))+ geom_point() + stat_smooth(method=\"lm\") +\n  geom_abline(slope=1) + geom_text(aes(label=Understanding), vjust = 0, nudge_y = 0.5)\n\n\n\n\n\n\n\n\nThis phenomon is called the regression effect.\n\n\n4.7.5 Test Scores Simulation - Highest Scores\n\nkable(head(TestSim%&gt;%arrange(desc(Score1))))\n\n\n\n\nUnderstanding\nScore1\nScore2\n\n\n\n\n97\n98.86412\n93.60285\n\n\n89\n98.57157\n88.25851\n\n\n94\n97.23330\n92.65175\n\n\n91\n93.92857\n98.23312\n\n\n85\n93.66503\n88.70963\n\n\n93\n92.06243\n88.67015\n\n\n\n\n\nThese students’ success on test 1 is due to a strong understanding and good luck. We would expect the understanding to carry over to test 2 (provided the student continues to study in a similar way), but not necessarily the luck.\n\n\n4.7.6 Test Scores Simulation - Lowest Scores\n\nkable(head(TestSim%&gt;%arrange(Score1)))\n\n\n\n\nUnderstanding\nScore1\nScore2\n\n\n\n\n58\n54.44354\n50.30597\n\n\n69\n59.86641\n77.04696\n\n\n61\n61.35228\n65.54305\n\n\n66\n65.22433\n73.45304\n\n\n75\n65.87041\n80.79416\n\n\n72\n69.53082\n74.96092\n\n\n\n\n\nThese students’ lack of success on test 1 is due to a low understanding and poor luck. We would expect the understanding to carry over to test 2 (unless the student improves their preparation), but not necessarily the luck.\n\n\n4.7.7 Another Example\nWins by NFL teams in 2021 and 2022\n\n\n\n\n\n\n\n\n\n\n\n4.7.8 Other Examples of Regression Effect\nA 1973 article by Kahneman, D. and Tversky, A., “On the Psychology of Prediction,” Pysch. Rev. 80:237-251 describes an instance of the regression effect in the training of air force pilots.\nTrainees were praised after performing well and criticized after performing badly. The flight instructors observed that “high praise for good execution of complex maneuvers typically results in a decrement of performance on the next try.”\nKahneman and Tversky write that :\n“We normally reinforce others when their behavior is good and punish them when their behavior is bad. By regression alone, therefore, they [the trainees] are most likely to improve after being punished and most likely to deteriorate after being rewarded. Consequently, we are exposed to a lifetime schedule in which we are most often rewarded for punishing others, and punished for rewarding.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#responsible-statistical-inference",
    "href": "Ch4.html#responsible-statistical-inference",
    "title": "4  Inference from Models",
    "section": "4.8 Responsible Statistical Inference",
    "text": "4.8 Responsible Statistical Inference\nWhile statistics offer insight that have lead to great advances in science, medicine, business and economics and many other areas, they have also been misused, (either intentionally or unintentionally) in ways that have resulted in harm. In an effort to better educate the public, the American Statistical Association released a report in 2016, highlighting common misuses of p-values, and the notion of statistical significance. Kenneth Rothman, Professor of Epidemiology at Boston University School of Public Health wrote:\n\n“(S)cientists have embraced and even avidly pursued meaningless differences solely because they are statistically significant, and have ignored important effects because they failed to pass the screen of statistical significance…It is a safe bet that people have suffered or died because scientists (and editors, regulators, journalists and others) have used significance tests to interpret results, and have consequently failed to identify the most beneficial courses of action.” -ASA statement on p-values, 2016\n\nIn this section, we’ll look at some real scenarios where it might be tempting to make an improper conclusion based on data. Think carefully about each scenario and identify possible misinterpretations that might arise. Then think about what conclusions or actions would be appropriate in each scenario.\nKeep in mind the following guidelines for responsible statistical inference.\nWhat a p-value tells us\nPerforming responsible statistical inference requires understanding what p-values do and do not tell us, and how they should and should not be interpreted.\n\nA low p-value tells us that the data we observed are inconsistent with our null hypothesis or some assumption we make in our model.\nA large p-value tells us that the data we observed could have plausibly been obtained under our supposed model and null hypothesis.\nA p-value never provides evidence supporting the null hypothesis, it only tells us the strength of evidence against it.\nA p-value is impacted by\n\nthe size of the difference between group, or change per unit increase (effect size)\n\nthe amount of variability in the data\n\nthe sample size\n\nSometimes, a p-value tells us more about sample size, than relationship we’re actually interested in.\nA p-value does not tell us the “size” of a difference or effect, or whether it is practically meaningful.\nWe should only generalize results from a hypothesis test performed on a sample to a larger population or process if the sample is representative of the population or process (such as when the sample is randomly selected).\nA correlation between two variables does not necessarily imply a causal relationship.\n\n\n4.8.1 Breakfast Cereals and Sex of Babies\nThe title of a 2008 article by New Scientist makes a surprising claim: “Breakfast cereals boost chances of conceiving boys.” Although the article lacks details on the underlying study, researchers, in fact, tracked the eating habits of women who were attempting to become pregnant. They tracked 133 different food items, and tested whether there was a difference in the proportion of baby boys conceived between women who ate the food, compared to those who didn’t. Of the 133 foods tested, only breakfast cereal showed a significant difference.\nQuestion: Why is the researcher’s conclusion that eating breakfast cereal increases the chances of conceiving a boy inappropriate? Hint: think about how this situation is similar to the one in the xkcd comic?\nA statistical procedure called the Bonferroni Correction suggests that when more than one test is performed simultaneously, we should multiply observed p-values by the number of tests performed. Why might this help prevent errors? Do you see any downsides of doing this?\n\n\n4.8.2 Flights from New York to Chicago\nA traveler lives in New York and wants to fly to Chicago. They consider flying out of two New York airports:\n\nNewark (EWR)\n\nLaGuardia (LGA)\n\nWe have data on the times of flights from both airports to Chicago’s O’Hare airport from 2013 (more than 14,000 flights).\nAssuming these flights represent a random sample of all flights from these airports to Chicago, consider how the traveler might use this information to decide which airport to fly out of.\n\nlibrary(nycflights13)\ndata(flights)\nflights$origin &lt;- as.factor(flights$origin)\nflights$dest &lt;- as.factor(flights$dest)\n\nWe’ll create a dataset containing only flights from Newark and Laguardia to O’Hare, and only the variables we’re interested in.\n\nFlights_NY_CHI &lt;- flights %&gt;% \n  filter(origin %in% c(\"EWR\", \"LGA\") & dest ==\"ORD\") %&gt;%\n  select(origin, dest, air_time)\n\nThe plot and table compare the duration of flights from New York to Chicago from each airport.\n\np1 &lt;- ggplot(data=Flights_NY_CHI, aes(x=air_time, fill=origin, color=origin)) + geom_density(alpha=0.2) + ggtitle(\"Flight Time\")\np2 &lt;- ggplot(data=Flights_NY_CHI, aes(x=air_time, y=origin)) + geom_boxplot() + ggtitle(\"Flight Time\")\ngrid.arrange(p1, p2, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(knitr)\nT &lt;- Flights_NY_CHI %&gt;% group_by(origin) %&gt;% \n  summarize(Mean_Airtime = mean(air_time, na.rm=TRUE), \n            SD = sd(air_time, na.rm=TRUE), n=sum(!is.na(air_time)))\nkable(T)\n\n\n\n\norigin\nMean_Airtime\nSD\nn\n\n\n\n\nEWR\n113.2603\n9.987122\n5828\n\n\nLGA\n115.7998\n9.865270\n8507\n\n\n\n\n\nWe fit a model to test whether there is evidence of a difference in average flight time.\n\nM_Flights &lt;- lm(data=Flights_NY_CHI, air_time~origin)\nsummary(M_Flights)\n\n\nCall:\nlm(formula = air_time ~ origin, data = Flights_NY_CHI)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-26.26  -7.26  -1.26   5.20  84.74 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 113.2603     0.1299  872.06 &lt;0.0000000000000002 ***\noriginLGA     2.5395     0.1686   15.06 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.915 on 14333 degrees of freedom\n  (622 observations deleted due to missingness)\nMultiple R-squared:  0.01558,   Adjusted R-squared:  0.01551 \nF-statistic: 226.9 on 1 and 14333 DF,  p-value: &lt; 0.00000000000000022\n\n\nConfidence Interval for Flights:\n\nconfint(M_Flights)\n\n                2.5 %     97.5 %\n(Intercept) 113.00572 113.514871\noriginLGA     2.20905   2.869984\n\n\n\nFlights from LGA are estimated to take 2.5 minutes longer than flights from EWR on average.\nThe very low p-value provides strong evidence of a difference in mean flight time.\nWe are 95% confident that flights from LGA to ORD take between 2.2 and 2.9 minutes longer, on average, than flights from EWR to ORD.\n\nQuestion: If you were planning a trip from New York to Chicago, how much of a factor would this information play in your decision? Why do you think the p-value is so small in this scenario?\n\n\n4.8.3 Smoking and Birthweight Example\nWe consider data on the relationship between a pregnant mother’s smoking and the birth weight of the baby. Data come from a sample of 80 babies born in North Carolina in 2004. Thirty of the mothers were smokers, and fifty were nonsmokers.\nThe plot and table show the distribution of birth weights among babies whose mothers smoked, compared to those who didn’t.\n\np1 &lt;- ggplot(data=NCBirths, aes(x=weight, fill=habit, color=habit)) + geom_density(alpha=0.2) + ggtitle(\"Birthweight and Smoking\")\np2 &lt;- ggplot(data=NCBirths, aes(x=weight, y=habit)) + geom_boxplot() + ggtitle(\"Birthweight and Smoking\")\ngrid.arrange(p1, p2, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(knitr)\nT &lt;- NCBirths %&gt;% group_by(habit) %&gt;% summarize(Mean_Weight = mean(weight), SD = sd(weight), n=n())\nkable(T)\n\n\n\n\nhabit\nMean_Weight\nSD\nn\n\n\n\n\nnonsmoker\n7.039200\n1.709388\n50\n\n\nsmoker\n6.616333\n1.106418\n30\n\n\n\n\n\nWe fit a model and test for differences in average birth weight.\n\nM_Birthwt &lt;- lm(data=NCBirths, weight~habit)\nsummary(M_Birthwt)\n\n\nCall:\nlm(formula = weight ~ habit, data = NCBirths)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0392 -0.6763  0.2372  0.8280  2.4437 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)   7.0392     0.2140   32.89 &lt;0.0000000000000002 ***\nhabitsmoker  -0.4229     0.3495   -1.21                0.23    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.514 on 78 degrees of freedom\nMultiple R-squared:  0.01842,   Adjusted R-squared:  0.005834 \nF-statistic: 1.464 on 1 and 78 DF,  p-value: 0.23\n\n\n\nconfint(M_Birthwt)\n\n                2.5 %    97.5 %\n(Intercept)  6.613070 7.4653303\nhabitsmoker -1.118735 0.2730012\n\n\n\nThe average birth weight of babies whose mothers are smokers is estimated to be about 0.42 lbs less than the average birthweight for babies whose mothers are nonsmokers.\nThe large p-value of 0.23, tells us that there is not enough evidence to say that a mother’s smoking is associated with lower birth weights. It is plausible that this difference could have occurred by chance.\nWe are 95% confident that the average birtweight of babies whose mothers are smokers is between 1.12 lbs less and 0.27 lbs more than the average birthweight for babies whose mothers are nonsmokers.\n\nQuestion: Many studies have shown that a mother’s smoking puts a baby at risk of low birthweight. Do our results contradict this research? Should we conclude that smoking has no impact on birthweights?\n\n4.8.3.1 Larger Study\nIn fact, this sample of 80 babies is part of a larger dataset, consisting of 1,000 babies born in NC in 2004. When we consider the full dataset, notice that the difference between the groups is similar, but the p-value is much smaller, providing stronger evidence of a relationship between a mother’s smoking and lower birthweight.\nWe’ll now fit a model to the larger dataset.\n\nM_Birthwt_Full &lt;- lm(data=ncbirths, weight~habit)\nsummary(M_Birthwt_Full)\n\n\nCall:\nlm(formula = weight ~ habit, data = ncbirths)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.1443 -0.7043  0.1657  0.9157  4.6057 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  7.14427    0.05086 140.472 &lt;0.0000000000000002 ***\nhabitsmoker -0.31554    0.14321  -2.203              0.0278 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.503 on 997 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.004846,  Adjusted R-squared:  0.003848 \nF-statistic: 4.855 on 1 and 997 DF,  p-value: 0.02779\n\n\nConfidence interval based on larger dataset:\n\nconfint(M_Birthwt_Full)\n\n                 2.5 %      97.5 %\n(Intercept)  7.0444697  7.24407557\nhabitsmoker -0.5965648 -0.03452013\n\n\nNotice that the estimated difference in birth weights is actually smaller in the full dataset than in the subset (though 0.3 lbs is still a pretty big difference for babies), and yet now the p-value is much smaller. Why do you think this happened? What should we learn from this?\n\n\n\n4.8.4 Cautions and Advice\np-values are only (a small) part of a statistical analysis.\n\nFor small samples, real differences might not be statistically significant.\n-Don’t accept null hypothesis. Gather more information.\n\nFor large, even very small differences will be statistically significant.\n-Look at confidence interval. Is difference practically important?\n\nWhen many hypotheses are tested at once (such as many food items) some will produce a significant result just by change.\n-Use a multiple testing correction, such as Bonferroni\n\nInterpret p-values on a “sliding scale”\n\n0.049 is practically the same as 0.051\n\nIs sample representative of larger population?\n\nWere treatments randomly assigned (for experiments)?\n\nAre there other variables to consider?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch5.html",
    "href": "Ch5.html",
    "title": "5  Building and Assessing Models",
    "section": "",
    "text": "5.1 Checking Regression Assumptions\nWe’ve seen that tests and intervals based on the normal error regression model depend on four assumptions. If these assumptions are not reasonable then the tests and intervals may not be reliable.\nThe statement \\(Y_i = \\beta_0 + \\beta_1X_{i1}+ \\ldots + \\beta_pX_{ip} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\) implies the following:\nIllustration of Model Assumptions\nWe know that these assumptions held true in the ice cream example, because we generated the data in a way that was consistent with these.\nIn practice, we will have only the data, without knowing the exact mechanism that produced it. We should only rely on the t-distribution based p-values and confidence intervals in the R output if these appear to be reasonable assumptions.\nOf course, these assumptions will almost never be truly satisfied, but they should at least be a reasonable approximation if we are to draw meaningful conclusions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Building and Assessing Models</span>"
    ]
  },
  {
    "objectID": "Ch5.html#checking-regression-assumptions",
    "href": "Ch5.html#checking-regression-assumptions",
    "title": "5  Building and Assessing Models",
    "section": "",
    "text": "Linearity: the expected value of \\(Y\\) is a linear function of \\(X_1, X_2, \\ldots, X_p\\). (This assumption is only relevant for models including at least one quantitative explanatory variable.)\nNormality: Given the values of \\(X_1, X_2, \\ldots, X_p\\), \\(Y\\) follows a normal distribution.\nConstant Variance: Regardless of the values of \\(X_1, X_2, \\ldots, X_p\\), the variance (or standard deviation) in the normal distribution for \\(Y\\) is the same.\nIndependence: The response value for each observation is not affected by any of the other observations (expect due to explanatory variables included in the model).\n\n\n\n\n\n\n\n5.1.1 Checking Model Assumptions\nThe following plots are useful when assessing the appropriateness of the normal error regression model.\n\nScatterplot of residuals against predicted values\nHistogram of standardized residuals\n\nheavy skewness indicates a problem with normality assumption\n\nNormal quantile plot\n\nsevere departures from diagonal line indicate problem with normality assumption\n\n\nResidual vs Predicted Plots\nA residual vs predicted plot is useful for detecting issues with the linearity or constant variance assumption.\n\ncurvature indicates a problem with linearity assumption\n\n“funnel” or “megaphone” shape indicates problem with constant variance assumption\n\n\nP1 &lt;- ggplot(data=Violations, aes(y=no_viol_Model$residuals, x=no_viol_Model$fitted.values)) + geom_point() + ggtitle(\"No Violation\") + xlab(\"Predicted Values\") + ylab(\"Residuals\")\nP2 &lt;- ggplot(data=Violations, aes(y=lin_viol_Model$residuals, x=no_viol_Model$fitted.values)) + geom_point() + ggtitle(\"Violation of Linearity Assumption\")+ xlab(\"Predicted Values\") + ylab(\"Residuals\")\nP3 &lt;- ggplot(data=Violations, aes(y=cvar_viol_Model$residuals, x=no_viol_Model$fitted.values)) + geom_point() + ggtitle(\"Violation of Constant Variance Assumption\")+ xlab(\"Predicted Values\") + ylab(\"Residuals\")\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nIf there is only one explanatory variable, plotting the residuals against that variable reveals the same information as a residual vs predicted plot.\nHistogram of Residuals\nA histogram of the residuals is useful for assessing the normality assumption.\n\nSevere skewness indicates violation of normality assumption\n\n\nP1 &lt;- ggplot(data=Violations, aes(x=no_viol_Model$residuals)) + geom_histogram() + ggtitle(\"No Violation\") +xlab(\"Residual\")\nP2 &lt;- ggplot(data=Violations, aes(x=norm_viol_Model$residuals)) + geom_histogram() + ggtitle(\"Violation of Normality Assumption\") + xlab(\"Residual\")\ngrid.arrange(P1, P2, ncol=2)\n\n\n\n\n\n\n\n\nNormal Quantile-Quantile (QQ) Plot\nSometimes histograms can be inconclusive, especially when sample size is smaller.\nA Normal quantile-quantile plot displays quantiles of the residuals against the expected quantiles of a normal distribution.\n\nSevere departures from diagonal line indicate a problem with normality assumption.\n\n\nP1 &lt;- ggplot(data=Violations, aes(sample = scale(no_viol_Model$residuals))) + stat_qq() + stat_qq_line() + xlab(\"Normal Quantiles\") + ylab(\"Residual Quantiles\") + ggtitle(\"No Violation\") + xlim(c(-4,4)) + ylim(c(-4,4))\nP2 &lt;- ggplot(data=Violations, aes(sample = scale(norm_viol_Model$residuals))) + stat_qq() + stat_qq_line() + xlab(\"Normal Quantiles\") + ylab(\"Residual Quantiles\") + ggtitle(\"Violation of Normality Assumption\") + xlim(c(-4,4)) + ylim(c(-4,4))\ngrid.arrange(P1, P2, ncol=2)\n\n\n\n\n\n\n\n\nChecking Model Assumptions - Independence\nIndependence is often difficult to assess through plots of data, but it is important to think about whether there were factors in the data collection that would cause some observations to be more highly correlated than others.\nFor example:\n\nPeople in the study who are related.\n\nSome plants grown in the same greenhouse and others in different greenhouses.\n\nSome observations taken in same time period and others at different times.\n\nAll of these require more complicated models that account for correlation using spatial and time structure.\n\n\n5.1.2 Summary of Checks for Model Assumptions\n\n\n\n\n\n\n\nModel assumption\nHow to detect violation\n\n\n\n\nLinearity\nCurvature in residual plot\n\n\nConstant Variance\nFunnel shape in residual plot\n\n\nNormality\nSkewness in histogram of residuals or departure from diag. line in QQ plot\n\n\nIndependence\nNo graphical check, carefully examine data collection\n\n\n\n\n\n5.1.3 Example: N v S Lakes\nRecall our sample of 53 Florida Lakes, 33 in the north, and 20 in the south.\n\\(\\text{Mercury}_i = \\beta_0 + \\beta_1\\times{\\text{South}_i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\).\n\nLakesBP\n\n\n\n\n\n\n\n\nWhen we use the normal error regression model, we are assuming the following:\n\nLinearity: there is an expected mercury concentration for lakes in North Florida, and another for lakes in South Florida.\nNormality: mercury concentrations of individual lakes in the north are normally distributed, and so are mercury concentrations in the south. These normal distributions might have different means.\nConstant Variance: the normal distribution for mercury concentrations in North Florida has the same standard deviation as the normal distribution for mercury concentrations in South Florida\nIndependence: no two lakes are any more alike than any others, except for being in the north or south, which we account for in the model. We might have concerns about this, do to some lakes being geographically closer to each other than others.\n\nWe should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable.\nA residual by predicted plot, histogram of residuals, and normal quantile-quantile plot are shown below.\n\nP1 &lt;- ggplot(data=FloridaLakes, aes(y=Lakes_M$residuals, x=Lakes_M$fitted.values)) + geom_point() + ggtitle(\"Residual vs Predicted Plot\") + xlab(\"Predicted Values\") + ylab(\"Residuals\")\nP2 &lt;- ggplot(data=FloridaLakes, aes(x=Lakes_M$residuals)) + geom_histogram() + ggtitle(\"Histogram of Residuals\") + xlab(\"Residual\")\nP3 &lt;- ggplot(data=FloridaLakes, aes(sample = scale(Lakes_M$residuals))) + stat_qq() + stat_qq_line() + xlab(\"Normal Quantiles\") + ylab(\"Residual Quantiles\") + ggtitle(\"Normal QQ Plot\")\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nNotice that we see two lines of predicted values and residuals. This makes sense since all lakes in North Florida will have the same predicted value, as will all lakes in Southern Florida.\nThere appears to be a little more variability in residuals for Southern Florida (on the right), than Northern Florida, causing some concern about the constant variance assumption.\nOverall, though, the assumptions seem mostly reasonable.\nWe shouldn’t be concerned about using theory-based hypothesis tests or confidence intervals for the mean mercury level or difference in mean mercury levels. There might be some concern that prediction intervals could be either too wide or too narrow, but this is not a major concern, since the constant variance assumption is not severe.\n\n\n5.1.4 Example: pH Model\nRecall the regression line estimating the relationship between a lake’s mercury level and pH.\n\\(\\text{Mercury}_i = \\beta_0 + \\beta_1\\times\\text{pH}_i + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\).\nThe model assumes:\n\nLinearity: the expected mercury level of a lake is a linear function of pH.\nNormality: for any given pH, the mercury levels of lakes with that pH follow a normal distribution. For example, mercury levels for lakes with pH of 6 is are normally distributed, and mercury levels for lakes with pH of 9 are normally distributed, though these normal distributions may have different means.\nConstant Variance: the variance (or standard deviation) in the normal distribution for mercury level is the same for each pH. For example, there is the same amount of variability associated with lakes with pH level 6, as pH level 8.\nIndependence: no two lakes are any more alike than any others, except with respect to pH, which is accounted for in the model. This may not be a reasonable assumption, but it’s unclear what the effects of such a violation would be.\n\nWe should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable.\nThe plots for checking these assumptions are shown below.\n\nP1 &lt;- ggplot(data=FloridaLakes, aes(y=M_pH$residuals, x=M_pH$fitted.values)) + geom_point() + ggtitle(\"Residual vs Predicted Plot\") + xlab(\"Predicted Values\") + ylab(\"Residuals\")\nP2 &lt;- ggplot(data=FloridaLakes, aes(x=M_pH$residuals)) + geom_histogram() + ggtitle(\"Histogram of Residuals\") + xlab(\"Residual\")\nP3 &lt;- ggplot(data=FloridaLakes, aes(sample = scale(M_pH$residuals))) + stat_qq() + stat_qq_line() + xlab(\"Normal Quantiles\") + ylab(\"Residual Quantiles\") + ggtitle(\"Normal QQ Plot\")\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nThe residual vs predicted plot does not show any linear trend, and variability appears to be about the same for low predicted values as for high ones. Thus, the linearity and constant variance assumptions appear reasonable.\nThe histogram shows some right-skewness, and the right-most points on the normal-qq plot are above the line, indicating a possible concern with the normality assumption. There is some evidence of right-skewness, which might impact the appropriatness of the normal error regression model.\nNevertheless, we obtained similar results using the simulation-based results as the normal error regression model, suggesting that the concern about normality did not have much impact on the estimation of \\(\\beta_1\\). It is possible that this concern could have implications for other kinds of inference, such as confidence intervals for an expected response, and prediction intervals, which we’ll explore later in the chapter.\n\n\n5.1.5 Example: House Prices\nRecall the model for estimating price of a house, using size, waterfront status, and an interaction term.\n\\(\\text{Price}_i = \\beta_0 + \\beta_1\\text{Sq.Ft.}_{i}+ \\beta_2\\text{Waterfront}_{i}+ \\beta_3\\times\\text{Sq.Ft.}_i\\times\\text{Waterfront}_{i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\).\nThe model assumes:\n\nLinearity: the expected price of a house is a linear function of its size. The slope and intercept of this function may be different for houses on the waterfront, compared to houses not on the waterfront.\nNormality: prices of houses of a given size and waterfront status are normally distributed.\nConstant Variance: the variance (or standard deviation) in the normal distribution for prices is the same for all sizes and waterfront statuses.\nIndependence: no two houses are any more alike than any others, except with respect to size and waterfront status.\n\nWe should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable.\nSeveral reasons come to mind that might cause us to doubt the validity of these assumptions, but let’s investigate them emperically, using our data on 100 houses.\nThe plots for checking these assumptions are shown below.\n\nP1 &lt;- ggplot(data=Houses, aes(y=M_House_Int$residuals, x=M_House_Int$fitted.values)) + geom_point() + ggtitle(\"Residual vs Predicted Plot\") + xlab(\"Predicted Values\") + ylab(\"Residuals\")\nP2 &lt;- ggplot(data=Houses, aes(x=M_House_Int$residuals)) + geom_histogram() + ggtitle(\"Histogram of Residuals\") + xlab(\"Residual\")\nP3 &lt;- ggplot(data=Houses, aes(sample = scale(M_House_Int$residuals))) + stat_qq() + stat_qq_line() + xlab(\"Normal Quantiles\") + ylab(\"Residual Quantiles\") + ggtitle(\"Normal QQ Plot\")\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nAlthough we might have had some initial concerns about the model assumptions, the plots do not raise any serious concerns. There is no sign of a nonlinear relationship in the residual vs predicted plot, so the linearity assumption appears reasonable.\nThere is possibly more variability associated with prices or more expensive houses than less expensive ones, so we might have some concerns about constant variance, but since there are only a few very high-priced houses, and the increasing variance is not too severe, this may not be much of a concern.\nThere are a few houses on each end of the normal qq plot that deviate from their expected line, but not very many. It’s not uncommon to have a few points deviate from the line on the end, so we do not have severe concerns about normality. The histogram of residuals is roughly symmetric.\nThus, the normal error regression model appears to be reasonable for these data.\n\n\n5.1.6 Impact of Model Assumption Violations\nIn this chapter, we’ve studied the normal error regression model and its underlying assumptions. We’ve seen that when these assumptions are realistic, we can use distributions derived from probability theory, such as t and F distributions to approximate sampling distributions, in place of the simulation-based methods seen in Chapters 3 and 4.\nOf course, real data don’t come exactly from processes like the fictional ice cream dispenser described in Section 5.1, so it’s really a question of whether this model is a realistic approximation (or simplification) of the true mechanism that led to the data we observe. We can use diagnostics like residual and Normal-QQ plots, as well as our intuition and background knowledge to assess whether the normal error regression model is a reasonable approximation.\nThe p-values provided by the lm summary output, and anova commands, and the and intervals produced by the confint, and predict command, as well as many other R commands, depend on the assumptions of the normal error regression model, and should only be used when these assumptions are reasonable.\nIn situations where some model assumptions appear to be violated, we might be okay using certain tests/intervals, but not others. In general, we should proceed with caution in these situations.\nThe table below provides guidance on the potential impact of model assumption violation on predicted values, confidence intervals, and prediction intervals.\n\n\n\n\n\n\n\n\n\nModel assumption Violated\nPredicted Values\nConfidence Intervals\nPrediction Intervals\n\n\n\n\nLinearity\nUnreliable\nUnreliable\nUnreliable\n\n\nConstant Variance\nReliable\nSomewhat unreliable - Some too wide, others too narrow\nVery unreliable - Some too wide, others too narrow\n\n\nNormality\nReliable\nPossibly unreliable - might be symmetric when they shouldn’t be. Might be okay when skewness isn’t bad and sample size is large.\nVery unreliable - will be symmetric when they shouldn’t be\n\n\nIndependence\nmight be reliable\nunreliable - either too wide or too narrow\nunreliable - either too wide or too narrow\n\n\n\nWhen model assumptions are a concern, consider a using a transformation of the data, a more advanced model, or a more flexible technique, such as a nonparametric approach or statistical machine learning algorithm.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Building and Assessing Models</span>"
    ]
  },
  {
    "objectID": "Ch5.html#transformations",
    "href": "Ch5.html#transformations",
    "title": "5  Building and Assessing Models",
    "section": "5.2 Transformations",
    "text": "5.2 Transformations\nWhen there are violations of model assumptions, we can sometimes correct for these by modeling a a function of the response variable, rather than the response variable itself. When the histogram of residuals and normal qq plot show signs of right-skewness, modeling \\(log(Y)\\) is often helpful.\n\n5.2.1 Example: Modeling Car Prices\nWe’ll look at the assumptions associated with the model for predicting car price, using acceleration time as the explanatory variable.\nDiagnostic plots are shown below.\n\nP1 &lt;- ggplot(data=Cars2015, aes(y=Cars_M_A060$residuals, x=Cars_M_A060$fitted.values)) + geom_point() + ggtitle(\"Residual Plot\") + xlab(\"Predicted Values\") + ylab(\"Residuals\")\nP2 &lt;- ggplot(data=Cars2015, aes(x=Cars_M_A060$residuals)) + geom_histogram() + ggtitle(\"Histogram of Residuals\") + xlab(\"Residual\")\nP3 &lt;- ggplot(data=Cars2015, aes(sample = scale(Cars_M_A060$residuals))) + stat_qq() + stat_qq_line() + xlab(\"Normal Quantiles\") + ylab(\"Residual Quantiles\") + ggtitle(\"Normal QQ Plot\")\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nThere is a funnel-shape in the residual plot, indicating a concern about the constant variance assumption. There appears to be more variability in prices for more expensive cars than for cheaper cars. There is also some concern about the normality assumption, as the histogram and QQ plot indicate right-skew in the residuals.\n\n\n5.2.2 Log Transformation\nWhen residual plots yield model inadequacy, we might try to correct these by applying a transformation to the response variable.\nWhen working a nonnegative, right-skewed response variable, it is often helpful to work with the logarithm of the response variable.\nNote: In R, log() denotes the natural (base e) logarithm, often denoted ln(). We can actually use any logarithm, but the natural logarithm is commonly used.\nWe’ll use the model:\n\\[\n\\text{Log Price} = \\beta_0 + \\beta_1\\times \\text{Acc060} + \\epsilon_i , \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nThe plot shows log(price) on the y-axis. We see that the relationship appears more linear than when we plot price itself.\n\nggplot(data=Cars2015, aes(x=Acc060, y=log(Price))) + geom_point() + \n  xlab(\"Acceleration Time\") + ylab(\"Log of Price\") + \n  ggtitle(\"Acceleration Time and Log Price\") + stat_smooth(method=\"lm\", se=FALSE)\n\n\n\n\n\n\n\n\nLog Transformation Model - What We’re Assuming\n\nLinearity: the log of expected price of a car is a linear function of its acceleration time.\nNormality: for any given acceleration time, the log of prices of actual cars follow a normal distribution.\nConstant Variance: the normal distribution for log of price is the same for all acceleration times.\nIndependence: no two cars are any more alike than any others.\n\nWe should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable.\nAssumption Check for Model on Log Price\n\nP1 &lt;- ggplot(data=Cars2015, aes(y=Cars_M_Log$residuals, x=Cars_M_Log$fitted.values)) + geom_point() + ggtitle(\"Cars Log Model Residual Plot\") + xlab(\"Predicted Values\") + ylab(\"Residuals\")\nP2 &lt;- ggplot(data=Cars2015, aes(x=Cars_M_Log$residuals)) + geom_histogram() + ggtitle(\"Histogram of Residuals\") + xlab(\"Residual\")\nP3 &lt;- ggplot(data=Cars2015, aes(sample = scale(Cars_M_Log$residuals))) + stat_qq() + stat_qq_line() + xlab(\"Normal Quantiles\") + ylab(\"Residual Quantiles\") + ggtitle(\"Cars Model QQ Plot\")\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nThere is still some concern about constant variance, though perhaps not as much. The normality assumption appears more reasonable.\n\n\n5.2.3 Inference for Log Model\nR output for the model is shown below.\n\nCars_M_Log &lt;- lm(data=Cars2015, log(Price)~Acc060)\nsummary(Cars_M_Log)\n\n\nCall:\nlm(formula = log(Price) ~ Acc060, data = Cars2015)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.84587 -0.19396  0.00908  0.18615  0.53350 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  5.13682    0.13021   39.45 &lt;0.0000000000000002 ***\nAcc060      -0.22064    0.01607  -13.73 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.276 on 108 degrees of freedom\nMultiple R-squared:  0.6359,    Adjusted R-squared:  0.6325 \nF-statistic: 188.6 on 1 and 108 DF,  p-value: &lt; 0.00000000000000022\n\n\nPrediction Equation:\n\\[\n\\begin{aligned}\n\\widehat{\\text{Price}} & = e^{5.13582-0.22064 \\times \\text{Acc060}}\n\\end{aligned}\n\\]\nPredicted price for car that takes 7 seconds to accelerate:\n\\[\n\\begin{aligned}\n\\widehat{\\text{Price}} & = e^{5.13582-0.22064 \\times \\text{7}} = 36.3\n\\end{aligned}\n\\]\nPredicted price for car that takes 10 seconds to accelerate:\n\\[\n\\begin{aligned}\n\\widehat{\\text{Price}} & = e^{5.13582-0.22064 \\times \\text{10}}= 18.7\n\\end{aligned}\n\\]\nPredictions are for log(Price), so we need to exponentiate.\n\npredict(Cars_M_Log, newdata=data.frame(Acc060=c(7)))\n\n       1 \n3.592343 \n\n\n\nexp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7))))\n\n       1 \n36.31908 \n\n\nA car that accelerates from 0 to 60 mph in 7 seconds is expected to cost 36.3 thousand dollars.\n\n\n5.2.4 Log Model Interpretations\n\\[\n\\begin{aligned}\n\\text{Log of Expected Price} & = \\beta_0 + \\beta_1\\times \\text{Acc060}\\  \\text{, Thus:} \\\\\n\\text{ Expected Price} & = e^{\\beta_0 + \\beta_1\\times \\text{Acc060} } \\\\\n& e^{\\beta_0}e^{\\beta_1 \\times \\text{Acc060}} \\\\\n& e^{\\beta_0}(e^{\\beta_1})^\\text{Acc060}\n\\end{aligned}\n\\]\n\n\\(e^{\\beta_0}\\) is theoretically the expected price of a car that can accelerate from 0 to 60 mph in no time, but this is not a meaningful interpretation.\nFor each additional second it takes a car to accelerate, price is expected to multiply by a factor of \\(e^{b_1}\\).\n\nExponentiating the model coefficients gives:\n\nexp(Cars_M_Log$coefficients)\n\n(Intercept)      Acc060 \n170.1730148   0.8020062 \n\n\n\nFor each additional second in acceleration time, price is expected to multiply by a a factor of \\(e^{-0.22} = 0.80\\). Thus, each 1-second increase in acceleration time is estimated to be associated with a 20% drop in price, on average.\n\nConfidence Intervals for \\(\\beta_0\\) and \\(\\beta_1\\)\n\nconfint(Cars_M_Log)\n\n                 2.5 %     97.5 %\n(Intercept)  4.8787105  5.3949208\nAcc060      -0.2524862 -0.1887916\n\n\n\nexp(confint(Cars_M_Log))\n\n                  2.5 %     97.5 %\n(Intercept) 131.4610408 220.284693\nAcc060        0.7768669   0.827959\n\n\n\nWe are 95% confident that the price of a car changes, on average, by multiplicative factor between \\(e^{-0.252} = 0.7773\\) and \\(e^{-0.189}=0.828\\) for each additional second in acceleration time. That is, we believe the price decreases between 17% and 23% on average for each additional second in acceleration time.\n\nLog Model CI for Expected Response\nIf we just use the predict function, we get a confidence interval for log(price).\n\npredict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=\"confidence\")\n\n       fit     lwr      upr\n1 3.592343 3.53225 3.652436\n\n\nTo get an interval for price itself, we exponentiate, using exp.\n\nexp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=\"confidence\"))\n\n       fit      lwr      upr\n1 36.31908 34.20083 38.56852\n\n\nWe are 95% confident that the mean price amoung all cars that accelerate from 0 to 60 mph in 7 seconds is between \\(e^{3.53225} =34.2\\) and \\(e^{3.652436}=38.6\\) thousand dollars.\nLog Model Prediction Interval\n\npredict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=\"prediction\")\n\n       fit      lwr      upr\n1 3.592343 3.042041 4.142645\n\n\n\nexp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=\"prediction\"))\n\n       fit      lwr      upr\n1 36.31908 20.94796 62.96917\n\n\nWe are 95% confident that the expected price for a car that accelerates from 0 to 60 mph in 7 seconds is between \\(e^{3.04} =20.9\\) and \\(e^{4.14}=63.9\\) thousand dollars.\n\n\n5.2.5 Model Comparisons\nWe’ll compare the intervals we obtain using the log transformation to those from the model without the transformation.\n95% Confidence interval for average price of cars that take 7 seconds to accelerate:\nOriginal Model:\n\npredict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=\"confidence\", level=0.95)\n\n      fit      lwr      upr\n1 39.5502 37.21856 41.88184\n\n\nTransformed Model:\n\nexp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=\"confidence\", level=0.95))\n\n       fit      lwr      upr\n1 36.31908 34.20083 38.56852\n\n\n95% Prediction interval for price of an individual car that takes 7 seconds to accelerate:\nOriginal Model:\n\npredict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=\"prediction\", level=0.95)\n\n      fit      lwr      upr\n1 39.5502 18.19826 60.90215\n\n\nTransformed Model:\n\nexp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=\"prediction\", level=0.95))\n\n       fit      lwr      upr\n1 36.31908 20.94796 62.96917\n\n\nNotice that the transformed interval is not symmetric and allows for a longer “tail” on the right than the left.\n\n\n5.2.6 Log Model Visualization\n\n\n\n\n\n\n\n\n\nThe log model suggests an nonlinear trend in price with respect to acceleration time and gives wider confidence and prediction intervals for cars that accelerate faster and tend to be more expensive. It also gives non-symmetric intervals. These results appear to be consistent with the observed data.\n\n\n5.2.7 Comments on Transformations\n\nWe could have used another transformation, such as \\(\\sqrt{\\text{Price}}\\)\nThe log tranform leads to a nice interpretation involving percent change. Other transformations might yield better predictions, but are often hard to interpret.\nThere is often a tradeoff between model complexity and interpretability. We’ll talk more about this.\nWe did an example of a transformation in a model with a single explanatory variable.\nIf the explanatory variable is categorical:\n\n\n\\(e^{\\beta_0}\\) represents the expected response in the baseline category\n\n\\(e^{\\beta_j}\\) represents the number of times larger the expected response in category \\(j\\) is, compared to the baseline category.\n\nWhen working with multiple regression models, it is still important to mention holding other variables constant when interpreting parameters associated with one of the variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Building and Assessing Models</span>"
    ]
  },
  {
    "objectID": "Ch5.html#building-models-for-interpretation-confounding-multicollinearity-and-polynomial-regression",
    "href": "Ch5.html#building-models-for-interpretation-confounding-multicollinearity-and-polynomial-regression",
    "title": "5  Building and Assessing Models",
    "section": "5.3 Building Models for Interpretation: Confounding, Multicollinearity, and Polynomial Regression",
    "text": "5.3 Building Models for Interpretation: Confounding, Multicollinearity, and Polynomial Regression\nSo far, we’ve dealt with models with 2 or fewer variables. Some real questions require accounting for more than two variables. In these situations, we’ll need to develop a model that is complex enough to capture the important aspects of the mechanism we’re modeling, but also simple enough for us to be able to explain and interpret. We’ll need to decide how many variables to include in the model, and whether to use transformations, or to include interaction terms.\nWe’ll examine strategies for modeling in two different contexts. In this chapter, we’ll focus on building models for situations when we want to make interpretations and draw conclusions about relationships between variables. In Chapter 7, we focus on modeling solely for the purpose of prediction, when we are not interested in making interpretations or conclusions about relationships between variables.\nWhen building a model for the purpose of interpretation, we are typically interested in investigating a research question pertaining to relationships between explanatory and response variables. We’ll need to think about things like:\n\nwhich explanatory variables should we include in the model, and how many?\n\nshould we include any interaction terms?\n\nshould we use any nonlinear terms?\n\nshould we use a transformation of the response variable?\n\nWe’ll go through a couple example to see how we can address these questions in building a model.\nKeep in mind, there is no single correct model, but there are common characteristics of a good model. While two statisticians might use different models for a given set of data, they will hopefully lead to reasonably similar conclusions if constructed carefully.\n\n5.3.1 Modeling SAT Scores\nWe’ll now look at a dataset containing education data on all 50 states. It includes the following variables.\nstate - a factor with names of each state\nexpend - expenditure per pupil in average daily attendance in public elementary and secondary schools, 1994-95 (in thousands of US dollars)\nratio - average pupil/teacher ratio in public elementary and secondary schools, Fall 1994\nsalary - estimated average annual salary of teachers in public elementary and secondary schools, 1994-95 (in thousands of US dollars)\nfrac - percentage of all eligible students taking the SAT, 1994-95\nsat - average total SAT score, 1994-95\nregion - region of the country\n\nlibrary(mosaicData)\ndata(SAT)\nSAT &lt;- SAT %&gt;% dplyr::select(-c(verbal, math))\nlibrary(Lock5Data)\ndata(\"USStates\")\nSAT &lt;- SAT %&gt;% left_join(USStates %&gt;% select(State, Region), by=c(\"state\"=\"State\")) %&gt;% rename(region = Region)\n\n\nSAT\n\n            state expend ratio salary frac  sat region\n1         Alabama  4.405  17.2 31.144    8 1029      S\n2          Alaska  8.963  17.6 47.951   47  934      W\n3         Arizona  4.778  19.3 32.175   27  944      W\n4        Arkansas  4.459  17.1 28.934    6 1005      S\n5      California  4.992  24.0 41.078   45  902      W\n6        Colorado  5.443  18.4 34.571   29  980      W\n7     Connecticut  8.817  14.4 50.045   81  908     NE\n8        Delaware  7.030  16.6 39.076   68  897     NE\n9         Florida  5.718  19.1 32.588   48  889      S\n10        Georgia  5.193  16.3 32.291   65  854      S\n11         Hawaii  6.078  17.9 38.518   57  889      W\n12          Idaho  4.210  19.1 29.783   15  979      W\n13       Illinois  6.136  17.3 39.431   13 1048     MW\n14        Indiana  5.826  17.5 36.785   58  882     MW\n15           Iowa  5.483  15.8 31.511    5 1099     MW\n16         Kansas  5.817  15.1 34.652    9 1060     MW\n17       Kentucky  5.217  17.0 32.257   11  999     MW\n18      Louisiana  4.761  16.8 26.461    9 1021      S\n19          Maine  6.428  13.8 31.972   68  896     NE\n20       Maryland  7.245  17.0 40.661   64  909     NE\n21  Massachusetts  7.287  14.8 40.795   80  907     NE\n22       Michigan  6.994  20.1 41.895   11 1033     MW\n23      Minnesota  6.000  17.5 35.948    9 1085     MW\n24    Mississippi  4.080  17.5 26.818    4 1036      S\n25       Missouri  5.383  15.5 31.189    9 1045     MW\n26        Montana  5.692  16.3 28.785   21 1009      W\n27       Nebraska  5.935  14.5 30.922    9 1050     MW\n28         Nevada  5.160  18.7 34.836   30  917      W\n29  New Hampshire  5.859  15.6 34.720   70  935     NE\n30     New Jersey  9.774  13.8 46.087   70  898     NE\n31     New Mexico  4.586  17.2 28.493   11 1015      W\n32       New York  9.623  15.2 47.612   74  892     NE\n33 North Carolina  5.077  16.2 30.793   60  865      S\n34   North Dakota  4.775  15.3 26.327    5 1107     MW\n35           Ohio  6.162  16.6 36.802   23  975     MW\n36       Oklahoma  4.845  15.5 28.172    9 1027      S\n37         Oregon  6.436  19.9 38.555   51  947      W\n38   Pennsylvania  7.109  17.1 44.510   70  880     NE\n39   Rhode Island  7.469  14.7 40.729   70  888     NE\n40 South Carolina  4.797  16.4 30.279   58  844      S\n41   South Dakota  4.775  14.4 25.994    5 1068     MW\n42      Tennessee  4.388  18.6 32.477   12 1040      S\n43          Texas  5.222  15.7 31.223   47  893      S\n44           Utah  3.656  24.3 29.082    4 1076      W\n45        Vermont  6.750  13.8 35.406   68  901     NE\n46       Virginia  5.327  14.6 33.987   65  896      S\n47     Washington  5.906  20.2 36.151   48  937      W\n48  West Virginia  6.107  14.8 31.944   17  932      S\n49      Wisconsin  6.930  15.9 37.746    9 1073     MW\n50        Wyoming  6.160  14.9 31.285   10 1001      W\n\n\nNote that the dataset is quite old (from 1994-95), so the financial information may be out of date. Nevertheless, it is useful for exploring relationships between SAT scores and other variables.\nResearch Question\nA good statistical research question should be one that has practical implications that people would care about. It should be complex enough to be worth investigating. If the answer is obvious, then there would be no need to use statistics, or scientific reasoning in general.\nFor the SAT score dataset, we’ll focus on the question:\nDo students in states that prioritize education spending achieve better SAT scores?\nWhile this may seem like a straightforward question, we’ll see that answering it properly requires careful thought and analysis.\n\n5.3.1.1 Initial Model\nOne way to measure a state’s investment in education is in how much it pays its teachers. The plot displays average SAT score against average teacher salary for all 50 US states.\n\nggplot(data=SAT, aes(y=sat, x=salary)) + geom_point() +\n  stat_smooth(method=\"lm\", se=FALSE) + \n  ggtitle(\"Average SAT score vs Average Teacher Salary\") + \n  xlab(\"Average Teacher Salary in Thousands\") \n\n\n\n\n\n\n\n\nFitting a simple linear regression model to the data, we obtain the following:\n\nSAT_M1 &lt;- lm(data=SAT, sat~salary)\nsummary(SAT_M1)\n\n\nCall:\nlm(formula = sat ~ salary, data = SAT)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-147.125  -45.354    4.073   42.193  125.279 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 1158.859     57.659  20.098 &lt; 0.0000000000000002 ***\nsalary        -5.540      1.632  -3.394              0.00139 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 67.89 on 48 degrees of freedom\nMultiple R-squared:  0.1935,    Adjusted R-squared:  0.1767 \nF-statistic: 11.52 on 1 and 48 DF,  p-value: 0.001391\n\n\nOn average, SAT score is expected to decrease by about 5.5 points for each additional one thousand dollars in average teacher salary in the state. The low p-value suggests a relationship like this is unlikely to occur by chance, though the practical importance of a 5-point decrease in SAT score (out of 1600) seems minimal. Furthermore, only 19% of the total variation in SAT score is explained by teaching salary. Nevertheless, a person looking to argue against raising teacher salaries might use the negative estimate and low p-value as a justification for their position.\n\n\n5.3.1.2 A Deeper Investigation\nNotice that there are large discrepancies in the frac variable, representing the percentage of students taking the SAT. In Connecticut, 81% of high school students took the SAT, compared to only 6% in Arkansas.\nLet’s break the data down by the percentage of students who take the SAT. We’ll (somewhat arbitrarily), divide the states into\nLow = 0%-22%\nMedium = 22-49%\nHigh = 49-81%\n\nSAT &lt;- mutate(SAT, fracgrp = cut(frac, \n      breaks=c(0, 22, 49, 81), \n      labels=c(\"low\", \"medium\", \"high\")))\n\nPlotting SAT score against average teacher salary in each state, we see that the picture changes.\n\nggplot(data=SAT, aes( y=sat, x=salary )) +geom_point() + facet_wrap(facets = ~fracgrp) +\nstat_smooth(method=\"lm\", se=FALSE) + xlab(\"Average Teacher Salary in Thousands\")\n\n\n\n\n\n\n\n\nThere appears to be a slight positive relationship between teacher salary and SAT score in each state.\nWhile breaking up the data into these three groups helps us visualize, we’ll simply add the frac variable to the model as a quantitative variable, rather than breaking it into these arbitrary categories.\n\nSAT_M2 &lt;- lm(data=SAT, sat~salary+frac)\nsummary(SAT_M2)\n\n\nCall:\nlm(formula = sat ~ salary + frac, data = SAT)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-78.313 -26.731   3.168  18.951  75.590 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 987.9005    31.8775  30.991 &lt; 0.0000000000000002 ***\nsalary        2.1804     1.0291   2.119               0.0394 *  \nfrac         -2.7787     0.2285 -12.163   0.0000000000000004 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 33.69 on 47 degrees of freedom\nMultiple R-squared:  0.8056,    Adjusted R-squared:  0.7973 \nF-statistic: 97.36 on 2 and 47 DF,  p-value: &lt; 0.00000000000000022\n\n\nFor each one thousand dollar increase in average teacher salary, a state’s average SAT score is expected to increase by 2.18 points, assuming percentage of students taking the test is the same.\nFor each one percent increase in percentage of students taking the SAT, a state’s average score is expected to decrease by 2.78 points, assuming average teacher salary is the same.\nBoth of these estimates are associated with low p-values. While the effect of a 2 point increase per $1,000 in average teacher salary might seem small, the ~3 point decrease for each percentage point of students taking the exam is quite meaningful. According to the model, if the percentage of students taking the SAT is 10 percentage points higher than another, and the states pay their teachers the same, then the state with more people taking the exam is expected to have an average score almost 30 points lower.\nAdding percentage of students taking the exam increased the \\(R^2\\) value substantially.\nWe see that the relationship between SAT score and salary appears to reverse when we account for percentage of students taking the test. States with low percentages of people taking the SAT tend to get higher scores, as the people taking the test tend to be those who are best prepared and have strong incentive for taking it, perhaps because they are trying to get into an elite college. At the same time, states that pay their teachers more tend to have higher percentages of people taking the SAT. This may be because states that prioritize education are more likely to cover the cost of students taking the test, or even to require it. It may also be that many of the states that require the SAT are coastal states, where cost of living, and thus teacher salaries, tend to be higher in general. Thus, it appears initially that teacher salaries are negatively correlated with SAT scores, but after accounting for percentage taking the test, the trend reverses. Situations where an apparent trend disappears or reverses after accounting for another variable are called Simpson’s Paradox.\n\n\n5.3.1.3 Student-to-Teacher Ratio\nLet’s see what other possible explanatory variables we might want to add to the model. Keep in mind that our goal is to understand the relationship between teacher salary and SAT scores in the state, so we should only use variables that help us understand this relationship.\nIn addition to teacher salaries, student-to-teacher ratio might be an indication of a state’s investment in education. We’ll add student-to-teacher ratio to the model and explore whether there is evidence that hiring enough teachers to keep student-to-teacher ratio low has a benefit, in terms of SAT score.\n\nSAT_M3 &lt;- lm(data=SAT, sat~salary+frac+ratio)\nsummary(SAT_M3)\n\n\nCall:\nlm(formula = sat ~ salary + frac + ratio, data = SAT)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-89.244 -21.485  -0.798  17.685  68.262 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 1057.8982    44.3287  23.865 &lt;0.0000000000000002 ***\nsalary         2.5525     1.0045   2.541              0.0145 *  \nfrac          -2.9134     0.2282 -12.764 &lt;0.0000000000000002 ***\nratio         -4.6394     2.1215  -2.187              0.0339 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.41 on 46 degrees of freedom\nMultiple R-squared:  0.8239,    Adjusted R-squared:  0.8124 \nF-statistic: 71.72 on 3 and 46 DF,  p-value: &lt; 0.00000000000000022\n\n\nInterpretations\nOn average, a $1,000 dollar increase in average teacher salary is associated with a 2.5 point increase in average SAT score assuming fraction of students taking the SAT, and student to teacher ratio are held constant.\nOn average, a 1% increase in percentage of students taking the SAT is associated with a 2.9 point decrease in average SAT score assuming average teacher salary, and student to teacher ratio are held constant.\nOn average, a 1 student per teacher increase in student to teacher ratio is associated with a 4.6 point from in average SAT score, assuming average teacher salary, and percentage of students taking the SAT are held constant.\nWe see that student to teacher ratio is negatively associated with SAT score, with an expected drop of about 4.6 points in average SAT score for each additional student per teacher, assuming average teacher salary and percentage of students taking the exam are held constant. This suggests that states should try to keep student to teacher ratios low. We see teacher salary remains positively correlated with SAT score and percentage taking the test remains negatively correlated, after accounting for student to teacher ratio.\n\n\n5.3.1.4 Multicollinearity\nNext, let’s add the variable expend, which measures the state’s expenditure per pupil.\n\nSAT_M4 &lt;- lm(data=SAT, sat~salary+frac+ratio+expend)\nsummary(SAT_M4)\n\n\nCall:\nlm(formula = sat ~ salary + frac + ratio + expend, data = SAT)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-90.531 -20.855  -1.746  15.979  66.571 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 1045.9715    52.8698  19.784 &lt; 0.0000000000000002 ***\nsalary         1.6379     2.3872   0.686                0.496    \nfrac          -2.9045     0.2313 -12.559 0.000000000000000261 ***\nratio         -3.6242     3.2154  -1.127                0.266    \nexpend         4.4626    10.5465   0.423                0.674    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.7 on 45 degrees of freedom\nMultiple R-squared:  0.8246,    Adjusted R-squared:  0.809 \nF-statistic: 52.88 on 4 and 45 DF,  p-value: &lt; 0.00000000000000022\n\n\nIt may be surprising to see that after accounting for expenditure per student, teacher salary is still positively correlated with SAT score, but that the p-value associated with teacher salary is quite large. Likewise, while student-to-teacher ratio is still negatively associated, it too has a large p-value. Also notice that \\(R^2\\) barely increased when accounting for total expenditures.\nThis happens because expenditures are highly correlated with teacher salary. States that pay their teacher more also spend more on education per pupil. The scatterplot matrix below shows a strong correlation of 0.87 between teacher salary and expenditures.\n\nSAT_Num &lt;- select_if(SAT, is.numeric)\nC &lt;- cor(SAT_Num, use = \"pairwise.complete.obs\")\nround(C,2)\n\n       expend ratio salary  frac   sat\nexpend   1.00 -0.37   0.87  0.59 -0.38\nratio   -0.37  1.00   0.00 -0.21  0.08\nsalary   0.87  0.00   1.00  0.62 -0.44\nfrac     0.59 -0.21   0.62  1.00 -0.89\nsat     -0.38  0.08  -0.44 -0.89  1.00\n\n\n\nlibrary(corrplot)\ncorrplot(C)\n\n\n\n\n\n\n\n\nBecause these variables are highly correlated, it doesn’t make sense to talk about the effect of increasing teacher salary, while holding expenditure constant, or vice-versa. Notice the standard error on the salary line in model SAT_M4 (which includes expenditures) is more than twice as high as in SAT_M3, which did not. This happens because the model is not able to separate the effect of salary from the effect of expenditure, and thus becomes very uncertain of the effect of both, resulting in high standard errors. In addition to reducing the t-statistic, and increasing the p-value, this leads to much wider and less informative confidence intervals associated with the effect of teacher salary.\nConfidence intervals for model involving teacher salary, percentage taking the test, and student-to-teacher ratio.\n\nconfint(SAT_M3)\n\n                  2.5 %       97.5 %\n(Intercept) 968.6691802 1147.1271438\nsalary        0.5304797    4.5744605\nfrac         -3.3727807   -2.4539197\nratio        -8.9098147   -0.3690414\n\n\nConfidence intervals for model with above variables plus expenditure.\n\nconfint(SAT_M4)\n\n                 2.5 %      97.5 %\n(Intercept) 939.486374 1152.456698\nsalary       -3.170247    6.446081\nfrac         -3.370262   -2.438699\nratio       -10.100417    2.851952\nexpend      -16.779204   25.704393\n\n\nModels with highly correlated explanatory variables suffer from multicollinearity, which increases standard errors, making the effect of variables harder to discern. When we have explanatory variables that are highly correlated (usually with correlation greater than 0.8), we should pick out just one to include in the model. In this case, we’ll stick with teacher salary.\n\n\n5.3.1.5 Check Model Assumptions\nLet’s return to the model with salary, ratio, and fraction taking test. We use residual plots to assess model assumptions.\n\nP1 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M3$residuals, x=SAT_M3$fitted.values)) + geom_point() + ggtitle(\"Residual Plot\") + xlab(\"Predicted Values\") + ylab(\"Residuals\")\nP2 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(x=SAT_M3$residuals)) + geom_histogram() + ggtitle(\"Histogram of Residuals\") + xlab(\"Residual\")\nP3 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(sample = scale(SAT_M3$residuals))) + stat_qq() + stat_qq_line() + xlab(\"Normal Quantiles\") + ylab(\"Residual Quantiles\") + ggtitle(\"QQ Plot\")\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nThere is some sign of a quadratic trend in the residual plot, creating concern about the linearity assumption.\nIn models with multiple explanatory variables, it is helpful to also plot our residuals against the explanatory variables to see whether the model is properly accounting for relationships involving each variable. If we see nonlinear trends, we should consider adding a nonlinear function of that explanatory variable.\n\nP1 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M3$residuals, x=SAT_M3$model$salary)) + geom_point() + ggtitle(\"Residual by Predictor Plot\") + xlab(\"Salary\") + ylab(\"Residuals\") \nP2 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M3$residuals, x=SAT_M3$model$frac)) + geom_point() + ggtitle(\"Residual by Predictor Plot\") + xlab(\"Fraction Taking Test\") + ylab(\"Residuals\")\nP3 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M3$residuals, x=SAT_M3$model$ratio)) + geom_point() + ggtitle(\"Residual by Predictor Plot\") + xlab(\"Student to Teach Ratio\") + ylab(\"Residuals\")\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nThere is also a quadratic trend in the plot involving the fraction variable. We might account for this by adding a quadratic term for frac to the model.\n\n\n5.3.1.6 Quadratic Term\n\nSAT_M5 &lt;- lm(data=SAT, sat~salary+frac+I(frac^2)+ratio )\nsummary(SAT_M5)\n\n\nCall:\nlm(formula = sat ~ salary + frac + I(frac^2) + ratio, data = SAT)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-66.09 -15.20  -4.64  15.06  52.77 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 1039.21242   36.28206  28.643 &lt; 0.0000000000000002 ***\nsalary         1.80708    0.83150   2.173               0.0351 *  \nfrac          -6.64001    0.77668  -8.549      0.0000000000555 ***\nI(frac^2)      0.05065    0.01025   4.942      0.0000111676728 ***\nratio         -0.04058    1.96174  -0.021               0.9836    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.38 on 45 degrees of freedom\nMultiple R-squared:  0.8858,    Adjusted R-squared:  0.8757 \nF-statistic: 87.28 on 4 and 45 DF,  p-value: &lt; 0.00000000000000022\n\n\nWe notice a small p-value associated with the quadratic term, indicating SAT scores do indeed show evidence of a quadratic trend with respect to the percentage of students taking the test.\nWe now examine residual plots for the model that includes the quadratic term for frac.\n\nP1 &lt;- ggplot(data=data.frame(SAT_M5$residuals), aes(y=SAT_M5$residuals, x=SAT_M5$fitted.values)) + geom_point() + ggtitle(\"Residual Plot\") + xlab(\"Predicted Values\") + ylab(\"Residuals\")\nP2 &lt;- ggplot(data=data.frame(SAT_M5$residuals), aes(x=SAT_M5$residuals)) + geom_histogram() + ggtitle(\"Histogram of Residuals\") + xlab(\"Residual\")\nP3 &lt;- ggplot(data=data.frame(SAT_M5$residuals), aes(sample = scale(SAT_M5$residuals))) + stat_qq() + stat_qq_line() + xlab(\"Normal Quantiles\") + ylab(\"Residual Quantiles\") + ggtitle(\"QQ Plot\")\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\n\nP1 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M5$residuals, x=SAT_M5$model$salary)) + geom_point() + ggtitle(\"Residual by Predictor Plot\") + xlab(\"Salary\") + ylab(\"Residuals\") \nP2 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M5$residuals, x=SAT_M5$model$frac)) + geom_point() + ggtitle(\"Residual by Predictor Plot\") + xlab(\"Fraction Taking Test\") + ylab(\"Residuals\")\nP3 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M5$residuals, x=SAT_M5$model$ratio)) + geom_point() + ggtitle(\"Residual by Predictor Plot\") + xlab(\"Student to Teach Ratio\") + ylab(\"Residuals\")\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nThe quadratic trend in the residual by predicted plot and second residual by fraction plot appear to have disappeared, suggesting this model has properly accounted for the quadratic trend.\nInterpretations for Model with Quadratic Term\nOn average, a $1,000 dollar increase in average teacher salary is associated with a 1.8 point increase in average SAT score assuming fraction of students taking the SAT, and student to teacher ratio are held constant.\nOn average, a 1 student per teacher increase in student to teacher ratio is associated with a 0.05 point from in average SAT score, assuming average teacher salary, and percentage of students taking the SAT are held constant.\nWe cannot give a clear interpretation of the fraction variable, since it occurs in both linear and quadratic terms. In fact, the vertex of the parabola given by \\(y=-6.64x + 0.05x^2\\) occurs at \\(x=\\frac{6.64}{2(0.05)}\\approx 66\\). So the model estimates that SAT score decreases in a quadratic fashion with respect to fraction taking the test, until that fraction reaches 66 percent of student, then is expected to increase.\n\nggplot(data=SAT, aes(x=frac, y=sat)) + geom_point() + stat_smooth(se=FALSE)\n\n\n\n\n\n\n\n\nWe do see some possible quadratic trend, but we should be really careful about extrapolation. Although the trend does seem to level off in a quadratic way, we wouldn’t expect SAT scores to start to increase if more than 80 percent of students took the exam!\n\n\n5.3.1.7 Account for Region?\nSo far, we’ve considered only quantitative explanatory variables. What if we add region of the country to the model.\n\nSAT_M6 &lt;- lm(data=SAT, sat~salary+frac+I(frac^2)+ratio + region )\nsummary(SAT_M6)\n\n\nCall:\nlm(formula = sat ~ salary + frac + I(frac^2) + ratio + region, \n    data = SAT)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-45.309 -15.407  -1.996  13.852  41.859 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 1085.46629   37.25849  29.133 &lt; 0.0000000000000002 ***\nsalary         0.19485    0.86256   0.226             0.822378    \nfrac          -6.12514    0.84697  -7.232        0.00000000679 ***\nI(frac^2)      0.04762    0.01217   3.914             0.000327 ***\nratio          0.83366    1.99687   0.417             0.678452    \nregionNE     -11.53616   18.18986  -0.634             0.529384    \nregionS      -40.07482   11.14606  -3.595             0.000845 ***\nregionW      -15.89290   11.77634  -1.350             0.184386    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.58 on 42 degrees of freedom\nMultiple R-squared:  0.9148,    Adjusted R-squared:  0.9007 \nF-statistic: 64.46 on 7 and 42 DF,  p-value: &lt; 0.00000000000000022\n\n\nWe find that on average, SAT scores were lower in the NE, S, and W regions, compared to the baseline region of MW, though only in the S is the difference large enough to yield a small p-value.\nNotice that the effect of teacher salary and student-to-teacher ratio are no longer statistically significant. This happens because now we are only comparing states in the same region of the country. The p-value associated with teacher salary is now testing the null hypothesis “There is no relationship between average teacher salary and SAT score among states in the same region of the country, with the same percentage of students taking the test, and same student to teacher ratio.”\nBecause we only have 50 states to begin with, breaking down by region results in small sample sizes, which contributes to the large p-values. Furthermore, it is unclear why we would need to account for region here. If our goal is to assess the impact of educational spending on SAT scores, it is probably okay to compare states in different regions of the country. Unless we have some reason for wanting to compare states in the same region, we shouldn’t include region as an explanatory variable (even though including it did raise our \\(R^2\\) value above 0.9). In general, we should only include variables if they help us address our research question. In this case, it’s not clear that accounting for region helps us better understand the relationship between a state’s investment in education, and its students average SAT scores.\nIt is important to note that we should not decide whether to include a variable based on whether or not it yielded a small p-value. Adding or deleting variables from a model until we get a desired p-value on a variable we’re interested in can lead to Confirmation bias (that is choosing our model in a way that intentionally confirms what we expected or hoped to be true), and to detecting spurious correlations that will not be replicable in future studies. This phenomenon, known as p-hacking has led to incorrect and unethical conclusions. We should make modeling decisions about which variables to include in a model before looking at the p-values and then draw conclusions based on the results we see, keeping in mind that p-values are only a part of the picture.\nPredictions and Intervals\nGoing back to Model M5, which did not include region, we can make confidence and predictions intervals corresponding to hypothetical states.\n\nnewstate &lt;- data.frame(salary = 45, frac=0.5, ratio=15)\n\n\npredict(SAT_M5, newdata = newstate, interval=\"confidence\", conf.level=0.95)\n\n       fit     lwr    upr\n1 1116.615 1085.93 1147.3\n\n\nWe are 95% confident that the average of average SAT scores among all states with average teacher salary of 45 thousand dollars, where 50% of students take the SAT and having student-to-teacher ratio of 15 is between 1085 and 1147.\n\npredict(SAT_M5, newdata = newstate, interval=\"prediction\", conf.level=0.95)\n\n       fit      lwr      upr\n1 1116.615 1055.256 1177.973\n\n\nWe are 95% confident that an individual state with average teacher salary of 45 thousand dollars, where 50% of students take the SAT and having student-to-teacher ratio of 15 will have an average SAT score between 1055 and 1178.\n\n\n\n5.3.2 Modeling Car Price\nWe’ll build a model for the price of a new 2015 car, to help us understand what factors are related to the price of a car.\n\ndata(Cars2015)\nCars2015 &lt;- Cars2015 %&gt;% rename(Price=LowPrice)\nCars2015 &lt;- Cars2015%&gt;% select(-HighPrice)\nglimpse(Cars2015)\n\nRows: 110\nColumns: 19\n$ Make      &lt;fct&gt; Chevrolet, Hyundai, Kia, Mitsubishi, Nissan, Dodge, Chevrole…\n$ Model     &lt;fct&gt; Spark, Accent, Rio, Mirage, Versa Note, Dart, Cruze LS, 500L…\n$ Type      &lt;fct&gt; Hatchback, Hatchback, Sedan, Hatchback, Hatchback, Sedan, Se…\n$ Price     &lt;dbl&gt; 12.270, 14.745, 13.990, 12.995, 14.180, 16.495, 16.170, 19.3…\n$ Drive     &lt;fct&gt; FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, AWD, …\n$ CityMPG   &lt;int&gt; 30, 28, 28, 37, 31, 23, 24, 24, 28, 30, 27, 27, 25, 27, 30, …\n$ HwyMPG    &lt;int&gt; 39, 37, 36, 44, 40, 35, 36, 33, 38, 35, 33, 36, 36, 37, 39, …\n$ FuelCap   &lt;dbl&gt; 9.0, 11.4, 11.3, 9.2, 10.9, 14.2, 15.6, 13.1, 12.4, 11.1, 11…\n$ Length    &lt;int&gt; 145, 172, 172, 149, 164, 184, 181, 167, 179, 154, 156, 180, …\n$ Width     &lt;int&gt; 63, 67, 68, 66, 67, 72, 71, 70, 72, 67, 68, 69, 70, 68, 69, …\n$ Wheelbase &lt;int&gt; 94, 101, 101, 97, 102, 106, 106, 103, 104, 99, 98, 104, 104,…\n$ Height    &lt;int&gt; 61, 57, 57, 59, 61, 58, 58, 66, 58, 59, 58, 58, 57, 58, 59, …\n$ UTurn     &lt;int&gt; 34, 37, 37, 32, 37, 38, 38, 37, 39, 34, 35, 38, 37, 36, 37, …\n$ Weight    &lt;int&gt; 2345, 2550, 2575, 2085, 2470, 3260, 3140, 3330, 2990, 2385, …\n$ Acc030    &lt;dbl&gt; 4.4, 3.7, 3.5, 4.4, 4.0, 3.4, 3.7, 3.9, 3.4, 3.9, 3.9, 3.7, …\n$ Acc060    &lt;dbl&gt; 12.8, 10.3, 9.5, 12.1, 10.9, 9.3, 9.8, 9.5, 9.2, 10.8, 11.1,…\n$ QtrMile   &lt;dbl&gt; 19.4, 17.8, 17.3, 19.0, 18.2, 17.2, 17.6, 17.4, 17.1, 18.3, …\n$ PageNum   &lt;int&gt; 123, 148, 163, 188, 196, 128, 119, 131, 136, 216, 179, 205, …\n$ Size      &lt;fct&gt; Small, Small, Small, Small, Small, Small, Small, Small, Smal…\n\n\nExploratory Analysis\nWe’ll look at a summary of the categorical variables in the dataset.\n\nCars_Cat &lt;- select_if(Cars2015, is.factor)\nsummary(Cars_Cat)\n\n        Make            Model            Type    Drive          Size   \n Chevrolet: 8   CTS        :  2   7Pass    :15   AWD:25   Large   :29  \n Ford     : 7   2 Touring  :  1   Hatchback:11   FWD:63   Midsized:34  \n Hyundai  : 7   200        :  1   Sedan    :46   RWD:22   Small   :47  \n Toyoto   : 7   3 i Touring:  1   Sporty   :11                         \n Audi     : 6   3 Series GT:  1   SUV      :18                         \n Nissan   : 6   300        :  1   Wagon    : 9                         \n (Other)  :69   (Other)    :103                                        \n\n\nWe examine the correlation matrix of quantitative variables.\n\nCars_Num &lt;- select_if(Cars2015, is.numeric)\nC &lt;- cor(Cars_Num, use = \"pairwise.complete.obs\")\nround(C,2)\n\n          Price CityMPG HwyMPG FuelCap Length Width Wheelbase Height UTurn\nPrice      1.00   -0.65  -0.59    0.57   0.47  0.48      0.46   0.02  0.40\nCityMPG   -0.65    1.00   0.93   -0.77  -0.72 -0.78     -0.69  -0.39 -0.73\nHwyMPG    -0.59    0.93   1.00   -0.75  -0.64 -0.75     -0.64  -0.54 -0.68\nFuelCap    0.57   -0.77  -0.75    1.00   0.82  0.85      0.79   0.58  0.76\nLength     0.47   -0.72  -0.64    0.82   1.00  0.81      0.92   0.46  0.84\nWidth      0.48   -0.78  -0.75    0.85   0.81  1.00      0.76   0.62  0.77\nWheelbase  0.46   -0.69  -0.64    0.79   0.92  0.76      1.00   0.49  0.81\nHeight     0.02   -0.39  -0.54    0.58   0.46  0.62      0.49   1.00  0.55\nUTurn      0.40   -0.73  -0.68    0.76   0.84  0.77      0.81   0.55  1.00\nWeight     0.55   -0.83  -0.84    0.91   0.82  0.91      0.81   0.71  0.80\nAcc030    -0.76    0.64   0.51   -0.47  -0.38 -0.41     -0.31   0.21 -0.36\nAcc060    -0.74    0.68   0.52   -0.49  -0.47 -0.46     -0.38   0.21 -0.41\nQtrMile   -0.76    0.65   0.49   -0.45  -0.42 -0.41     -0.35   0.25 -0.37\nPageNum   -0.23    0.28   0.15   -0.15  -0.23 -0.20     -0.24   0.06 -0.22\n          Weight Acc030 Acc060 QtrMile PageNum\nPrice       0.55  -0.76  -0.74   -0.76   -0.23\nCityMPG    -0.83   0.64   0.68    0.65    0.28\nHwyMPG     -0.84   0.51   0.52    0.49    0.15\nFuelCap     0.91  -0.47  -0.49   -0.45   -0.15\nLength      0.82  -0.38  -0.47   -0.42   -0.23\nWidth       0.91  -0.41  -0.46   -0.41   -0.20\nWheelbase   0.81  -0.31  -0.38   -0.35   -0.24\nHeight      0.71   0.21   0.21    0.25    0.06\nUTurn       0.80  -0.36  -0.41   -0.37   -0.22\nWeight      1.00  -0.41  -0.43   -0.39   -0.20\nAcc030     -0.41   1.00   0.95    0.95    0.25\nAcc060     -0.43   0.95   1.00    0.99    0.26\nQtrMile    -0.39   0.95   0.99    1.00    0.26\nPageNum    -0.20   0.25   0.26    0.26    1.00\n\n\n\nlibrary(corrplot)\nC &lt;- corrplot(C)\n\n\n\n\n\n\n\n\nNote the high correlation between many variables in the dataset. We’ll need to be careful about multicollinearity.\n\n5.3.2.1 Acc. and Qrt. Mile Time\nWe saw in Section 5.2 that it was better to model log(Price) than price itself, so we’ll continue modeling logprice here.\nModel Using Just Acceleration Time\nFirst, we fit a model using only the time it takes to accelerate from 0 to 60 mph as an explanatory variable.\n\nCars_M1 &lt;- lm(data=Cars2015, log(Price) ~ Acc060)\nsummary(Cars_M1)\n\n\nCall:\nlm(formula = log(Price) ~ Acc060, data = Cars2015)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.84587 -0.19396  0.00908  0.18615  0.53350 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  5.13682    0.13021   39.45 &lt;0.0000000000000002 ***\nAcc060      -0.22064    0.01607  -13.73 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.276 on 108 degrees of freedom\nMultiple R-squared:  0.6359,    Adjusted R-squared:  0.6325 \nF-statistic: 188.6 on 1 and 108 DF,  p-value: &lt; 0.00000000000000022\n\n\nConfidence Interval for Effect of Acceleration Time:\n\nexp(confint(Cars_M1))\n\n                  2.5 %     97.5 %\n(Intercept) 131.4610408 220.284693\nAcc060        0.7768669   0.827959\n\n\nWe are 95% confident that a 1-second increase in acceleration time is associated with an average price decrease between 17% and 22.5%.\nModel Using Just Quarter Mile Time\nNow, let’s fit a different model using only the time it takes to drive a quarter mile as an explanatory variable.\n\nCars_M2 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile)\nsummary(Cars_M2)\n\n\nCall:\nlm(formula = log(Price) ~ QtrMile, data = Cars2015)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.91465 -0.19501  0.02039  0.17538  0.60073 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)   7.8559     0.3248   24.19 &lt;0.0000000000000002 ***\nQtrMile      -0.2776     0.0201  -13.81 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.275 on 108 degrees of freedom\nMultiple R-squared:  0.6385,    Adjusted R-squared:  0.6351 \nF-statistic: 190.7 on 1 and 108 DF,  p-value: &lt; 0.00000000000000022\n\n\nConfidence Interval for Effect of Quarter Mile Time:\n\nexp(confint(Cars_M2))\n\n                   2.5 %      97.5 %\n(Intercept) 1355.8297704 4913.077313\nQtrMile        0.7279941    0.788385\n\n\nWe are 95% confident that a 1-second increase in quarter mile time is associated with a price decrease between 21% and 27%, on average.\nModel Using Both Acceleration and Quarter Mile Time\n\nCars_M3 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile + Acc060)\nsummary(Cars_M3)\n\n\nCall:\nlm(formula = log(Price) ~ QtrMile + Acc060, data = Cars2015)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89124 -0.20030  0.01001  0.17576  0.57462 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)  6.83974    1.54354   4.431 0.0000227 ***\nQtrMile     -0.17316    0.15640  -1.107     0.271    \nAcc060      -0.08389    0.12455  -0.673     0.502    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2757 on 107 degrees of freedom\nMultiple R-squared:   0.64, Adjusted R-squared:  0.6332 \nF-statistic:  95.1 on 2 and 107 DF,  p-value: &lt; 0.00000000000000022\n\n\nConfidence Intervals from 2-variable Model\n\nexp(confint(Cars_M3))\n\n                 2.5 %       97.5 %\n(Intercept) 43.8095999 19922.799158\nQtrMile      0.6168071     1.146686\nAcc060       0.7183525     1.177065\n\n\nIt does not make sense to talk about holding QtrMile constant as Acc060 increases, or vice-versa. Trying to do so leads to nonsensical answers.\nWe are 95% confident that a 1-second increase in quarter mile time is associated with an average price change between a 38% decrease and 15% increase, assuming acceleration time is held constant.\nWe are 95% confident that a 1-second increase in acceleration time is associated with an average price change between a 28% decrease and 18% increase, assuming quarter mile time is held constant.\nBecause these variables are so highly correlated, it the model cannot separate the effect of one from the other, and thus is uncertain about both. Notice the very large standard errors associated with both regression coefficients, which lead to very wide confidence intervals.\nIn fact, if two variables are perfectly correlated, it will be impossible to fit them both in a model, and you will get an error message.\nImpact on Prediction\nSuppose we want to predict the price of a car that can accelerate from 0 to 60 mph in 9.5 seconds, and completes a quarter mile in 17.3 seconds.\n\nexp(predict(Cars_M1, newdata = data.frame(Acc060=9.5, QtrMile=17.3)))\n\n       1 \n20.92084 \n\n\n\nexp(predict(Cars_M2, newdata = data.frame(Acc060=9.5, QtrMile=17.3)))\n\n       1 \n21.18223 \n\n\n\nexp(predict(Cars_M3, newdata = data.frame(Acc060=9.5, QtrMile=17.3)))\n\n       1 \n21.05489 \n\n\nThe predicted values are similar. Multicollinearity does not hurt predictions, only interpretations.\n\n\n5.3.2.2 Adding Weight to Model\nWe could use either quarter mile time or acceleration time as an explanatory variable, but we shouldn’t use both. We’ll proceed with quarter mile time.\n\nCars_M4 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile + Weight)\nsummary(Cars_M4)\n\n\nCall:\nlm(formula = log(Price) ~ QtrMile + Weight, data = Cars2015)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.79365 -0.13931 -0.01368  0.15773  0.42234 \n\nCoefficients:\n               Estimate  Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  6.21326823  0.33491778  18.552 &lt; 0.0000000000000002 ***\nQtrMile     -0.22482146  0.01748563 -12.858 &lt; 0.0000000000000002 ***\nWeight       0.00020606  0.00002641   7.803      0.0000000000043 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2206 on 107 degrees of freedom\nMultiple R-squared:  0.7696,    Adjusted R-squared:  0.7653 \nF-statistic: 178.7 on 2 and 107 DF,  p-value: &lt; 0.00000000000000022\n\n\n\\(R^2\\) went up from 0.64 to 0.76!\nWe might consider adding an interaction term between quarter mile time and weight. This would mean that we think the effect of quarter mile time on price of a car is different for heavier cars than for lighter cars. It’s not clear to me why that would be the case.\n\nCars_M5 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile * Weight)\nsummary(Cars_M5)\n\n\nCall:\nlm(formula = log(Price) ~ QtrMile * Weight, data = Cars2015)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82013 -0.12076 -0.01464  0.14717  0.41928 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)     4.1114189  1.3270870   3.098  0.00249 **\nQtrMile        -0.0963226  0.0804413  -1.197  0.23381   \nWeight          0.0008110  0.0003707   2.188  0.03089 * \nQtrMile:Weight -0.0000373  0.0000228  -1.636  0.10482   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 106 degrees of freedom\nMultiple R-squared:  0.7752,    Adjusted R-squared:  0.7689 \nF-statistic: 121.9 on 3 and 106 DF,  p-value: &lt; 0.00000000000000022\n\n\np-value on interaction is not that small. \\(R^2\\) didn’t go up much. There doesn’t seem to be much reason to complicate the model by adding an interaction term.\n\n\n5.3.2.3 Adding More Variables\nWe’ll consider adding highway MPG to the model.\n\nCars_M6 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile + Weight + HwyMPG)\nsummary(Cars_M6)\n\n\nCall:\nlm(formula = log(Price) ~ QtrMile + Weight + HwyMPG, data = Cars2015)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82308 -0.14513 -0.01922  0.16732  0.41390 \n\nCoefficients:\n               Estimate  Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  6.54954436  0.42196132  15.522 &lt; 0.0000000000000002 ***\nQtrMile     -0.21699008  0.01843615 -11.770 &lt; 0.0000000000000002 ***\nWeight       0.00015922  0.00004456   3.573             0.000532 ***\nHwyMPG      -0.00961141  0.00737658  -1.303             0.195410    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2198 on 106 degrees of freedom\nMultiple R-squared:  0.7732,    Adjusted R-squared:  0.7668 \nF-statistic: 120.5 on 3 and 106 DF,  p-value: &lt; 0.00000000000000022\n\n\nHwyMPG doesn’t make change \\(R^2\\) much, and has a high correlation with weight. Let’s not include it.\nNext, we’ll consider adding categorical explanatory variables Size, and Drive.\n\nP1 &lt;- ggplot(data=Cars2015, aes(x=log(Price), y=Size)) + geom_boxplot() + ggtitle(\"Price by Size\")\nP2 &lt;- ggplot(data=Cars2015, aes(x=log(Price), y=Drive)) + geom_boxplot() + ggtitle(\"Price by Drive\")\ngrid.arrange(P1, P2, ncol=2)\n\n\n\n\n\n\n\n\nInformation about size is already included, through the weight variable. Let’s add drive type to the model.\n\nCars_M7 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile + Weight + Drive)\nsummary(Cars_M7)\n\n\nCall:\nlm(formula = log(Price) ~ QtrMile + Weight + Drive, data = Cars2015)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.72386 -0.10882  0.01269  0.13306  0.45304 \n\nCoefficients:\n               Estimate  Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  5.81406789  0.33961789  17.119 &lt; 0.0000000000000002 ***\nQtrMile     -0.19007439  0.01959554  -9.700 0.000000000000000289 ***\nWeight       0.00020496  0.00002583   7.936 0.000000000002420675 ***\nDriveFWD    -0.22403222  0.05704513  -3.927             0.000154 ***\nDriveRWD    -0.13884399  0.06227709  -2.229             0.027913 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2077 on 105 degrees of freedom\nMultiple R-squared:  0.7995,    Adjusted R-squared:  0.7919 \nF-statistic: 104.7 on 4 and 105 DF,  p-value: &lt; 0.00000000000000022\n\n\nWe found evidence of differences in price between front-wheel drive and rear-wheel drive, compared to all wheel drive cars.\nNext, we’ll explore adding size to the model.\n\nCars_M8 &lt;- lm(data=Cars2015, log(Price) ~ QtrMile + Weight + Drive + Size)\nsummary(Cars_M8)\n\n\nCall:\nlm(formula = log(Price) ~ QtrMile + Weight + Drive + Size, data = Cars2015)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.71092 -0.12126  0.01355  0.11831  0.44439 \n\nCoefficients:\n                Estimate  Std. Error t value             Pr(&gt;|t|)    \n(Intercept)   5.66594310  0.37169625  15.243 &lt; 0.0000000000000002 ***\nQtrMile      -0.19256547  0.02000711  -9.625 0.000000000000000505 ***\nWeight        0.00023978  0.00004101   5.847 0.000000059416930182 ***\nDriveFWD     -0.21598794  0.05780323  -3.737             0.000306 ***\nDriveRWD     -0.15259183  0.06410851  -2.380             0.019142 *  \nSizeMidsized  0.04699095  0.06271499   0.749             0.455398    \nSizeSmall     0.08875861  0.08105810   1.095             0.276071    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2085 on 103 degrees of freedom\nMultiple R-squared:  0.8018,    Adjusted R-squared:  0.7903 \nF-statistic: 69.46 on 6 and 103 DF,  p-value: &lt; 0.00000000000000022\n\n\nAdding size barely increased \\(R^2\\) at all. We find no evidence of differences in price between the three sizes, after accounting for the other variables.\nNote: Information about car size is already being taken into account through the Weight variable.\nWe could keep looking at other variables to add, but at this point, we have a model that gives us a good sense of the factors related to price of a car, capturing 80% of total variability in car price, and is still easy to interpret.\nFor our research purposes, this model is good enough.\n\n\n5.3.2.4 Check of Model Assumptions\nWe’ll use residuals to check the model assumptions.\nResidual by Predicted Plot, Histogram of Residuals, and Normal Quantile-Quantile Plot\n\nP1 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(y=Cars_M7$residuals, x=Cars_M7$fitted.values)) + geom_point() + ggtitle(\"Residual Plot\") + xlab(\"Predicted Values\") + ylab(\"Residuals\")\nP2 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(x=Cars_M7$residuals)) + geom_histogram() + ggtitle(\"Histogram of Residuals\") + xlab(\"Residual\")\nP3 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(sample = scale(Cars_M7$residuals))) + stat_qq() + stat_qq_line() + xlab(\"Normal Quantiles\") + ylab(\"Residual Quantiles\") + ggtitle(\"QQ Plot\")\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nThere is slight concern about constant variance, but otherwise, the model assumptions look good.\nResidual by Predictor Plots\n\nP1 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(y=Cars_M7$residuals, x=Cars_M7$model$QtrMile)) + geom_point() + ggtitle(\"Residual by Predictor Plot\") + xlab(\"QtrMile\") + ylab(\"Residuals\") \nP2 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(y=Cars_M7$residuals, x=Cars_M7$model$Weight)) + geom_point() + ggtitle(\"Residual by Predictor Plot\") + xlab(\"Weight\") + ylab(\"Residuals\") \nP3 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(y=Cars_M7$residuals, x=Cars_M7$model$Drive)) + geom_point() + ggtitle(\"Residual by Predictor Plot\") + xlab(\"Drive\") + ylab(\"Residuals\") \ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nThese plots don’t raise any concerns.\n\n\n5.3.2.5 Coefficients and Exponentiation\nThe model coefficients are shown below.\n\nCars_M7$coefficients\n\n  (Intercept)       QtrMile        Weight      DriveFWD      DriveRWD \n 5.8140678915 -0.1900743859  0.0002049586 -0.2240322171 -0.1388439916 \n\n\nSince we used a log transformation, we should interpret \\(e^{b_j}\\) rather than \\(b_j\\) itself.\n\nexp(Cars_M7$coefficients)\n\n(Intercept)     QtrMile      Weight    DriveFWD    DriveRWD \n334.9790161   0.8268976   1.0002050   0.7992894   0.8703638 \n\n\nThe price of a car is expected to decrease by 17% for each additional second it takes to drive a quartermile, assuming weight, and drive type are held constant.\nThe price of a car is expected to increase by 0.02% for each additional pound, assuming quarter mile time, and drive type are held constant. Thus, a 100 lb increase is associated with an expected 2% increase in price, assuming quarter mile time, and drive type are held constant.\nFWD cars are expected to cost 20% less than AWD cars, assuming quarter mile time and weight are held constant.\nRWD cars are expected to cost 13% less than AWD cars, assuming quarter mile time and weight are held constant.\n\n\n5.3.2.6 Confidence and Prediction Intevals\nWe’ll use our model to estimate the average price with the following characteristics, and also to predict the price of a new car with the given characteristics.\n\nnewcar &lt;- data.frame(QtrMile = 18, Weight=2400, Drive = \"AWD\")\n\nThis is an interval for log(Price).\n\npredict(Cars_M7, newdata=newcar, interval=\"confidence\", level=0.95)\n\n       fit      lwr      upr\n1 2.884629 2.741423 3.027836\n\n\nExponentiating, we obtain\n\nexp(predict(Cars_M7, newdata=newcar, interval=\"confidence\", level=0.95))\n\n       fit      lwr      upr\n1 17.89693 15.50904 20.65249\n\n\nWe are 95% Confident that the average price of all new 2015 cars that weigh 2400 lbs, drive a quarter mile in 18 seconds on a fast track, and have all wheel drive is between 15.5 thousand and 20.7 thousand dollars.\nNext, we calculate a prediction interval for an individual car with these characteristics.\n\nexp(predict(Cars_M7, newdata=newcar, interval=\"prediction\", level=0.95))\n\n       fit      lwr     upr\n1 17.89693 11.57309 27.6763\n\n\nWe are 95% Confident that the price of an individual new 2015 car that weighs 2400 lbs, drives a quarter mile in 18 seconds on a fast track, and has all wheel drive will be between 11.6 thousand and 27.7 thousand dollars.\n\n\n5.3.2.7 Model Building Summary\nConsider the following when building a model for the purpose of interpreting parameters and understanding and drawing conclusions about a population or process.\n\nModel driven by research question\nInclude variables of interest\n\nInclude potential confounders (like in SAT example)\n\nAvoid including highly correlated explanatory variables\n\nAvoid messy transformations and interactions where possible\n\nUse residual plots to assess appropriateness of model assumptions\n\nAim for high \\(R^2\\) but not highest\n\nAim for model complex enough to capture nature of data, but simple enough to give clear interpretations",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Building and Assessing Models</span>"
    ]
  },
  {
    "objectID": "Ch6.html",
    "href": "Ch6.html",
    "title": "6  Logistic Regression",
    "section": "",
    "text": "6.1 Logistic Regression",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Ch6.html#logistic-regression",
    "href": "Ch6.html#logistic-regression",
    "title": "6  Logistic Regression",
    "section": "",
    "text": "6.1.1 Modeling Binary Response\nSo far, we have modeled only quantitative response variables. The normal error regression model makes the assumption that the response variable is normally distributed, given the value(s) of the explanatory variables.\nNow, we’ll look at how to model a categorical response variable. We’ll consider only situations where the response is binary (i.e. has 2 categories). Problems with categorical response variables are sometimes called classification problems, while problems with numeric response variables are sometimes called regression problems.\n\n\n6.1.2 Credit Card Dataset\nWe’ll work with a dataset pertaining to 10,000 credit cards. The goal is to predict whether or not the user will default on the payment, using information on the credit card balance, user’s annual income, and whether or not the user is a student. Data come from Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani.\n\nlibrary(ISLR)\ndata(Default)\nsummary(Default)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\n\nDefault and Balance\nThe plot displays each person’s credit card balance on the x-axis, and whether or not they defaulted (a 0 or 1) on the y-axis.\n\nggplot(data=Default, aes(y=default, x=balance)) + geom_point(alpha=0.2) \n\n\n\n\n\n\n\n\nWe see that defaults are rare when the balance is less than $1,000, and more common for balances above $2,000.\nWe’ll first try fitting a linear regression model to the data to try to estimate the probability of a person defaulting on a loan, using the size of their balance as the explanatory variable.\n\n#convert default from yes/no to 0/1\nDefault$default &lt;- as.numeric(Default$default==\"Yes\") \n\n\nggplot(data=Default, aes(y=default, x= balance)) + geom_point(alpha=0.2)  + stat_smooth(method=\"lm\", se=FALSE)\n\n\n\n\n\n\n\n\nThere are a lot of problems with this model!\nIt allows the estimated probability of of default to be negative. It also assumes a linear trend that doesn’t seem to fit the data very well.\nA sigmoidal curve, like the one below, seems like a better model for default probabilities. This curve stays between 0 and 1, and its curved nature seems like a better fit for the data.\n\nggplot(data=Default, aes(y=default, x= balance)) + geom_point(alpha=0.2) + \n  stat_smooth(method=\"glm\", se=FALSE, method.args = list(family=binomial)) \n\n\n\n\n\n\n\n\n\n\n6.1.3 Logistic Regression Model\nA logistic regression model uses a sigmoidal curve like the one we saw to model default probabilities, using balance as an explanatory variable.\nThe model makes use of the function\n\\[ f(x) = \\frac{e^x}{1+x^x}, \\]\nwhose graph is shown below. This function is called an inverse logit function.\n\n\n\n\n\n\n\n\n\nStarting with our linear model \\(E(Y_i) = \\beta_0+\\beta_1x_{i1}\\), we need to transform \\(\\beta_0+\\beta_1x_{i1}\\) into the interval (0,1).\n\nLet \\(\\pi_i = \\frac{e^{\\beta_0+\\beta_1x_{i1} }}{1+e^{\\beta_0+\\beta_1x_{i1}}}\\).\nThen \\(0 \\leq \\pi_i \\leq 1\\), and \\(\\pi_i\\) represents an estimate of \\(P(Y_i=1)\\).\nThis function maps the values of \\(\\beta_0+\\beta_1x_{i1}\\) into the interval (0,1).\n\n\n\n\n\n\n\n\n\n\nThe logistic regression model assumes that:\n\n\n\\(Y_i \\in \\{0,1\\}\\)\n\n\\(E(Y_i) = P(Y_i=1) = \\pi_i=\\frac{e^{\\beta_0+\\beta_1x_{i1} + \\ldots \\beta_px_{ip}}}{1+e^{\\beta_0+\\beta_1x_{i1} + \\ldots \\beta_px_{ip}}}\\) i.e. \\(\\beta_0+\\beta_1x_{i1} + \\ldots \\beta_px_{ip}= \\text{log}\\left(\\frac{\\pi_i}{1-\\pi_i}\\right).\\) (This is called the logit function and can be written \\(\\text{logit}(\\pi_i)\\).\n\nInstead of assuming that the expected response is a linear function of the explanatory variables, we are assuming that it is a function of a linear function of the explanatory variables.\nWe fit the logistic curve to the credit card data.\n\nggplot(data=Default, aes(y=default, x= balance)) + geom_point(alpha=0.2) + \n  stat_smooth(method=\"glm\", se=FALSE, method.args = list(family=binomial)) \n\n\n\n\n\n\n\n\nTo fit the logistic regression model in R, we use the function glm, instead of lm. The function is specified the same way as before, and we add family = binomial(link = \"logit\").\n\nCCDefault_M &lt;- glm(data=Default, default ~ balance, family = binomial(link = \"logit\"))\nsummary(CCDefault_M)\n\n\nCall:\nglm(formula = default ~ balance, family = binomial(link = \"logit\"), \n    data = Default)\n\nCoefficients:\n               Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept) -10.6513306   0.3611574  -29.49 &lt;0.0000000000000002 ***\nbalance       0.0054989   0.0002204   24.95 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n\n\nThe regression equation is:\n\\[ P(\\text{Default}) = \\hat{\\pi}_i = \\frac{e^{-10.65+0.0055\\times\\text{balance}}}{1+e^{-10.65+0.0055\\times\\text{balance}}} \\]\nPredictions\n\nFor a $1,000 balance, the estimated default probability is \\(\\frac{e^{-10.65+0.0055(1000) }}{1+e^{-10.65+0.0055(1000)}} \\approx 0.006\\)\nFor a $1,500 balance, the estimated default probability is \\(\\frac{e^{-10.65+0.0055(1500) }}{1+e^{-10.65+0.0055(1500)}} \\approx 0.08\\)\nFor a $2,000 balance, the estimated default probability is \\(\\frac{e^{-10.65+0.0055(2000) }}{1+e^{-10.65+0.0055(2000)}} \\approx 0.59\\)\n\nWe confirm these, using the predict command in R.\n\npredict(CCDefault_M, newdata=data.frame((balance=1000)), type=\"response\")\n\n          1 \n0.005752145 \n\n\n\npredict(CCDefault_M, newdata=data.frame((balance=1500)), type=\"response\")\n\n         1 \n0.08294762 \n\n\n\npredict(CCDefault_M, newdata=data.frame((balance=2000)), type=\"response\")\n\n        1 \n0.5857694 \n\n\n\n6.1.3.1 Where do the b’s come from?\nRecall that for a quantitative response variable, the values of \\(b_1, b_2, \\ldots, b_p\\) are chosen in a way that minimizes \\(\\displaystyle\\sum_{i=1}^n \\left(y_i-(\\beta_0+\\beta_1x_{i1}+\\ldots+\\beta_px_{ip})^2\\right)\\). Least squares does not work well in this generalized setting. Instead, the b’s are calculated using a more advanced technique, known as maximum likelihood estimation. We won’t say anything more about that topic here, but it is a prominent technique, widely used in statistic modeling. It is explored in more detail in advanced statistics courses, such as STAT 445:Mathematical Statistics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Ch6.html#interpreting-coefficients-in-logistic-regression",
    "href": "Ch6.html#interpreting-coefficients-in-logistic-regression",
    "title": "6  Logistic Regression",
    "section": "6.2 Interpreting Coefficients in Logistic Regression",
    "text": "6.2 Interpreting Coefficients in Logistic Regression\n\n6.2.1 Odds and Odds Ratio\nFor an event with probability \\(p\\), the odds of the event occurring are \\(\\frac{p}{1-p}\\).\nExamples: 1. The odds of a fair coin landing heads are \\(\\frac{0.5}{1-0.5}=1\\), sometimes written 1:1.\n\nThe odds of a fair 6-sided die landing on a 1 are \\(\\frac{1/6}{1-1/6}=\\frac{1}{5}\\), sometimes written 1:5.\n\nIn the credit card example, the odds of default are:\n\nFor a $1,000 balance - odds of default are \\(\\frac{0.005752145}{1-0.005752145} \\approx 1:173.\\)\nFor a $1,500 balance - odds of default are \\(\\frac{0.08294762 }{1-0.08294762 } \\approx 1:11.\\)\nFor a $2,000 balance - odds of default are \\(\\frac{0.5857694}{1-0.5857694} \\approx 1.414:1.\\)\n\nThe quantity \\(\\frac{\\frac{\\pi_i}{1-\\pi_i}}{\\frac{\\pi_j}{1-\\pi_j}}\\) is called the odds ratio and represents the odds ratio of a default for user \\(i\\), compared to user \\(j\\).\nExample:\nThe default odds ratio for a $1,000 payment, compared to a $2,000 payment is\nThe odds ratio is \\(\\frac{\\frac{1}{173}}{\\frac{1.414}{1}}\\approx 1:244.\\)\nThe odds of a default are about 244 times larger for a $2,000 payment than a $1,000 payment.\n\n\n6.2.2 Interpretation of \\(\\beta_1\\)\nConsider the odds ratio for a case \\(j\\) with explanatory variable \\(x + 1\\), compared to case \\(i\\) with explanatory variable \\(x\\).\nThat is \\(\\text{log}\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\beta_0+\\beta_1x\\), and \\(\\text{log}\\left(\\frac{\\pi_j}{1-\\pi_j}\\right) = \\beta_0+\\beta_1(x+1)\\).\n\\(\\text{log}\\left(\\frac{\\frac{\\pi_j}{1-\\pi_j}}{\\frac{\\pi_i}{1-\\pi_i}}\\right)=\\text{log}\\left(\\frac{\\pi_j}{1-\\pi_j}\\right)-\\text{log}\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)=\\beta_0+\\beta_1(x+1)-(\\beta_0+\\beta_1(x))=\\beta_1.\\)\nFor every 1-unit increase in \\(x\\) we expect the log odds of “success” to multiply by a factor of \\(\\beta_1\\).\nFor every 1-unit increase in \\(x\\) we expect the odds of “success” to multiply by a factor of \\(e^{\\beta_1}\\).\nInterpretation in Credit Card Example\n\\(b_1=0.0055\\)\nFor each 1-dollar increase in balance on the credit card., the odds of default are estimated to multiply by \\(e^{0.0055}\\approx1.0055\\).\nThat is, for each additional dollar on the card balance, the odds of default are estimated to increase by 0.55%\nFor each increase of \\(d\\) dollars in credit card balance, odds of default are estimated to multiply by a factor of \\(e^{0.0055d}\\).\nFor every $1,000 increase in balance, the odds of default are expected to multiply by a factor of \\(e^{0.0055\\times 1000}\\approx 244\\).\nThus, the odds of default for a balance of $2,000 are estimated to be \\(e^{0.0055\\times 1000}\\approx 244\\) times as great as the odds of default for a $1,000 balance. This matches our result when we actually calculated out the probabilities and odds.\nHypothesis test for \\(\\beta_1=0\\)\nThe p-value on the “balance” line of the regression output is associated with the null hypothesis \\(\\beta_1=0\\), that is that there is no relationship between balance and the odds of defaulting on the payment.\nThe fact that the p-value is so small tells us that there is strong evidence of a relationship between balance and odds of default.\nConfidence Intervals for \\(\\beta_1\\)\n\nconfint(CCDefault_M, level = 0.95)\n\n                    2.5 %       97.5 %\n(Intercept) -11.383288936 -9.966565064\nbalance       0.005078926  0.005943365\n\n\nWe are 95% confident that for each 1 dollar increase in credit card balance, the odds of default are expected to multiply by a factor between \\(e^{0.00508}\\approx 1.0051\\) and \\(e^{0.00594}\\approx 1.0060\\).\nThis is a profile-likelihood interval, which you can read more about here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Ch6.html#multiple-logistic-regression",
    "href": "Ch6.html#multiple-logistic-regression",
    "title": "6  Logistic Regression",
    "section": "6.3 Multiple Logistic Regression",
    "text": "6.3 Multiple Logistic Regression\n\n6.3.1 Logistic Regression Models with Multiple Explanatory Variables\nWe can also perform logistic regression in situations where there are multiple explanatory variables. We’ll estimate probability of default, using both balance and whether or not the person is a student (a categorical variable) as explanatory variables.\n\nCCDefault_M2 &lt;- glm(data=Default, default ~ balance + student, family = binomial(link = \"logit\"))\nsummary(CCDefault_M2)\n\n\nCall:\nglm(formula = default ~ balance + student, family = binomial(link = \"logit\"), \n    data = Default)\n\nCoefficients:\n               Estimate  Std. Error z value             Pr(&gt;|z|)    \n(Intercept) -10.7494959   0.3691914 -29.116 &lt; 0.0000000000000002 ***\nbalance       0.0057381   0.0002318  24.750 &lt; 0.0000000000000002 ***\nstudentYes   -0.7148776   0.1475190  -4.846           0.00000126 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.7  on 9997  degrees of freedom\nAIC: 1577.7\n\nNumber of Fisher Scoring iterations: 8\n\n\nThe following graph gives an illustration of the model.\n\nggplot(data=Default, aes(y=default, x= balance, color=student)) + geom_point(alpha=0.2) + stat_smooth(method=\"glm\", se=FALSE, method.args = list(family=binomial)) \n\n\n\n\n\n\n\n\nThe regression equation is:\n\\[\nP(\\text{Default}) = \\hat{\\pi}\\_i = \\frac{e^{-10.75+0.005738\\times\\text{balance}-0.7149\\times\\text{student}_i}}{1+e^{-10.75+0.005738\\times\\text{balance}-0.7149\\times\\text{student}_i}}\n\\]\n\nFor each 1 dollar increase in balance, the odds of default are estimated to multiply by a factor \\(e^{0.005738}\\approx 1.00575\\), assuming whether or not the person is a student is held constant. Thus, the estimated odds of default increase by about 0.5%, for each 1-dollar increase in balance..\nFor every $100 increase in balance, the odds of default are estimated to multiply by \\(e^{0.005738\\times100}\\approx 1.775\\), assuming whether or not the person is a student is held constant. Thus, the estimated odds of default increase by about 77.5%.\nThe odds of default for students are estimated to be \\(e^{-0.7149} \\approx 0.49\\) as high for students as non-students, assuming balance amount is held constant.\n\nHypothesis Tests in Multiple Logistic Regression Model\n\nSince the p-value associated with balance is very small, there is strong evidence of a relationship between balance and odds of default, after accounting for whether or not the person is a student.\nSince the p-value associated with StudentYes is very small, there is strong evidence that students are less likely to default than nonstudents, provided the balance on the card is the same.\n\n\n\n6.3.2 Multiple Logistic Regression Model with Interaction\nThe previous model assumes the effect of balance on default probability is the same for students as for nonstudents. If we suspect that the effect of having a larger balance might be different for students than for nonstudents, then we could use a model with interaction between the balance and student variables.\n\nCCDefault_M_Int &lt;- glm(data=Default, default ~ balance * student, family = binomial(link = \"logit\"))\nsummary(CCDefault_M_Int)\n\n\nCall:\nglm(formula = default ~ balance * student, family = binomial(link = \"logit\"), \n    data = Default)\n\nCoefficients:\n                      Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        -10.8746818   0.4639679 -23.438 &lt;0.0000000000000002 ***\nbalance              0.0058188   0.0002937  19.812 &lt;0.0000000000000002 ***\nstudentYes          -0.3512310   0.8037333  -0.437               0.662    \nbalance:studentYes  -0.0002196   0.0004781  -0.459               0.646    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.5  on 9996  degrees of freedom\nAIC: 1579.5\n\nNumber of Fisher Scoring iterations: 8\n\n\nInterpretations for Logistic Model with Interaction\n\nThe regression equation is:\n\n\\[P(\\text{Default}) = \\hat{\\pi}\\_i = \\frac{e^{-10.87+0.0058\\times\\text{balance}-0.35\\times\\text{I}_{\\text{student}}-0.0002\\times\\text{balance}\\times{\\text{I}_{\\text{student}}}}}{1+e^{-10.87+0.0058\\times\\text{balance}-0.35\\times\\text{I}_{\\text{student}}-0.0002\\times\\text{balance}\\times{\\text{I}_{\\text{student}}}}}\n\\]\nEquation for Students\n\\[P(\\text{Default}) = \\hat{\\pi}\\_i = \\frac{e^{-10.52+0.0056\\times\\text{balance}}}{1+e^{-10.52+0.0056\\times\\text{balance}}}\n\\]\nAssuming a person is a student, for every $100 increase in balance, the odds of default are expected to multiply by a factor of \\(e^{0.0056\\times 100}=1.75\\), a 75% increase.\nEquation for Non-Students\n\\[\nP(\\text{Default}) = \\hat{\\pi}\\_i = \\frac{e^{-10.87+0.0058\\times\\text{balance}}}{1+e^{-10.87+0.0058\\times\\text{balance}}}\n\\]\nAssuming a person is a student, for every $100 increase in balance, the odds of default are expected to multiply by a factor of \\(e^{0.0058\\times 100}=1.786\\), a 78.6% increase.\n\nSince estimate of the interaction effect is so small and the p-value on this estimate is large, it is plausible that there is no interaction at all. Thus, the simpler non-interaction model is preferable.\n\n\n\n6.3.3 Logistic Regression Key Points\n\n\\(Y\\) is a binary response variable.\n\\(\\pi_i\\) is a function of explanatory variables \\(x_{i1}, \\ldots x_{ip}\\).\n\\(E(Y_i) = \\pi_i = \\frac{e^{\\beta_0+\\beta_1x_i + \\ldots\\beta_px_{ip}}}{1+e^{\\beta_0+\\beta_1x_i + \\ldots\\beta_px_{ip}}}\\)\n\\(\\beta_0+\\beta_1x_i + \\ldots\\beta_px_{ip} = \\text{log}\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)\\)\nFor quantitative \\(x_j\\), when all other explanatory variables are held constant, the odds of “success” multiply be a factor of \\(e^{\\beta_j}\\) for each 1 unit increase in \\(x_j\\)\nFor categorical \\(x_j\\), when all other explanatory variables are held constant, the odds of “success” are \\(e^{\\beta_j}\\) times higher for category \\(j\\) than for the “baseline category.”\nFor models with interaction, we can only interpret \\(\\beta_j\\) when the values of all other explanatory variables are given (since the effect of \\(x_j\\) depends on the other variables.)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Ch7.html",
    "href": "Ch7.html",
    "title": "7  Predictive Modeling",
    "section": "",
    "text": "7.1 Modeling for Prediction",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#modeling-for-prediction",
    "href": "Ch7.html#modeling-for-prediction",
    "title": "7  Predictive Modeling",
    "section": "",
    "text": "7.1.1 Overview\nWe’ve previously learned how to build models for the purpose of interpretation, when our primary focus is on understanding relationships between variables in the model. In this chapter, we’ll examine how to build models for situations when we are not interested in understanding relationships between variables, and instead care only about making the most accurate predictions possible.\nWe’ve seen that when we model for interpretation, we encounter a tradeoff between model complexity and interpretability. We wanted to choose a model that is complex enough to reasonably approximate the structure of the underlying data, but at the same time, not so complicated that it becomes hard to interpret. When modeling for prediction, we don’t need to worry about interpretability, which can sometimes make more complex models more desirable. Nevertheless, we’ll encounter a different kind of tradeoff, involving model complexity, that we’ll have to think about, and we’ll see that more complex models do not always lead to better predictions.\nPredictive Modeling Vocabulary\n\nThe new data on which we make predictions is called test data.\nThe data used to fit the model is called training data.\n\nIn the training data, we know the values of the explanatory and response variables. In the test data, we know only the values of the explanatory variables and want to predict the values of the response variable.\n\n\n7.1.2 Illustration of Predictive Modeling\nThe illustration shows observations from a simulated dataset consisting of 100 observations of a single explanatory variable \\(x\\), and response variable \\(y\\). We want to find a model that captures the trend in the data and will be best able to predict new values of y, for given x.\n\n\n\n\n\n\n\n\n\nWe’ll fit several different polynomial models to the data, increasing in complexity from the most simple model we could possibly use, a constant model, to a very complex eighth degree polynomial model.\nConstant Model to Sample Data\n\n\n\n\n\n\n\n\n\nLinear Model to Sample Data\n\n\n\n\n\n\n\n\n\nQuadratic Model\n\n\n\n\n\n\n\n\n\nDegree 3, 4, and 8 Models\nWe continue exploring higher order polynomial models. The blue curve represents a third degree (cubic) polynomial model, while the red curve represents a fourth degree (quartic) model and the green represents an eighth degree model.\n\n\n\n\n\n\n\n\n\nWe see that the flexibility of the model increases as we add higher-order terms. The curve is allowed to have more twists and bends. For higher-order, more complex models, individual points have more influence on the shape of the curve. This can be both a good and bad thing, as it allows the model to better bend and fit the data, but also makes it susceptible to the influence of outliers.\n\n\n7.1.3 Predicting New Data\nNow, suppose we have a new dataset of 100, x-values, and want to predict \\(y\\). The first 5 rows of the new dataset are shown\n\n\n\n\n\nx\nPrediction\n\n\n\n\n3.196237\n?\n\n\n1.475586\n?\n\n\n5.278882\n?\n\n\n5.529299\n?\n\n\n7.626731\n?\n\n\n\n\n\nWe fit polynomial models of degree 0 through 8 to the data. Note that although we did not show the 5th through 7th degree models in our illustrations, we’ll still fit these to the data.\n\nSim_M0 &lt;-lm(data=Sampdf, y~1)\nSim_M1 &lt;-lm(data=Sampdf, y~x)\nSim_M2 &lt;- lm(data=Sampdf, y~x+I(x^2))\nSim_M3 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3))\nSim_M4 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4))\nSim_M5 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5))\nSim_M6 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6))\nSim_M7 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7))\nSim_M8 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8))\n\nWe predict the values of the new observations, using each of the 9 models.\n\nNewdf$Deg0Pred &lt;- predict(Sim_M0, newdata=Newdf)\nNewdf$Deg1Pred &lt;- predict(Sim_M1, newdata=Newdf)\nNewdf$Deg2Pred &lt;- predict(Sim_M2, newdata=Newdf)\nNewdf$Deg3Pred &lt;- predict(Sim_M3, newdata=Newdf)\nNewdf$Deg4Pred &lt;- predict(Sim_M4, newdata=Newdf)\nNewdf$Deg5Pred &lt;- predict(Sim_M5, newdata=Newdf)\nNewdf$Deg6Pred &lt;- predict(Sim_M6, newdata=Newdf)\nNewdf$Deg7Pred &lt;- predict(Sim_M7, newdata=Newdf)\nNewdf$Deg8Pred &lt;- predict(Sim_M8, newdata=Newdf)\n\nIn fact, since these data were simulated, we know the true value of \\(y\\), so we can compare the predicted values to the true ones.\n\nkable(Newdf %&gt;% dplyr::select(-c(samp)) %&gt;% round(2) %&gt;% head(5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\ny\nDeg0Pred\nDeg1Pred\nDeg2Pred\nDeg3Pred\nDeg4Pred\nDeg5Pred\nDeg6Pred\nDeg7Pred\nDeg8Pred\n\n\n\n\n108\n5.53\n5.49\n1.17\n1.05\n0.41\n-0.14\n-0.30\n0.10\n0.40\n0.25\n0.31\n\n\n4371\n2.05\n3.92\n1.17\n1.89\n2.02\n3.66\n3.95\n4.26\n3.82\n3.37\n3.49\n\n\n4839\n3.16\n1.46\n1.17\n1.63\n1.29\n3.24\n3.35\n2.78\n2.24\n2.15\n2.07\n\n\n6907\n2.06\n6.79\n1.17\n1.89\n2.02\n3.66\n3.95\n4.25\n3.80\n3.36\n3.47\n\n\n7334\n2.92\n-1.03\n1.17\n1.68\n1.42\n3.43\n3.60\n3.14\n2.51\n2.31\n2.25\n\n\n\n\n\n\n\n7.1.4 Evaluating Predictions - RMSPE\nFor quantitative response variables, we can evaluate the predictions by calculating the average of the squared differences between the true and predicted values. Often, we look at the square root of this quantity. This is called the Root Mean Square Prediction Error (RMSPE).\n\\[\n\\text{RMSPE} = \\sqrt{\\displaystyle\\sum_{i=1}^{n'}\\frac{(y_i-\\hat{y}_i)^2}{n'}},\n\\]\nwhere \\(n'\\) represents the number of new cases being predicted.\nWe calcuate RMSPE for each of the 9 models.\n\nRMSPE0 &lt;- sqrt(mean((Newdf$y-Newdf$Deg0Pred)^2))\nRMSPE1 &lt;- sqrt(mean((Newdf$y-Newdf$Deg1Pred)^2))\nRMSPE2 &lt;- sqrt(mean((Newdf$y-Newdf$Deg2Pred)^2))\nRMSPE3 &lt;- sqrt(mean((Newdf$y-Newdf$Deg3Pred)^2))\nRMSPE4 &lt;- sqrt(mean((Newdf$y-Newdf$Deg4Pred)^2))\nRMSPE5 &lt;- sqrt(mean((Newdf$y-Newdf$Deg5Pred)^2))\nRMSPE6 &lt;- sqrt(mean((Newdf$y-Newdf$Deg6Pred)^2))\nRMSPE7 &lt;- sqrt(mean((Newdf$y-Newdf$Deg7Pred)^2))\nRMSPE8 &lt;- sqrt(mean((Newdf$y-Newdf$Deg8Pred)^2))\n\n\n\n\n\n\nDegree\nRMSPE\n\n\n\n\n0\n4.051309\n\n\n1\n3.849624\n\n\n2\n3.726767\n\n\n3\n3.256592\n\n\n4\n3.283513\n\n\n5\n3.341336\n\n\n6\n3.346908\n\n\n7\n3.370821\n\n\n8\n3.350198\n\n\n\n\n\nThe third degree model did the best at predicting the new data.\nNotice that making the model more complex beyond third degree not only didn’t help, but actually hurt prediction accuracy.\n\n\n7.1.5 Training Data Error\nNow, let’s examine the behavior if we had fit the models to the data, instead of the test data.\n\nRMSE0 &lt;- sqrt(mean(Sim_M0$residuals^2))\nRMSE1 &lt;- sqrt(mean(Sim_M1$residuals^2))\nRMSE2 &lt;- sqrt(mean(Sim_M2$residuals^2))\nRMSE3 &lt;- sqrt(mean(Sim_M3$residuals^2))\nRMSE4 &lt;- sqrt(mean(Sim_M4$residuals^2))\nRMSE5 &lt;- sqrt(mean(Sim_M5$residuals^2))\nRMSE6 &lt;- sqrt(mean(Sim_M6$residuals^2))\nRMSE7 &lt;- sqrt(mean(Sim_M7$residuals^2))\nRMSE8 &lt;- sqrt(mean(Sim_M8$residuals^2))\n\n\nDegree &lt;- 0:8\nTest &lt;- c(RMSPE0, RMSPE1, RMSPE2, RMSPE3, RMSPE4, RMSPE5, RMSPE6, RMSPE7, RMSPE8)\nTrain &lt;- c(RMSE0, RMSE1, RMSE2, RMSE3, RMSE4, RMSE5, RMSE6, RMSE7, RMSE8)\nRMSPEdf &lt;- data.frame(Degree, Test, Train)\nRMSPEdf\n\n  Degree     Test    Train\n1      0 4.051309 3.431842\n2      1 3.849624 3.366650\n3      2 3.726767 3.296821\n4      3 3.256592 2.913233\n5      4 3.283513 2.906845\n6      5 3.341336 2.838738\n7      6 3.346908 2.809928\n8      7 3.370821 2.800280\n9      8 3.350198 2.799066\n\n\nNotice that the most complex model achieves the best performance on the training data, but not on the test data.\nAs the model complexity grows, the model will always fit the training data better, but that does not mean it will perform better on new data. It is possible to start modeling noise, rather than true signal in the training data, which hurts the accuracy of the model when applied to new data.\n\n\n7.1.6 Graph of RMSPE\n\n\n\n\n\n\n\n\n\n\nTraining error decreases as model becomes more complex\n\nTesting error is lowest for the 3rd degree model, then starts to increase again\n\n\n\n7.1.7 Best Model\nOf the models we looked at, the third degree model does the best. The estimates of its coefficients are shown below.\n\nsummary(Sim_M3)\n\n\nCall:\nlm(formula = y ~ x + I(x^2) + I(x^3), data = Sampdf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9451 -1.7976  0.1685  1.3988  6.8064 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept) -0.54165    1.26803  -0.427   0.670221    \nx            4.16638    1.09405   3.808   0.000247 ***\nI(x^2)      -1.20601    0.25186  -4.788 0.00000610 ***\nI(x^3)       0.08419    0.01622   5.191 0.00000117 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.973 on 96 degrees of freedom\nMultiple R-squared:  0.2794,    Adjusted R-squared:  0.2569 \nF-statistic: 12.41 on 3 and 96 DF,  p-value: 0.0000006309\n\n\nIn fact, the data were generated from the model \\(y_i = 4.5x  - 1.4x^2 +  0.1x^3 + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,3)\\)\nWe compare the true expected response curve (in yellow) to the estimates from the various polynomial models.\n\n\n\n\n\n\n\n\n\nThe 8th degree model performs worse than the cubic. The extra terms cause the model to be “too flexible,” and it starts to model random fluctuations (noise) in the training data, that do not capture the true trend for the population. This is called overfitting.\n\n\n7.1.8 Model Complexity, Training Error, and Test Error",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#variance-bias-tradeoff",
    "href": "Ch7.html#variance-bias-tradeoff",
    "title": "7  Predictive Modeling",
    "section": "7.2 Variance-Bias Tradeoff",
    "text": "7.2 Variance-Bias Tradeoff\n\n7.2.1 What Contributes to Prediction Error?\nSuppose \\(Y_i = f(x_i) + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\).\nLet \\(\\hat{f}\\) represent the function of our explanatory variable(s) \\(x^*\\) used to predict the value of response variable \\(y^*\\). Thus \\(\\hat{y}^* = f(x^*)\\).\nThere are three factors that contribute to the expected value of \\(\\left(y^* - \\hat{y}\\right)^2 = \\left(y^* - \\hat{f}(x^*)\\right)^2\\).\n\nBias associated with fitting model: Model bias pertains to the difference between the true response function value \\(f(x^*)\\), and the average value of \\(\\hat{f}(x^*)\\) that would be obtained in the long run over many samples.\n\n\nfor example, if the true response function \\(f\\) is cubic, then using a constant, linear, or quadratic model would result in biased predictions for most values of \\(x^*\\).\n\nVariance associated with fitting model: Individual observations in the training data are subject to random sampling variability. The more flexible a model is, the more weight is put on each individual observation increasing the variance associated with the model.\nVariability associated with prediction: Even if we knew the true value \\(f(x^*)\\), which represents the expected value of \\(y^*\\) given \\(x=x^*\\), the actual value of \\(y^*\\) will vary due to random noise (i.e. the \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\) term).\n\n\n\n7.2.2 Variance and Bias\nThe third source of variability cannot be controlled or eliminated. The first two, however are things we can control. If we could figure out how to minimize bias while also minimizing variance associated with a prediction, that would be great! But…\nThe constant model suffers from high bias. Since it does not include a linear, quadratic, or cubic term, it cannot accurately approximate the true regression function.\nThe Eighth degree model suffers from high variance. Although it could, in theory, approximate the true regression function correctly, it is too flexible, and is thrown off because of the influence of individual points with high degrees of variability.\n\n\n\n\n\n\n\n\n\n\n\n7.2.3 Variance-Bias Tradeoff\nAs model complexity (flexibility) increases, bias decreases. Variance, however, increases.\n\n\n\n\n\n\n\n\n\nIn fact, it can be shown that:\n\\(\\text{Expected RMSPE} = \\text{Variance} + \\text{Bias}^2\\)\nOur goal is the find the “sweetspot” where expected RMSPE is minimized.\n\n\n7.2.4 Modeling for Prediction\n\nWhen our purpose is purely prediction, we don’t need to worry about keeping the model simple enough to interpret.\n\nGoal is to fit data well enough to make good predictions on new data without modeling random noise in the training (overfitting)\n\nA model that is too simple suffers from high bias\n\nA model that is too complex suffers from high variance and is prone to overfitting\n\nThe right balance is different for every dataset\n\nMeasuring error on data used to fit the model (training data) does not accurately predict how well model will be able to predict new data (test data)\n\n\n\n7.2.5 Cross-Validation\nWe’ve seen that training error is not an accurate approximation of test error. Instead, we’ll approximate test error, by setting aside a set of the training data, and using it as if it were a test set. This process is called cross-validation, and the set we put aside is called the validation set.\n\nPartition data into disjoint sets (folds). Approximately 5 folds recommended.\n\nBuild a model using 4 of the 5 folds.\n\nUse model to predict responses for remaining fold.\nCalculate root mean square error \\(RMSPE=\\displaystyle\\sqrt{\\frac{\\sum((\\hat{y}_i-y_i)^2)}{n'}}\\).\n\nRepeat for each of 5 folds.\n\nAverage RMSPE values across folds.\n\nIf computational resources permit, it is often beneficial to perform CV multiple times, using different sets of folds.\n\n\n7.2.6 Cross-Validation Illustration\n\n\n\n\n\nhttps://www.researchgate.net/figure/A-schematic-illustration-of-K-fold-cross-validation-for-K-5-Original-dataset-shown_fig5_311668395\n\n\n\n\n\n\n7.2.7 CV in R\nThe train function in the caret R package performs cross validation automatically. We’ll use it to compare five different models for house prices among a dataset of 1,000 houses sold in Ames, IA between 2006 and 2010.\nWe’ll consider six different models of (mostly) increasing complexity.\n\nlibrary(tidyverse)\nTrain_Data &lt;- read_csv(\"Ames_Train_Data.csv\")  # Load data\nlibrary(caret)   # load caret package\n\n\n# set cross-validation settings - use 10 repeats of 10-fold CV\ncontrol &lt;- trainControl(method=\"repeatedcv\", number=10, repeats=10, savePredictions = \"all\" )\n\n# define models\n# set same random seed before each model to ensure same partitions are used in CV, making them comparable\n\nset.seed(10302023)   \nmodel1 &lt;- train(data=Train_Data, \n                SalePrice ~ `Overall Qual` ,  \n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel2 &lt;- train(data=Train_Data, \n                SalePrice ~ `Overall Qual` +  `Gr Liv Area` + `Garage Area`,  \n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel3 &lt;- train(data=Train_Data, SalePrice ~ `Overall Qual` + \n                  `Gr Liv Area` + `Garage Area` + \n                  `Neighborhood` + `Bldg Type`,  \n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel4 &lt;- train(data=Train_Data, SalePrice ~ `Overall Qual` \n                + `Gr Liv Area`  + `Garage Area` \n                + `Neighborhood` + `Bldg Type` + `Year Built`,  \n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel5 &lt;- train(data=Train_Data, SalePrice ~ `Overall Qual` + \n                  `Gr Liv Area` + `Garage Area` + `Neighborhood` + \n                  `Bldg Type` + `Year Built` + I(`Overall Qual`^2) + \n                  I(`Gr Liv Area`^2) + I(`Garage Area`^2) + \n                  I(`Year Built`^2),  method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel6 &lt;- train(data=Train_Data, SalePrice ~ .,  method=\"lm\", trControl=control)  # include everything linearly\n\n\n# Calculate RMSPE for each model\nRMSPE1 &lt;- sqrt(mean((model1$pred$obs-model1$pred$pred)^2))\nRMSPE2 &lt;- sqrt(mean((model2$pred$obs-model2$pred$pred)^2))\nRMSPE3 &lt;- sqrt(mean((model3$pred$obs-model3$pred$pred)^2))\nRMSPE4 &lt;- sqrt(mean((model4$pred$obs-model4$pred$pred)^2))\nRMSPE5 &lt;- sqrt(mean((model5$pred$obs-model5$pred$pred)^2))\nRMSPE6 &lt;- sqrt(mean((model6$pred$obs-model6$pred$pred)^2))\n\n\nRMSPE1\n\n[1] 51710.58\n\nRMSPE2\n\n[1] 44192.34\n\nRMSPE3\n\n[1] 38212.96\n\nRMSPE4\n\n[1] 38016.67\n\nRMSPE5\n\n[1] 38245.46\n\nRMSPE6\n\n[1] 40586.14\n\n\nWe see that in this case, model M4 performed the best on the hold-out data. We should use Model 4 to make predictions on new data over the other models seen here. It is likely that there are better models out there than model 4, likely with complexity somewhere between that of model 4 and models 5 and 6. Perhaps you can find one.\nOnce we have our preferred model, we can read in our test data and make predictions, and display the first 10 predicted values.\n\nTestData &lt;- read_csv(\"Ames_Test_Data.csv\")\npredictions &lt;- predict(model4, newdata=TestData)  # substitute your best model\nhead(data.frame(predictions), 10)\n\n   predictions\n1    156767.46\n2    252017.22\n3    222084.65\n4    247418.75\n5    116156.38\n6    161242.97\n7    114106.22\n8     46470.89\n9    344009.51\n10   190402.70\n\n\nWe create a csv file containing the predictions, using the code below.\n\nwrite.csv(predictions, file = \"predictions.csv\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#ridge-regression",
    "href": "Ch7.html#ridge-regression",
    "title": "7  Predictive Modeling",
    "section": "7.3 Ridge Regression",
    "text": "7.3 Ridge Regression\n\n7.3.1 Complexity in Model Coefficients\nWe’ve thought about complexity in terms of the number of terms we include in a model, as well as whether we include quadratic terms and higher order terms and interactions. We can also think about model complexity in terms of the coefficients \\(b_1, \\ldots, b_p\\). Larger values of \\(b_1, \\ldots, b_p\\) are associated with more complex models. Smaller values of \\(b_1, \\ldots, b_p\\) are associated with less complex models. When \\(b_j=0\\), this mean variable \\(j\\) is not used in the model.\nTo illustrate, we fit a regression model to the Ames housing dataset, which includes 71 possible explanatory variables, in addition to price.\n\nset.seed(10302021)\nsamp &lt;- sample(1:nrow(ames_raw), 1000)\nTrain_Data &lt;- ames_raw[samp,]\n\nThe full list of coefficient estimates is shown below.\n\nM_OLS &lt;- lm(data=Train_Data, SalePrice ~ .)\nM_OLS$coefficients\n\n            (Intercept)          `Overall Qual`            `Year Built` \n      -14136063.8944780            6975.9249580             494.5819845 \n         `Mas Vnr Area`          `Central Air`Y           `Gr Liv Area` \n             33.3816934           -3745.3629147              37.2290416 \n         `Lot Frontage`            `1st Flr SF`         `Bedroom AbvGr` \n            -14.9594204              13.3690338           -2353.0379012 \n        `TotRms AbvGrd`                   Order                     PID \n            914.5594852               9.0730317               0.7938731 \n       `MS SubClass`030        `MS SubClass`040        `MS SubClass`045 \n           1747.3877587            5867.2885074            7182.7066393 \n       `MS SubClass`050        `MS SubClass`060        `MS SubClass`070 \n          -1330.0097462           -7400.8589386            -953.6999925 \n       `MS SubClass`075        `MS SubClass`080        `MS SubClass`085 \n         -10585.7402995           -6516.1314579           -6527.2337681 \n       `MS SubClass`090        `MS SubClass`120        `MS SubClass`160 \n         -21891.2260114          -20296.4699853          -34837.6663542 \n       `MS SubClass`180        `MS SubClass`190      `MS Zoning`C (all) \n         -18822.1874094          -12421.4283083          -38785.6865317 \n          `MS Zoning`FV      `MS Zoning`I (all)           `MS Zoning`RH \n         -23114.7182938          -17213.3331514          -13306.2219908 \n          `MS Zoning`RL           `MS Zoning`RM              `Lot Area` \n         -17643.5478889          -24783.4465053               0.7407590 \n             StreetPave               AlleyPave                 AlleyNA \n          37262.6393526            2527.1840221              29.0268338 \n         `Lot Shape`IR2          `Lot Shape`IR3          `Lot Shape`Reg \n           8575.5874307           10397.8001841            2676.2691253 \n      `Land Contour`HLS       `Land Contour`Low       `Land Contour`Lvl \n          11318.1617440          -22452.1871122           12230.7109977 \n    `Lot Config`CulDSac         `Lot Config`FR2         `Lot Config`FR3 \n          10427.6521788          -12317.5423512          -14557.2814204 \n     `Lot Config`Inside         `Land Slope`Mod         `Land Slope`Sev \n          -1168.2308323           10550.3766984          -24213.7593573 \n    NeighborhoodBlueste      NeighborhoodBrDale     NeighborhoodBrkSide \n          19001.1401867           20558.0541049           14314.5790198 \n    NeighborhoodClearCr     NeighborhoodCollgCr     NeighborhoodCrawfor \n           7379.3465594             -46.6307558           28775.4384686 \n    NeighborhoodEdwards     NeighborhoodGilbert      NeighborhoodGreens \n          -7491.2919144            1951.6205024            4667.7592928 \n     NeighborhoodIDOTRR     NeighborhoodMeadowV     NeighborhoodMitchel \n          12473.1492830           18891.1264802           -9274.3206260 \n      NeighborhoodNAmes     NeighborhoodNoRidge     NeighborhoodNPkVill \n           1781.9098984           32109.4882191           19601.4137971 \n    NeighborhoodNridgHt      NeighborhoodNWAmes     NeighborhoodOldTown \n          34407.2004203           -3367.8475918            9857.0794600 \n     NeighborhoodSawyer     NeighborhoodSawyerW     NeighborhoodSomerst \n           6761.6510470            3945.8508858           20453.1722509 \n    NeighborhoodStoneBr       NeighborhoodSWISU      NeighborhoodTimber \n          44556.0799048            8518.9310629            1455.9163924 \n    NeighborhoodVeenker      `Condition 1`Feedr       `Condition 1`Norm \n           5759.5517197            -315.5372848           10041.9749570 \n      `Condition 1`PosA       `Condition 1`PosN       `Condition 1`RRAe \n          49498.8746234           15873.9313786          -11599.3284625 \n      `Condition 1`RRAn       `Condition 1`RRNn      `Condition 2`Feedr \n           7894.0460905            6045.1253614            7639.7933280 \n      `Condition 2`Norm       `Condition 2`PosA       `Condition 2`PosN \n           6804.6578647            1496.1296933         -224695.9129765 \n      `Condition 2`RRNn          `Overall Cond`        `Year Remod/Add` \n          20826.2553462            5652.4500122             115.1029835 \n      `Roof Style`Gable     `Roof Style`Gambrel         `Roof Style`Hip \n           -983.4965913           -3775.4699187           -1020.9768277 \n    `Roof Style`Mansard        `Roof Style`Shed      `Roof Matl`CompShg \n          18711.8071355           -9552.0308918          671041.0670584 \n     `Roof Matl`Membran      `Roof Matl`Tar&Grv      `Roof Matl`WdShngl \n         738250.2085208          653237.7170668          757954.9462501 \n  `Mas Vnr Type`BrkFace      `Mas Vnr Type`None     `Mas Vnr Type`Stone \n         -14297.1433973           -5178.2753573          -11764.3736400 \n         `Exter Qual`Fa          `Exter Qual`Gd          `Exter Qual`TA \n         -21903.6214492          -37435.0319282          -40396.2768748 \n         `Exter Cond`Fa          `Exter Cond`Gd          `Exter Cond`Po \n          -2499.1726078            9625.5181637           -9090.3355705 \n         `Exter Cond`TA        FoundationCBlock         FoundationPConc \n          10976.5039644            2136.6596942            4341.3771422 \n         FoundationSlab         FoundationStone          FoundationWood \n          -8011.2100540            5209.4079083          -21499.8677282 \n          `Bsmt Qual`Fa           `Bsmt Qual`Gd           `Bsmt Qual`Po \n         -19118.1141774          -13338.1127108           58476.8071349 \n          `Bsmt Qual`TA           `Bsmt Qual`NA       `Bsmt Exposure`Gd \n         -14904.6825425           29237.6694620            8445.8558434 \n      `Bsmt Exposure`Mn       `Bsmt Exposure`No       `Bsmt Exposure`NA \n          -5636.1528491           -5726.8655933          -28286.6797423 \n         `BsmtFin SF 1`          `BsmtFin SF 2`           `Bsmt Unf SF` \n             33.8127701              24.9524651              13.2017599 \n            HeatingGasW             HeatingWall          `Heating QC`Fa \n           3554.5996405           16477.8270033           -7277.0668485 \n         `Heating QC`Gd          `Heating QC`Po          `Heating QC`TA \n           -760.5324990          -17982.8274261           -5710.2836368 \n        ElectricalFuseF         ElectricalFuseP           ElectricalMix \n            728.4351710           25345.9011297           51715.0815893 \n        ElectricalSBrkr            `2nd Flr SF`        `Bsmt Full Bath` \n          -2872.3538220              12.4886691            -498.4665953 \n       `Bsmt Half Bath`             `Full Bath`             `Half Bath` \n           3026.8853409            3781.8172133            3333.8342115 \n        `Kitchen AbvGr`        `Kitchen Qual`Fa        `Kitchen Qual`Gd \n         -12956.8475513           -7789.8981158          -13554.1557391 \n       `Kitchen Qual`TA          FunctionalMaj2          FunctionalMin1 \n         -13298.5375890          -24570.4189413           -8664.3164291 \n         FunctionalMin2           FunctionalMod           FunctionalTyp \n         -12709.0598893          -14394.7176699            4206.1036385 \n             Fireplaces        `Fireplace Qu`Fa        `Fireplace Qu`Gd \n          11467.2013154          -12766.1232883          -14204.2309228 \n       `Fireplace Qu`Po        `Fireplace Qu`TA        `Fireplace Qu`NA \n         -23140.9430388          -17684.2617792           -5555.0116338 \n    `Garage Type`Attchd    `Garage Type`Basment    `Garage Type`BuiltIn \n          -1098.5463523            -698.4291295           -4596.8780051 \n   `Garage Type`CarPort     `Garage Type`Detchd         `Garage Yr Blt` \n         -10478.5627774            -138.9775476             -61.6633788 \n     `Garage Finish`RFn      `Garage Finish`Unf           `Garage Cars` \n          -4343.2684170           -1482.5096317            2990.4177680 \n          `Garage Area`         `Garage Qual`Fa         `Garage Qual`Gd \n             24.3013657          -70420.9028312          -51114.4969380 \n        `Garage Qual`Po         `Garage Qual`TA         `Garage Cond`Fa \n        -134052.9764560          -67768.2607876           73986.1047213 \n        `Garage Cond`Gd         `Garage Cond`Po         `Garage Cond`TA \n          64104.4268776          112712.4021077           71259.2488661 \n         `Paved Drive`P          `Paved Drive`Y          `Wood Deck SF` \n           5257.0317668            2077.1863821               7.6414867 \n        `Open Porch SF`        `Enclosed Porch`            `3Ssn Porch` \n            -16.2882422             -11.3738346              16.1217441 \n         `Screen Porch`             `Pool Area`             `Pool QC`TA \n             38.2573038            -106.8682816           24551.0371793 \n            `Pool QC`NA               FenceGdWo              FenceMnPrv \n        -100661.2221692               3.7225602            1567.2382267 \n              FenceMnWw                 FenceNA      `Misc Feature`Gar2 \n           -880.4035636            1297.2148680          545161.5427966 \n     `Misc Feature`Othr      `Misc Feature`Shed        `Misc Feature`NA \n         569599.2024080          551469.2569906          547892.2454606 \n             `Misc Val`               `Mo Sold`               `Yr Sold` \n              1.6361319            -262.1972861            5937.2902795 \n         `Sale Type`Con        `Sale Type`ConLD        `Sale Type`ConLI \n           7985.9754307           23972.1096774            9525.1931028 \n       `Sale Type`ConLw          `Sale Type`CWD          `Sale Type`New \n          11056.3865076           -9551.0936247           23118.1311002 \n         `Sale Type`Oth          `Sale Type`VWD          `Sale Type`WD  \n          39733.0338449          -11729.6976216            4819.9397983 \n`Sale Condition`AdjLand  `Sale Condition`Alloca  `Sale Condition`Family \n          41276.9513339           24461.6278440             404.8499384 \n `Sale Condition`Normal `Sale Condition`Partial \n           1592.2625631           -4641.7581345 \n\n\nLet’s focus on the first 10 rows.\n\nhead(coef(M_OLS),10) %&gt;% round(3)\n\n    (Intercept)  `Overall Qual`    `Year Built`  `Mas Vnr Area`  `Central Air`Y \n  -14136063.894        6975.925         494.582          33.382       -3745.363 \n  `Gr Liv Area`  `Lot Frontage`    `1st Flr SF` `Bedroom AbvGr` `TotRms AbvGrd` \n         37.229         -14.959          13.369       -2353.038         914.559 \n\n\nIf all coefficients in the model were 0, then we would be using the most simple constant model, and the prediction for the price of each house would be exactly the same as the overall mean. As \\(b_j's\\) get farther from 0, predictions begin move away from the overall mean and depend more and more on the values or categories of the explanatory variable(s) associated with individual houses. This creates a risk, however, of overfitting.\nA way to combat this, other than dropping variables from the model, is to shrink some or all of the regression coefficients closer to 0, pushing predictions closer to the overall mean.\nA statistical technique for doing this is called ridge regression.\n\n\n7.3.2 Ridge Regression Penalty\nWe’ve seen that in ordinary least-squares regression, \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that to minimizes\n\\[\n\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}i)^2 =\\displaystyle\\sum_{i=1}^{n} (y_i -(b_0 + b_1x{i1} + b_2{x_i2} + \\ldots +b_px\\_{ip}))^2\n\\]\nWhen \\(p\\) is large and we want to be careful of overfitting, a common approach is to add a “penalty term” to this function, to incentive choosing values of \\(b_1, \\ldots, b_p\\) that are closer to 0, thereby “shrinking” the predictions toward the overall mean house price.\nSpecifically, we minimize:\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\n\\end{aligned}\n\\]\nwhere \\(\\lambda\\) is a pre-determined positive constant.\nLarger values of \\(b_j\\) typically help the model better fit the training data, thereby making the first term smaller, but also make the second term larger. The idea is the find optimal values of \\(b_0, b_1, \\ldots, b_p\\) that are large enough to allow the model to fit the data well, thus keeping the first term (SSR) small, while also keeping the penalty term small as well.\n\n\n7.3.3 Choosing \\(\\lambda\\)\nThe value of \\(\\lambda\\) is predetermined by the user. The larger the value of \\(\\lambda\\), the more heavily large \\(b_j's\\) are penalized. A value of \\(\\lambda=0\\) corresponds to ordinary least-squares.\n\\[\n\\begin{aligned}\nQ=& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\n\\end{aligned}\n\\]\n\nSmall values of \\(\\lambda\\) lead to more complex models, with larger \\(|b_j|\\)’s.\n\nAs \\(\\lambda\\) increases, \\(|b_j|\\)’s shrink toward 0. The model becomes less complex, thus bias increases, but variance decreases.\n\nWe can use cross validation to determine the optimal value of \\(\\lambda\\)\n\n\n\n\n\n\n\n\n\n\nWhen using ridge regression, it is important to standardize each explanatory variable (i.e. subtract the mean and divide by the standard deviation). This ensures each variable has mean 0 and standard deviation 1. Without standardizing the optimal choice of \\(b_j\\)’s would depend on scale, with variables with larger absolute measurements having more influence. We’ll standardize the response variable too. Though this is not strictly necessary, it doesn’t hurt. We can always transform back if necessary.\nStandardization is performed using the scale command in R.\n\nTrain_sc &lt;- Train_Data %&gt;% mutate_if(is.numeric, scale)\n\n\n\n7.3.4 Ridge Regression on Housing Dataset\nWe’ll use the caret package to perform cross validation in order to find the optimal value of \\(\\lambda\\). To use ridge regression, we specify method = \"glmnet\", and tuneGrid=expand.grid(alpha=0, lambda=l_vals). Note the alpha value can be changed to use other types of penalized regression sometimes used in predictive modeling, such as lasso or elastic net.\n\ncontrol = trainControl(\"repeatedcv\", number = 10, repeats=10)\nl_vals = 10^seq(-3, 3, length = 100)  # test values between 1/1000 and 1000\n\nset.seed(11162020)\nHousing_ridge &lt;- train(SalePrice ~ .,\n                       data = Train_sc, method = \"glmnet\", trControl=control , \n                      tuneGrid=expand.grid(alpha=0, lambda=l_vals))\n\nValue of \\(\\lambda\\) minimizing RMSPE:\n\nHousing_ridge$bestTune$lambda\n\n[1] 0.6135907\n\n\nWe examine RMSPE on the withheld data as a function of \\(\\lambda\\).\n\n\n\n\n\n\n\n\n\nUsing \\(\\lambda\\) = 0.6135907, obtain the following set of ridge regression coefficients. Notice how the ridge coefficients are typically closer to 0 than the ordinary least squares coefficients, indicating a less complex model.\n\nM_OLS_sc &lt;- lm(data=Train_sc, SalePrice ~ .)\nOLS_coef &lt;- M_OLS_sc$coefficients\nRidge_coef &lt;- coef(Housing_ridge$finalModel, Housing_ridge$bestTune$lambda)[,1]\ndf &lt;- data.frame(OLS_coef[2:10], Ridge_coef[2:10])\nnames(df) &lt;-c(\"OLS Coeff\", \"Ridge Coeff\")\ndf\n\n                   OLS Coeff Ridge Coeff\n`Overall Qual`   0.121728754  0.10435284\n`Year Built`     0.187102422  0.03451303\n`Mas Vnr Area`   0.080212607  0.06202880\n`Central Air`Y  -0.046191694  0.04289126\n`Gr Liv Area`    0.237623291  0.00000000\n`Lot Frontage`  -0.004290945  0.07967743\n`1st Flr SF`     0.069910650  0.01020597\n`Bedroom AbvGr` -0.022457937  0.07194208\n`TotRms AbvGrd`  0.017574153  0.01342224\n\n\nPredictions and residuals for the first six houses in the traning data, using ordinary least squares and ridge regression, are shown below.\n\nlibrary(glmnet)\nMAT &lt;- model.matrix(SalePrice~., data=Train_sc)\nridge_mod &lt;- glmnet(x=MAT, y=Train_sc$SalePrice, alpha = 0, lambda=Housing_ridge$bestTune$lambda )\n\n\ny &lt;- Train_sc$SalePrice\nPred_OLS &lt;- predict(M_OLS_sc)\nPred_Ridge &lt;- predict(ridge_mod, newx=MAT)\nOLS_Resid &lt;- y - Pred_OLS\nRidge_Resid &lt;- y - Pred_Ridge\nResdf &lt;- data.frame(y, Pred_OLS, Pred_Ridge, OLS_Resid, Ridge_Resid)\nnames(Resdf) &lt;- c(\"y\", \"OLS Pred\", \"Ridge Pred\", \"OLS Resid\", \"Ridge Resid\")\nkable(head(Resdf))\n\n\n\n\n\ny\nOLS Pred\nRidge Pred\nOLS Resid\nRidge Resid\n\n\n\n\n859\n-0.6210832\n-0.4637429\n-0.4651589\n-0.1573403\n-0.1559243\n\n\n1850\n0.6800520\n1.1897467\n1.0528536\n-0.5096947\n-0.3728016\n\n\n1301\n-0.4545873\n-0.4527781\n-0.4958630\n-0.0018092\n0.0412758\n\n\n981\n-0.6408161\n-0.6626212\n-0.7711186\n0.0218051\n0.1303025\n\n\n2694\n-0.7937457\n-0.8679455\n-0.7543093\n0.0741997\n-0.0394365\n\n\n2209\n-0.7906625\n-0.6955254\n-0.6449779\n-0.0951370\n-0.1456845\n\n\n\n\n\n\n\n7.3.5 Ridge vs OLS\nIn OLS, we choose \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that minimizes\n\\[\n\\displaystyle\\sum*{i=1}\\^n (y_i -\\hat{y}i)\\^2 =\\* \\displaystyle\\sum{i=1}\\^n (y_i -(b_0 + b_1x{i1} + b_2x\\_{i2} + \\ldots + b_px\\_{ip}))\\^2\n\\]\nOLS: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\)\n\nsum((y-Pred_OLS)^2)\n\n[1] 56.94383\n\n\nRidge: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\)\n\nsum((y-Pred_Ridge)^2)\n\n[1] 127.1331\n\n\nNot surprisingly the OLS model achieves smaller \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\). This has to be true, since the OLS coefficients are chosen to minimize this quantity.\nIn ridge regression, \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that minimizes\n\\[\n\\begin{aligned}\nQ=& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\n\\end{aligned}\n\\]\nOLS: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\)\n\nsum((y-Pred_OLS)^2) + 0.6136*sum(coef(M_OLS_sc)[-1]^2) \n\n[1] 373.1205\n\n\nRidge: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\)\n\nsum((y-Pred_Ridge)^2) + 0.6136*sum((Ridge_coef)[-1]^2)\n\n[1] 130.3375\n\n\nWe see that the ridge coefficients achieve a lower value of Q than the OLS ones.\n\n\n7.3.6 Lasso and Elastic Net\nTwo other techniques that are similar to ridge regression are lasso and elastic net. Both also aim to avoid overfitting by shrinking regression coefficients toward 0 in a manner similar to ridge regression.\nLasso regression is very similar to ridge regression. Coefficients \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that to minimizes\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^p|b_j|\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^p|b_j|\n\\end{aligned}\n\\] Regression with an elastic net uses both ridge and lasso penalty terms and determines the values of \\(b_0, b_1, \\ldots, b_p\\) by minimizing\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^p|b_j|\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda_1\\displaystyle\\sum_{j=1}^pb_j^2+ \\lambda_2\\displaystyle\\sum_{j=1}^p|b_j|\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#decision-trees",
    "href": "Ch7.html#decision-trees",
    "title": "7  Predictive Modeling",
    "section": "7.4 Decision Trees",
    "text": "7.4 Decision Trees\n\n7.4.1 Basics of Decision Trees\nA decision tree is a flexible alternative to a regression model. It is said to be nonparametric because it does not involve parameters like \\(\\beta_0, \\beta_1, \\ldots \\beta_p\\). A tree makes no assumption about the nature of the relationship between the response and explanatory variables, and instead allows us to learn this relationship from the data. A tree makes prediction by repeatedly grouping together like observations in the training data. We can make predictions for a new case, by tracing it through the tree, and averaging responses of training cases in the same terminal node.\nDecision Tree Example:\nWe fit a decision tree to the Ames Housing dataset, using the rpart function in a package by the same name.\n\nlibrary(rpart)\nlibrary(rpart.plot)\ntree &lt;- rpart(SalePrice~., data=Train_Data, cp=0.04)\nrpart.plot(tree, box.palette=\"RdBu\", shadow.col=\"gray\", nn=TRUE, cex=1, extra=1)\n\n\n\n\n\n\n\n\nWe see that the houses are first split based on whether or not their overall quality rating was less than 8. Each of the resulting nodes are then split again, using information from other explanatory variables. Each split partitions the data further, so that houses in the same node can be thought of as being similar to one another.\n\nThe predicted price of a House with overall quality 7, and was built in 1995 is $200,000.\nThe predicted price of a House overall quality 8 and 1,750 sq. ft. on the first floor is $370,000.\n\n\n\n7.4.2 Partitioning in A Decision Tree\nFor a quantitative response variable, data are split into two nodes so that responses in the same node are as similar as possible, while responses in the different nodes are as different as possible.\nLet L and R represent the left and right nodes from a possible split. Let \\(n_L\\) and \\(n_R\\) represent the number of observations in each node, and \\(\\bar{y}_L\\) and \\(\\bar{y}_R\\) represent the mean of the training data responses in each node.\nFor each possible split, involving an explanatory variable, we calculate:\n\\[\n\\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 + \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2\n\\]\nWe choose the split that minimizes this quantity.\nPartitioning Example\nConsider a dataset with two explanatory variables, \\(x_1\\) and \\(x_2\\), and a response variable \\(y\\), whose values are shown numerically in the graph.\n\n\n   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\nx1    8    2    8    1    8    6    2    5    1     8     4    10     9     8\nx2    5    3    1    1    4    3    8    1   10     8     6     5     0     2\ny   253   64  258   21  257  203  246  114  331   256   213   406   326   273\n   [,15]\nx1     6\nx2     1\ny    155\n\n\n\n\n\n\n\n\n\n\n\nThe goal is to split up the data, using information about \\(x_1\\) and \\(x_2\\) in a way that makes the \\(y\\) values grouped together as similar as possible.\n1. One Possible Split (\\(x_1 &lt; 5.5\\))\nWe could split the data into 2 groups depending on whether \\(x_1 &lt; 5.5\\).\n\n\n\n\n\n\n\n\n\nWe calcuate the mean y-value in each resulting node:\n\n\\(\\bar{y}_L = (331+246+213+21+64+114)/6 \\approx 164.84\\)\n\n\\(\\bar{y}_R = (203+155+256+253+257+273+258+326+406)/9 \\approx 265.22\\)\n\nTo measure measure the amount of deviation in the node, we calculate the sum of the squared difference between each individual value and the overall mean in each node.\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2  \\\\\n& =(331-164.83)^2+(246-164.33)^2 + \\ldots+(114-164.33)^2 \\\\\n& =69958.83\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\\\\n& =(203-265.22)^2+(155-265.22)^2 + \\ldots+(406-265.22)^2 \\\\\n& =39947.56\n\\end{aligned}\n\\]\nAdding together these two quantities, we obtain an overall measure of the squared deviations between observations in the same node.\n\n69958.83 + 39947.56 = 109906.4\n\n2.Second Possible Split (\\(x_1 &lt; 6.5\\))\nWe could alternatively split the data into 2 groups depending on whether \\(x_1 &lt; 6.5\\).\n\n\n\n\n\n\n\n\n\nUsing this split,\n\n\\(\\bar{y}_L = (331+246+213+21+64+114 + 203+155)/8 \\approx 168.375\\)\n\n\\(\\bar{y}_R = (256+253+257+273+258+326+406)/7 \\approx 289.857\\)\n\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2  \\\\\n& =(331-168.375)^2+(246-168.375)^2 + \\ldots+(203-168.375)^2 \\\\\n& =71411.88\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\\\\n& =(203-289.857)^2+(155-289.857)^2 + \\ldots+(406-289.857)^2 \\\\\n& =19678.86\n\\end{aligned}\n\\]\nThe total squared deviation is:\n\n71411.88 + 19678.86 = 91090.74\n\nThe split at \\(x1 &lt; 6.5\\) is better than \\(x_1&lt;5.5\\)\n3. Third Possible Split (\\(x_2 &lt; 5.5\\))\nWe could also split the data into 2 groups depending on whether \\(x_2 &lt; 5.5\\).\n\n\n\n\n\n\n\n\n\nUsing this split,\n\n\\(\\bar{y}_L = (331+246+213+256)/4 \\approx 261.5\\)\n\n\\(\\bar{y}_R = (21 + 64 + \\ldots + 406)/11 \\approx 211.82\\)\n\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2  \\\\\n& =(331-261.5)^2+(246-261.5)^2 + (213-261.5)^2+(256-261.5)^2 \\\\\n& =7453\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\\\\n& =(21-211.82)^2+(64-211.82)^2 + \\ldots+(406-211.82)^2 \\\\\n& =131493.6\n\\end{aligned}\n\\]\nThe sum of squared deviations is:\n\n7453 + 131493.6 = 138946.6\n\nComparison of Splits\n\nOf the three split’s we’ve calculated, \\(\\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 + \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2\\) is minimized using \\(x_1 &lt; 6.5\\).\nIn fact, if we calculate all possible splits over \\(x_1\\) and \\(x_2\\), \\(\\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 + \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2\\) is minimized by splitting on \\(x_1 &lt; 6.5\\)\n\nThus, we perform the first split in the tree, using \\(x_1 &lt; 6.5\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.3 Next Splits\nNext, we find the best splits on the resulting two nodes. It turns out that the left node is best split on \\(x_2 &lt; 4.5\\), and the right node is best split on \\(x_1 &lt; 8.5\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.4 Recursive Partitioning\nSplitting continues until nodes reach a certain predetermined minimal size, or until change improvement in model fit drops below a predetermined value\n\n\n\n\n\n\n\n\n\n\n\n7.4.5 Model Complexity in Trees\nThe more we partition data into smaller nodes, the more complex the model becomes. As we continue to partition, bias decreases, as cases are grouped with those that are more similar to themselves. On the other hand, variance increases, as there are fewer cases in each node to be averaged, putting more weight on each individual observation.\nSplitting into too small of nodes can lead to drastic overfitting. In the extreme case, if we split all the way to nodes of size 1, we would get RMSE of 0 on the training data, but should certainly not expect RMSPE of 0 on the test data.\nThe optimal depth of the tree, or minimal size for terminal nodes can be determined using cross-validation. The rpart package uses a complexity parameter cp, which determines how much a split must improve model fit in order to be made. Smaller values of cp are associated with more complex tree models, since they allow splits even when model fit only improves by a little.\n\n\n7.4.6 Cross-Validation on Housing Data\nWe’ll use caret to determine the optimal value of the cp parameter. We use method=\"rpart\" to grow decision trees.\n\ncp_vals = 10^seq(-8, 1, length = 100) # test values between 1/10^8 and 1\ncolnames(Train_sc) &lt;- make.names(colnames(Train_sc))\n\nset.seed(11162020)\nHousing_Tree &lt;- train(data=Train_sc, SalePrice ~ .,  method=\"rpart\", trControl=control, \n                     tuneGrid=expand.grid(cp=cp_vals))\n\nThe optimal value of cp is:\n\nHousing_Tree$bestTune\n\n             cp\n52 0.0004328761\n\n\nWe plot RMSPE on the holdout data as a function of cp.\n\ncp &lt;- Housing_Tree$results$cp\nRMSPE &lt;- Housing_Tree$results$RMSE\nggplot(data=data.frame(cp, RMSPE), aes(x=cp, y=RMSPE))+geom_line() + xlim(c(0,0.001)) + ylim(c(0.475,0.485))  + \n  ggtitle(\"Regression Tree Cross Validation Results\")\n\n\n\n\n\n\n\n\n\n\n7.4.7 Comparing OLS, Lasso, Ridge, and Tree\n\nset.seed(11162020)\nHousing_OLS &lt;- train(data=Train_sc, SalePrice ~ .,  method=\"lm\", trControl=control)\nset.seed(11162020)\nHousing_lasso &lt;- train(SalePrice ~., data = Train_sc, method = \"glmnet\", trControl=control, \n                      tuneGrid=expand.grid(alpha=1, lambda=l_vals))\n\nRMSPE on the standardized version of the response variable is displayed below for ordinary least squares, ridge regression, lasso regression, and a decision tree.\n\nmin(Housing_OLS $results$RMSE)\n\n[1] 0.5634392\n\nmin(Housing_ridge$results$RMSE)\n\n[1] 0.4570054\n\nmin(Housing_lasso$results$RMSE)\n\n[1] 0.4730672\n\nmin(Housing_Tree$results$RMSE)\n\n[1] 0.477414\n\n\nIn this situation, the tree outperforms OLS, but does not do as well as lasso or ridge. The best model will vary depending on the nature of the data. We can use cross-validation to determine which model is likely to perform best in prediction.\n\n\n7.4.8 Random Forest\nA popular extension of a decision tree is a random forest. A random forest consists of many (often ~10,000) trees. Predictions are made by averaging predictions from individual trees.\n\nIn order to ensure the trees are different from each other:\n\neach tree is grown from a different bootstrap sample of the training data.\n\nwhen deciding on a split, only a random subset of explanatory variables are considered.\n\n\nGrowing deep trees ensures low bias. In a random forest, averaging across many deep trees decreases variance, while maintaining low bias.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#regression-splines",
    "href": "Ch7.html#regression-splines",
    "title": "7  Predictive Modeling",
    "section": "7.5 Regression Splines",
    "text": "7.5 Regression Splines\n\n7.5.1 Regression Splines\nWe’ve seen that we can use polynomial regression to capture nonlinear trends in data.\n\nA regression spline is a piecewise function of polynomials.\n\nHere we’ll keep thing simple by focusing on a spline with a single explanatory variable. Splines can also be used for multivariate data.\nWe’ll examine the use of splines on the car price prediction dataset.\nWe divide the data into a set of 75 cars, which we’ll use to train the model, and 35 cars, on which we’ll make and evaluate predictions.\nThe 75 cars in the training set are shown below.\n\n\n\n\n\n\n\n\n\n\n\n7.5.2 Two Models with High Bias\n\n\n\n\n\n\n\n\n\nThe constant and linear models have high bias, as they are not complex enough to capture the apparent curvature in the relationship between price and acceleration time.\nA cubic model, on the other hand might better capture the trend.\n\n\n\n\n\n\n\n\n\n\n\n7.5.3 Cubic Splines\nIt’s possible that the behavior of the response variable might differ in different regions of the x-axis. A cubic spline allows us to fit different models in different regions of the x-axis.\n\n\n\n\n\n\n\n\n\nThe region boundaries are called knots\nCubic Spline with 5 Knots\n\n\n\n\n\n\n\n\n\nCubic Spline with 10 Knots\n\n\n\n\n\n\n\n\n\nCubic Spline with 20 Knots\n\n\n\n\n\n\n\n\n\nNotice that as the number of knots increases, the model becomes more and more complex. We would not expect the relationship between price and acceleration time to look like it does in these more complicated pictures. It is likely that as the number of knots gets big, the model overfits the training data.\n\n\n7.5.4 Predicting Test Data\nShown below is a plot of RMSPE when predictions are made on the new test data.\n\n\n\n\n\n\n\n\n\nWe see that RMSPE is minimized using the model with three knots.\n\n\n7.5.5 Implementation of Splines\nImportant Considerations:\n\nhow many knots\n\nwhere to place knots\n\ndegree of polynomial\n\nThe best choices for all of these will vary between datasets and can be assessed through cross-validation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#summary-and-comparision",
    "href": "Ch7.html#summary-and-comparision",
    "title": "7  Predictive Modeling",
    "section": "7.6 Summary and Comparision",
    "text": "7.6 Summary and Comparision\nIn the previous sections, we’ve applied various predictive modeling techniques to predict house prices in Ames, IA. In each section, we’ve focused on an individual predictive technique (OLS, ridge/lasso regression, trees, splines), but in practice, we often test out these techniques together to find which is likely to perform best on a set of data. Here, we’ll go through the steps to test out and evaluate these techniques on the Ames Housing dataset.\nThere are no new statistical ideas presented in this section, just a synthesis of the preceding material. We leave out splines, since we did not discuss using splines in a multivariate setting, but we compare OLS, ridge and decision trees.\nWe use a subset of variables for illustrative purposes.\n\nset.seed(10302021)\nsamp &lt;- sample(1:nrow(ames_raw), 1000)\nAmes_Houses &lt;- ames_raw[samp,]\n\n\nNew_Houses &lt;- ames_raw &lt;- ames_raw[-samp,]\nNew_Houses &lt;- New_Houses[1:5, ]\n\nWe’ll begin by doing some data preparation.\nWe standardize all explanatory variables in the training and new data. We do not standardize the response variable, price, so we can interpret predicted values more easily.\n\nHouses_Combined &lt;- rbind(Ames_Houses, New_Houses)\nHouses_sc &lt;- Houses_Combined %&gt;% mutate_if(is.numeric, scale)\nHouses_sc$SalePrice &lt;- as.numeric(Houses_Combined$SalePrice)\nHouses_sc_Train &lt;- Houses_sc[1:1000, ]\nHouses_sc_New &lt;- Houses_sc[1001:1005, ]\n\nThe Houses_sc_Train dataset contains standardized values for the 1000 houses in the training data. The first six rows are shown below.\n\nhead(Houses_sc_Train)\n\n     Overall.Qual  Year.Built Central.Air  Gr.Liv.Area X1st.Flr.SF\n2771  -0.07619084  0.79212207           Y -0.003620472  -0.9340706\n2909  -0.77229808  0.18658251           Y -0.392399656  -1.3891530\n2368  -0.07619084  0.01837707           Y -0.938684253  -1.6993209\n2604  -2.16451257 -1.89916487           Y -0.571836202  -1.1908489\n669   -0.07619084 -0.14982836           Y -0.918746859  -0.3111924\n1427   2.01213088  1.22945620           Y  0.528707949   1.5345608\n     Bedroom.AbvGr TotRms.AbvGrd    Lot.Area Lot.Shape Land.Contour\n2771     0.1632018    -0.2910546  0.28570669       IR1          Lvl\n2909     0.1632018    -0.9160758 -0.88703104       Reg          Lvl\n2368     0.1632018    -0.2910546 -0.98002927       Reg          Lvl\n2604     0.1632018    -0.2910546  0.04235132       Reg          Lvl\n669      0.1632018    -0.2910546  0.17314883       IR1          Lvl\n1427     0.1632018     0.9589877  0.20650820       Reg          Lvl\n     Overall.Cond Exter.Qual Heating.QC Paved.Drive SalePrice\n2771   -0.4680319         Gd         Ex           Y    187000\n2909    0.4279149         TA         TA           Y    104500\n2368    1.3238618         TA         Ex           Y    116000\n2604   -1.3639788         TA         TA           Y    105000\n669    -1.3639788         Fa         Gd           Y    163000\n1427   -0.4680319         Ex         Ex           Y    395039\n\n\nThe Houses_sc_New displays standardized values for the new houses that we’re trying to predict.\n\nhead(Houses_sc_New)\n\n  Overall.Qual Year.Built Central.Air Gr.Liv.Area X1st.Flr.SF Bedroom.AbvGr\n2  -0.77229808 -0.3516749           Y  -1.2058453  -0.6772922    -1.0608118\n4   0.61991640 -0.1161873           Y   1.2145543   2.4091326     0.1632018\n6  -0.07619084  0.8930453           Y   0.2057222  -0.6010214     0.1632018\n7   1.31602364  0.9939686           Y  -0.3246125   0.4464308    -1.0608118\n8   1.31602364  0.6911988           Y  -0.4402494   0.2989739    -1.0608118\n  TotRms.AbvGrd   Lot.Area Lot.Shape Land.Contour Overall.Cond Exter.Qual\n2    -0.9160758  0.1877886       Reg          Lvl    0.4279149         TA\n4     0.9589877  0.1323496       Reg          Lvl   -0.4680319         Gd\n6     0.3339665 -0.0094877       IR1          Lvl    0.4279149         TA\n7    -0.2910546 -0.6164362       Reg          Lvl   -0.4680319         Gd\n8    -0.9160758 -0.6062364       IR1          HLS   -0.4680319         Gd\n  Heating.QC Paved.Drive SalePrice\n2         TA           Y    105000\n4         Ex           Y    244000\n6         Ex           Y    195500\n7         Ex           Y    213500\n8         Ex           Y    191500\n\n\nSince the glmnet command requires training data to be entered as a matrix, we create versions of the datasets in matrix form.\n\nHouses_sc$SalePrice[is.na(Houses$SalePrice)] &lt;- 0 #can't take NA's when fitting model matrix, doesn't matter since only need x-coeffs\nHouses_sc_Combined_MAT &lt;- model.matrix(SalePrice~., data=rbind(Houses_sc))\nHouses_sc_Train_MAT &lt;- Houses_sc_Combined_MAT[1:1000, ]\nHouses_sc_New_MAT &lt;- Houses_sc_Combined_MAT[1001:1005, ]\n\n\n7.6.1 Modeling with OLS\nWe first fit an ordinary least squares regression model to the data.\n\nHousing_OLS &lt;- lm(data=Houses_sc_Train, SalePrice~ .)\ncoef(Housing_OLS)\n\n    (Intercept)    Overall.Qual      Year.Built    Central.AirY     Gr.Liv.Area \n    238908.0614      23368.4152      14036.2549      -4497.1153      29640.4606 \n    X1st.Flr.SF   Bedroom.AbvGr   TotRms.AbvGrd        Lot.Area    Lot.ShapeIR2 \n      8320.1472      -5011.0485       1554.7785       7566.9377       1570.1676 \n   Lot.ShapeIR3    Lot.ShapeReg Land.ContourHLS Land.ContourLow Land.ContourLvl \n     19082.7508      -4566.5111      44704.8906      22406.5959      19096.4163 \n   Overall.Cond    Exter.QualFa    Exter.QualGd    Exter.QualTA    Heating.QCFa \n      7965.6704     -68750.2773     -62800.5804     -74028.3841      -3972.4036 \n   Heating.QCGd    Heating.QCPo    Heating.QCTA    Paved.DriveP    Paved.DriveY \n     -4478.6876     -23000.4394      -5272.7136      -1901.9235        709.1532 \n\n\n\n\n7.6.2 Ridge Regression with Housing Data\nNow, we’ll use ridge regression to predict insurance costs.\nWe use cross validation to determine the optimal value of lamba. We perform 10 repeats of 10-fold cross-validation. We test 100 lambda-values ranging from \\(10^-5\\) to \\(10^5\\).\n\ncontrol = trainControl(\"repeatedcv\", number = 10, repeats=10)\nl_vals = 10^seq(-5, 5, length = 100)\n\nset.seed(2022)\nHousing_ridge &lt;- train( SalePrice ~ ., data = Houses_sc_Train, method = \"glmnet\", trControl=control , tuneGrid=expand.grid(alpha=0, lambda=l_vals))\n\n\nHousing_ridge$bestTune$lambda\n\n[1] 6135.907\n\n\nWe fit a model to the full training dataset using the optimal value of \\(lambda\\) .\n\nridge_mod &lt;- glmnet(x=Houses_sc_Train_MAT, y=Houses_sc_Train$SalePrice, alpha = 0, lambda=Housing_ridge$bestTune$lambda )\ncoef(ridge_mod)\n\n26 x 1 sparse Matrix of class \"dgCMatrix\"\n                         s0\n(Intercept)     206079.5442\n(Intercept)          .     \nOverall.Qual     24716.6502\nYear.Built       12909.8224\nCentral.AirY     -1693.4832\nGr.Liv.Area      24137.8185\nX1st.Flr.SF      10707.5001\nBedroom.AbvGr    -5034.2266\nTotRms.AbvGrd     5394.5170\nLot.Area          7086.7613\nLot.ShapeIR2      3322.3720\nLot.ShapeIR3     18987.3176\nLot.ShapeReg     -5345.7478\nLand.ContourHLS  41239.7682\nLand.ContourLow  15011.2269\nLand.ContourLvl  12784.0351\nOverall.Cond      6560.4987\nExter.QualFa    -29042.5581\nExter.QualGd    -24942.6445\nExter.QualTA    -35102.6069\nHeating.QCFa     -8118.6371\nHeating.QCGd     -6380.1279\nHeating.QCPo    -19693.6611\nHeating.QCTA     -7645.0855\nPaved.DriveP      -613.3798\nPaved.DriveY      1324.2704\n\n\nThe regression coefficients are displayed together with the OLS coefficients in a data.frame.\n\nRidge_coef &lt;- as.vector(ridge_mod$beta)[-1] #leave off intercept using [-1]\nOLS_coef &lt;- coef(Housing_OLS)[-1]\ndata.frame(OLS_coef, Ridge_coef)\n\n                   OLS_coef  Ridge_coef\nOverall.Qual     23368.4152  24716.6502\nYear.Built       14036.2549  12909.8224\nCentral.AirY     -4497.1153  -1693.4832\nGr.Liv.Area      29640.4606  24137.8185\nX1st.Flr.SF       8320.1472  10707.5001\nBedroom.AbvGr    -5011.0485  -5034.2266\nTotRms.AbvGrd     1554.7785   5394.5170\nLot.Area          7566.9377   7086.7613\nLot.ShapeIR2      1570.1676   3322.3720\nLot.ShapeIR3     19082.7508  18987.3176\nLot.ShapeReg     -4566.5111  -5345.7478\nLand.ContourHLS  44704.8906  41239.7682\nLand.ContourLow  22406.5959  15011.2269\nLand.ContourLvl  19096.4163  12784.0351\nOverall.Cond      7965.6704   6560.4987\nExter.QualFa    -68750.2773 -29042.5581\nExter.QualGd    -62800.5804 -24942.6445\nExter.QualTA    -74028.3841 -35102.6069\nHeating.QCFa     -3972.4036  -8118.6371\nHeating.QCGd     -4478.6876  -6380.1279\nHeating.QCPo    -23000.4394 -19693.6611\nHeating.QCTA     -5272.7136  -7645.0855\nPaved.DriveP     -1901.9235   -613.3798\nPaved.DriveY       709.1532   1324.2704\n\n\n\n\n7.6.3 Decision Tree\nNow, we’ll predict house prices using using a decision tree.\nFirst, we grow and display a small decision tree, by setting the cp parameter equal to 0.05.\n\ntree &lt;- rpart(SalePrice~., data=Houses_sc_Train, cp=0.05)\nrpart.plot(tree, box.palette=\"RdBu\", shadow.col=\"gray\", nn=TRUE, cex=1, extra=1)\n\n\n\n\n\n\n\n\nNow we use cross-validation to determine the optimal value of the cp parameter. We use 10 repeats of 10-fold cross-validation. We test 1000 cp-values ranging from \\(10^-5\\) to \\(10^5\\).\n\ncp_vals = 10^seq(-5, 5, length = 100)\ncolnames(Houses_sc_Train) &lt;- make.names(colnames(Houses_sc_Train))\n\nset.seed(2022)\nHousing_Tree &lt;- train(data=Houses_sc_Train, SalePrice ~ .,  method=\"rpart\", trControl=control,tuneGrid=expand.grid(cp=cp_vals))\nHousing_Tree$bestTune\n\n             cp\n4 0.00002009233\n\n\nWe grow a full tree using the optimal cp value.\n\nHousing_Best_Tree &lt;- rpart(SalePrice~., data=Houses_sc_Train, cp=Housing_Tree$bestTune)\n\n\n\n7.6.4 Comparing Performance\nWe use cross-validation to compare the performance of the linear model, ridge regression model, and decision tree.\n\nset.seed(2022)\nHousing_OLS &lt;- train(data=Houses_sc_Train, SalePrice ~ .,  method=\"lm\", trControl=control)\n\n\nmin(Housing_OLS$results$RMSE)\n\n[1] 35313.3\n\n\n\nmin(Housing_ridge$results$RMSE)\n\n[1] 35747.22\n\n\n\nmin(Housing_Tree$results$RMSE)\n\n[1] 33675.46\n\n\nThe tree predictions give slightly lower RMSPE.\n\n\n7.6.5 Predictions on New Data\nWe now predict the sale price of the five new houses using each technique.\nOrdinary Least-Squares model:\n\nOLS_pred &lt;- predict(Housing_OLS, newdata=Houses_sc_New)\nhead(OLS_pred)\n\n       2        4        6        7        8 \n114709.4 253695.8 195078.1 222117.6 242493.9 \n\n\nRidge regression model:\nWe use the Customers_sc_New_MAT dataset, since the glmnet package requires inputs in matrix form.\n\nridge_pred &lt;- predict(ridge_mod, newx=Houses_sc_New_MAT)\nhead(ridge_pred)\n\n        s0\n2 114950.4\n4 259359.8\n6 195288.0\n7 226841.6\n8 249064.8\n\n\nDecision tree:\n\ntree_pred &lt;- predict(Housing_Best_Tree, newdata=Houses_sc_New)\nhead(tree_pred)\n\n       2        4        6        7        8 \n132926.5 287045.1 183978.6 207079.4 207079.4",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#assessing-a-classifiers-performance",
    "href": "Ch7.html#assessing-a-classifiers-performance",
    "title": "7  Predictive Modeling",
    "section": "7.7 Assessing a Classifier’s Performance",
    "text": "7.7 Assessing a Classifier’s Performance\n\n7.7.1 Measuring Prediction Accuracy\nJust as we’ve done for models with quantitative variables, we’ll want to compare and assess the performance of models for predicting categorical responses. This might involve comparing llogistic regression models with different explanatory variables, or comparing a regression model to another technique such as a decision tree.\nJust as we did before, we’ll divide the data so that we can evaluate predictions on a subset of the data that was not used to fit the model.\nWe’ll divide the credit card dataset into a set of 9,000 observations, on which we’ll fit our models and assess predictions on the remaining 1,000.\n\nset.seed(08172022)\nsamp &lt;- sample(1:nrow(Default), 1000)\nDefault_Test &lt;- Default[samp, ]\nDefault_Train &lt;- Default[-samp, ]\n\nWe fit the model with interaction to the training data:\n\nLR_Default_M_Int &lt;- glm(data=Default_Train, default ~ balance * student, family = binomial(link = \"logit\"))\nsummary(LR_Default_M_Int)\n\n\nCall:\nglm(formula = default ~ balance * student, family = binomial(link = \"logit\"), \n    data = Default_Train)\n\nCoefficients:\n                      Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        -11.2714061   0.5188284 -21.725 &lt;0.0000000000000002 ***\nbalance              0.0060696   0.0003273  18.547 &lt;0.0000000000000002 ***\nstudentYes           0.0924588   0.8606304   0.107               0.914    \nbalance:studentYes  -0.0004749   0.0005142  -0.924               0.356    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2617.1  on 8999  degrees of freedom\nResidual deviance: 1385.5  on 8996  degrees of freedom\nAIC: 1393.5\n\nNumber of Fisher Scoring iterations: 8\n\n\nWe then use the model to estimate the probability of a person defaulting on their credit card payment.\nInformation about 10 different credit card users, as well as the logistic regression estimate of their probability of default are shown below. The table also shows whether or not the user really defaulted on their payment.\n\nLR_Prob &lt;- predict(LR_Default_M_Int, newdata=Default_Test, type=\"response\") %&gt;% round(2)\nActual_Default &lt;- factor(ifelse(Default_Test$default==1, \"Yes\", \"No\"))\nstudent &lt;- Default_Test$student\nbalance &lt;- Default_Test$balance\nLR_Res_df &lt;- data.frame(student, balance, LR_Prob, Actual_Default)\nkable(head(LR_Res_df, 50)%&gt;% arrange(desc(LR_Prob)) %&gt;% head(10))\n\n\n\n\n\nstudent\nbalance\nLR_Prob\nActual_Default\n\n\n\n\n2465\nYes\n2026.864\n0.54\nNo\n\n\n1228\nNo\n1682.201\n0.26\nNo\n\n\n6656\nNo\n1551.028\n0.14\nNo\n\n\n1185\nNo\n1541.813\n0.13\nNo\n\n\n9963\nYes\n1635.175\n0.12\nNo\n\n\n6635\nNo\n1434.128\n0.07\nYes\n\n\n9691\nNo\n1391.318\n0.06\nNo\n\n\n5921\nYes\n1513.542\n0.06\nNo\n\n\n9755\nNo\n1233.619\n0.02\nNo\n\n\n7569\nYes\n1294.286\n0.02\nNo\n\n\n\n\n\n\n\n7.7.2 Decision Tree Classifier\nFor comparison, let’s use a decision tree to predict whether a person will default.\nIn a binary classification problem, we can treat a default as \\(y=1\\) and non-default as \\(y=0\\), and grow the tree as we would in regression.\nThe mean response in a node \\(\\bar{Y}\\), which is equivalent to the proportion of people in the node who defaulted, can be interpreted as the probability of default.\nThe first few splits of the tree are shown.\n\nlibrary(rpart)\nlibrary(rpart.plot)\n# grow shorter tree for illustration\ntree &lt;- rpart(data=Default_Train, default~balance + student, cp=0.005)\nrpart.plot(tree, box.palette=\"RdBu\", shadow.col=\"gray\", nn=TRUE, cex=1, extra=1)\n\n\n\n\n\n\n\n\n\n# grow full tree\ntree &lt;- rpart(data=Default_Train, default~balance + student)\n\n\nTree_Prob &lt;- predict(tree, newdata = Default_Test) %&gt;% round(2)\n\nWe add the decision tree probabilities to the table seen previously.\n\nLR_Res_df &lt;- data.frame(student, balance, LR_Prob, Tree_Prob, Actual_Default)\nkable(head(LR_Res_df, 50)%&gt;% arrange(desc(LR_Prob)) %&gt;% head(10))\n\n\n\n\n\nstudent\nbalance\nLR_Prob\nTree_Prob\nActual_Default\n\n\n\n\n2465\nYes\n2026.864\n0.54\n0.77\nNo\n\n\n1228\nNo\n1682.201\n0.26\n0.16\nNo\n\n\n6656\nNo\n1551.028\n0.14\n0.16\nNo\n\n\n1185\nNo\n1541.813\n0.13\n0.16\nNo\n\n\n9963\nYes\n1635.175\n0.12\n0.16\nNo\n\n\n6635\nNo\n1434.128\n0.07\n0.01\nYes\n\n\n9691\nNo\n1391.318\n0.06\n0.01\nNo\n\n\n5921\nYes\n1513.542\n0.06\n0.16\nNo\n\n\n9755\nNo\n1233.619\n0.02\n0.01\nNo\n\n\n7569\nYes\n1294.286\n0.02\n0.01\nNo\n\n\n\n\n\nWe see that the tree estimates that the first person has a 0.77 probability of defaulting on the payment, compared to an estimate of 0.54, given by the logistic regression model. On the other hand, the tree estimates only a 0.16 probability of the second person defaulting, compared to 0.26 for the logistic regression model.\n\n\n7.7.3 Assessing Classifier Accuracy\nWe’ve seen \\(\\text{RMSPE} = \\sqrt{\\displaystyle\\sum_{i=1}^{n}{(\\hat{y}_i-y_i)^2}}\\) used as a measure of predictive accuracy in a regression problem.\nSince our outcome is not numeric, this is not a good measure of predictive accuracy in a classification problem. We’ll examine some alternatives we can use instead.\nClassification Accuracy\nOne simple approach is calculate the proportion of credit card users classified correctly. If a person has model estimates a predicted probability of default greater than 0.5, the person is predicted to default, while if the probability estimate is less than 0.5, the person is predicted to not default.\nThe table shows the prediction for each of the 10 users, using both logistic regression and the decision tree.\n\nLR_Pred &lt;- factor(ifelse(LR_Prob &gt; 0.5, \"Yes\", \"No\"))\nTree_Pred &lt;- factor(ifelse(Tree_Prob &gt; 0.5, \"Yes\", \"No\"))\nLR_Res_df &lt;- data.frame(student, balance, LR_Prob, Tree_Prob, LR_Pred,Tree_Pred, Actual_Default)\nkable(head(LR_Res_df, 50)%&gt;% arrange(desc(LR_Prob)) %&gt;% head(10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudent\nbalance\nLR_Prob\nTree_Prob\nLR_Pred\nTree_Pred\nActual_Default\n\n\n\n\n2465\nYes\n2026.864\n0.54\n0.77\nYes\nYes\nNo\n\n\n1228\nNo\n1682.201\n0.26\n0.16\nNo\nNo\nNo\n\n\n6656\nNo\n1551.028\n0.14\n0.16\nNo\nNo\nNo\n\n\n1185\nNo\n1541.813\n0.13\n0.16\nNo\nNo\nNo\n\n\n9963\nYes\n1635.175\n0.12\n0.16\nNo\nNo\nNo\n\n\n6635\nNo\n1434.128\n0.07\n0.01\nNo\nNo\nYes\n\n\n9691\nNo\n1391.318\n0.06\n0.01\nNo\nNo\nNo\n\n\n5921\nYes\n1513.542\n0.06\n0.16\nNo\nNo\nNo\n\n\n9755\nNo\n1233.619\n0.02\n0.01\nNo\nNo\nNo\n\n\n7569\nYes\n1294.286\n0.02\n0.01\nNo\nNo\nNo\n\n\n\n\n\nNotice that although the probabilities differ, the logistic regression model and classification tree give the same predictions for these ten cases. Both correctly predict 8 out of the 10 cases, but mistakenly predict the first person to default, when they didn’t, and mistakenly predict that the sixth person would not default when they did.\nWe’ll check the classification accuracy for the model and the tree.\n\nsum(LR_Pred == Actual_Default)/1000\n\n[1] 0.972\n\n\n\nsum(Tree_Pred == Actual_Default)/1000\n\n[1] 0.971\n\n\nWe see that the two techniques are each right approximately 97% of the time.\nThis may not really be as good as it sounds. Can you think of a very simple classification strategy that would achieve a similarly impressive predictive accuracy on these data?\n\n\n7.7.4 Confusion Matrix\nIn addition to assessing overall accuracy, it is sometimes helpful to assess how well models are able to predict outcomes in each class. For example, how accurately can a model detect people who do actually default on their payments?\nA confusion matrix is a two-by-two table displaying the number of cases predicted in each category as columns, and the number of cases actually in each category as rows\n\n\n\n\nActually Negative\nActually Positive\n\n\n\n\nPredicted Negative\n# True Negative\n# False Negative\n\n\nPredicted Positive\n# False Positive\n# True Positive\n\n\n\nThe confusionMatrix matrix command in R returns the confusion matrix for all 1,000 test cases.\nLet’s look at the confusion matrix for all 1,000 test cases. The data argument is the predicted outcome, and the reference argument is the true outcome. The positive argument is the category that we’ll classify as a positive.\nLogistic Regression Confusion Matrix\n\nconfusionMatrix(data=LR_Pred, reference=factor(Actual_Default) , positive=\"Yes\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  957  20\n       Yes   8  15\n                                          \n               Accuracy : 0.972           \n                 95% CI : (0.9598, 0.9813)\n    No Information Rate : 0.965           \n    P-Value [Acc &gt; NIR] : 0.12988         \n                                          \n                  Kappa : 0.5035          \n                                          \n Mcnemar's Test P-Value : 0.03764         \n                                          \n            Sensitivity : 0.4286          \n            Specificity : 0.9917          \n         Pos Pred Value : 0.6522          \n         Neg Pred Value : 0.9795          \n             Prevalence : 0.0350          \n         Detection Rate : 0.0150          \n   Detection Prevalence : 0.0230          \n      Balanced Accuracy : 0.7101          \n                                          \n       'Positive' Class : Yes             \n                                          \n\n\nOut of 965 people who did not default, the logistic regression model correctly predicted 957 of them.\nOut of 35 people that did default, the model correctly predicted 15 of them.\nTree Confusion Matrix\n\n# data is predicted class\n# reference is actual class\nconfusionMatrix( data = Tree_Pred , reference= Actual_Default, \"Yes\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  960  24\n       Yes   5  11\n                                          \n               Accuracy : 0.971           \n                 95% CI : (0.9586, 0.9805)\n    No Information Rate : 0.965           \n    P-Value [Acc &gt; NIR] : 0.1724819       \n                                          \n                  Kappa : 0.4186          \n                                          \n Mcnemar's Test P-Value : 0.0008302       \n                                          \n            Sensitivity : 0.3143          \n            Specificity : 0.9948          \n         Pos Pred Value : 0.6875          \n         Neg Pred Value : 0.9756          \n             Prevalence : 0.0350          \n         Detection Rate : 0.0110          \n   Detection Prevalence : 0.0160          \n      Balanced Accuracy : 0.6546          \n                                          \n       'Positive' Class : Yes             \n                                          \n\n\nOut of 965 people who did not default, the logistic regression model correctly predicted 960 of them.\nOut of 35 people that did default, the model correctly predicted 11 of them.\nNotice that the tree was less likely to predict a person to default in general, returning only 16 positive predictions, compared to 23 for the logistic regression model.\n\n\n7.7.5 Sensitivity and Specificity\nThe sensitivity of a classifier is the proportion of all positive cases that the model correctly identifies as positive. (i.e. probability model says “positive” given actually is positive.)\n\\[\n\\text{Sensitivity} = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Negative}} = \\frac{\\text{Correctly Predicted Positives}}{\\text{Total Number of Actual Positives}}\n\\]\nLR Sensitivity\n\\[\n\\frac{15}{15+20} \\approx 0.4286\n\\]\nTree Sensitivity\n\\[\n\\frac{11}{11+24} \\approx 0.3143\n\\]\nThe specificity of a classifier is the proportion of all negative cases that the model correctly identifies as negative (i.e probabiltiy model says “negative” given truly is negative.)\n\\[\\text{Specificity} = \\frac{\\text{True Negative}}{\\text{True Negative} + \\text{False Positive}}= \\frac{\\text{Correctly Predicted Negatives}}{\\text{Total Number of Actual Negatives}}\n\\]\nLR Specificity\n\\[\\frac{957}{957+8} \\approx 0.9917\\]\nTree Specificity\n\\[\\frac{960}{960+5} \\approx 0.9948 \\]\nIn a given situation, we should think about the cost of a false negative vs a false positive when determining whether to place more weight on sensitivity or specificity. For example, “is it worse to tell a patient they tested positive for a disease when they really don’t have it, or to not tell them they tested positive when they really do have it?”",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#receiver-operating-characteristic-curve",
    "href": "Ch7.html#receiver-operating-characteristic-curve",
    "title": "7  Predictive Modeling",
    "section": "7.8 Receiver Operating Characteristic Curve",
    "text": "7.8 Receiver Operating Characteristic Curve\n\n7.8.1 Separating +’s and -’s\nThe prediction accuracy, sensitivity, and specificity measures, seen in the previous section are based only on the predicted outcome, without considering the probability estimates themselves. These techniques treat a 0.49 estimated probability of default the same as a 0.01 estimated probability.\nWe would hope to see more defaults among people with high estimated default probabilities than low ones. To assess this, we can list the people in order from highest to lowest probability estimates and see where the true defaults lie.\nFor example, consider the following fictional probability estimates produced by two different classifiers (models) for eight credit card users:\nClassifier 1\n\n\n  Classifier1_Probability_Estimate True_Outcome\n1                             0.90          Yes\n2                             0.75          Yes\n3                             0.60           No\n4                             0.40          Yes\n5                             0.30           No\n6                             0.15           No\n7                             0.05           No\n8                             0.01           No\n\n\nClassifier 2\n\n\n  Classifier2_Probability_Estimate True_Outcome\n1                             0.80          Yes\n2                             0.70           No\n3                             0.55           No\n4                             0.40          Yes\n5                             0.35           No\n6                             0.15           No\n7                             0.10          Yes\n8                             0.02           No\n\n\nClassifier 1 is better able to separate the “Yes’s” from “No’s” as the three true “Yes’s” are among the four highest probabilities. Classifier 2 is less able to separate the true “Yes’s” from true “No’s.”\n\n\n7.8.2 ROC Curve\nA receiver operating characteristic (ROC) curve tells us how well a predictor is able to separate positive cases from negative cases.\nThe blog (Toward Data Science) [https://towardsdatascience.com/applications-of-different-parts-of-an-roc-curve-b534b1aafb68] writes\n“Receiver Operating Characteristic (ROC) curve is one of the most common graphical tools to diagnose the ability of a binary classifier, independent of the inherent classification algorithm. The ROC analysis has been used in many fields including medicine, radiology, biometrics, natural hazards forecasting, meteorology, model performance assessment, and other areas for many decades and is increasingly used in machine learning and data mining research [1]. If you are a Data Scientist, you might be using it on a daily basis.”\nThe ROC curve plots the true positive (or hit) rate against the false positive rate (false alarm) rate, as the cutoff for a positive classification varies.\n\n\n\n\n\n\n\n\n\nThe higher the curve, the better the predictor is able to separate positive cases from negative ones.\nPredictions made totally at random would be expected to yield a diagonal ROC curve.\n\n\n7.8.3 Constructing ROC Curve\n\nOrder the probabilities from highest to lowest.\n\nAssume only the case with the highest probability is predicted as a positive.\n\nCalculate the true positive rate (hit rate) \\[\\frac{\\text{\\# True Positives}}{\\text{\\# Actual Positives}}\\] and false positive (false alarm) \\[\\frac{\\text{\\# False Positives}}{\\text{\\# Actual Negatives}}\\]rate.\nPlot the point \\[\\left( \\frac{\\text{\\# False Positives}}{\\text{\\# Actual Negatives}}, \\frac{\\text{\\# True Positives}}{\\text{\\# Actual Positives}} \\right)\\] in the coordinate plane.\n\nNow assume the cases with the two highest probabilities are predicted as positives, and repeat steps 3-4.\n\nContinue, by classifiying one more case as positive in each step.\n\n\n\n7.8.4 Construct ROC Example\nLet’s practice constructing an ROC curve for a small set of probability estimates.\n\nprob &lt;- c(0.9, 0.8, 0.7, 0.65, 0.45, 0.3, 0.2, 0.15, 0.1, 0.05)\nActual &lt;- c(\"+\", \"-\", \"+\", \"+\", \"-\", \"-\", \"-\", \"-\", \"+\", \"-\")\nHit_Rate &lt;- c(\"1/4\", \"1/4\", \"2/4\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")\nFA_Rate &lt;- c(\"0/6\", \"1/6\", \"1/6\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")\nkable(data.frame(prob, Actual, Hit_Rate, FA_Rate))\n\n\n\n\nprob\nActual\nHit_Rate\nFA_Rate\n\n\n\n\n0.90\n+\n1/4\n0/6\n\n\n0.80\n-\n1/4\n1/6\n\n\n0.70\n+\n2/4\n1/6\n\n\n0.65\n+\n\n\n\n\n0.45\n-\n\n\n\n\n0.30\n-\n\n\n\n\n0.20\n-\n\n\n\n\n0.15\n-\n\n\n\n\n0.10\n+\n\n\n\n\n0.05\n-\n\n\n\n\n\n\n\nFinish filling in the table and sketch a graph of the resulting ROC curve.\nQuestion: If the probability estimate of 0.45 were instead 0.5 or 0.55, would this change the ROC curve? Why or why not?\n\n\n7.8.5 AUC\nThe area under the ROC curve, (AUC) provides a measure of the model’s predictive strength.\nWhile there is no standard for what constitutes a good\" AUC, higher is better, andAUC” is useful for comparing models.\nA model that can perfectly separate successes from failures will have an AUC of 1.\nA model that assigns probabilities at random is expected to have an AUC of 0.5.\n\n\n7.8.6 LR and Tree ROC Curves\n\nlibrary(pROC)\nlibrary(verification)\nroc.plot(x=Default_Test$default, pred = LR_Prob)\n\n\n\n\n\n\n\n\n\nauc(response=Default_Test$default, predictor = LR_Prob)\n\nArea under the curve: 0.8953\n\n\n\nroc.plot(x=Default_Test$default, pred = Tree_Prob)\n\n\n\n\n\n\n\n\n\nauc(response=Default_Test$default, predictor = Tree_Prob)\n\nArea under the curve: 0.8176\n\n\n\nRandProb &lt;- runif(1000, 0, 1)\n\n\nroc.plot(x=Default_Test$default, pred = RandProb)\n\n\n\n\n\n\n\n\n\nauc(response=Default_Test$default, predictor = RandProb)\n\nArea under the curve: 0.563\n\n\nEven though a model that assigns predictions randomly, with 97% predicted as negatives will have a high accuracy rate, it will yield a poor ROC curve indicating an inability to separate positive cases from negative ones.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#ethical-considerations-in-predictive-modeling",
    "href": "Ch7.html#ethical-considerations-in-predictive-modeling",
    "title": "7  Predictive Modeling",
    "section": "7.9 Ethical Considerations in Predictive Modeling",
    "text": "7.9 Ethical Considerations in Predictive Modeling\n\n7.9.1 Assumptions in Predictive Models\nLike any other statistical technique, predictive inference (sometimes done through machine learning algorithms) depends on the validity of assumptions.\n\nThe response variable observed in the data is actually the thing we want to predict\n\nTraining/Test data representative of population of interest\nPrediction accuracy is appropriate metric\n\nBelow are some examples of real uses of predictive inference in which some of these assumptions were violated, leading to inappropriate and unethical conclusions.\n\n\n7.9.2 Amazon Hiring Algorithm\nIn 2014, Amazon began working on an algorithm to predict whether a job applicant would be suitable for hire for software developer positions, based on characteristics of their job application.\nresponse variable: rating of candidate’s strength (1-5) explanatory variables: many variables based on information included on the resume (e.g. highest degree, major, GPA, college/university, prior job experiences, internships, frequency of certain words on resume, etc.)\nThe algorithm was trained using data from past applications, rated by humans, over the past 10 years. It could then be used to predict ratings of future job applicants.\nAccording to [Reuters])(https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G),\n“In effect, Amazon’s system taught itself that male candidates were preferable. It penalized resumes that included the word “women’s,” as in “women’s chess club captain.” And it downgraded graduates of two all-women’s colleges, according to people familiar with the matter.”\nWhile the algorithm was intended to predict candidate quality, the response variable on the training data actually reflected biases in past hiring decisions, leading the algorithm to do the same.\n\n\n7.9.3 Facial Recognition\nFacial recognition technology is used by law enforcement surveillance, airport passenger screening, and employment and housing decisions. It has, however, been banned for use by police in some cities, including San Francisco and Boston, due to concerns about inequity and privacy.\nResearch has shown that although certain facial recognition algorithms achieve over 90% accuracy overall, accuracy rate is lower among subjects who are female, Black, or 18-30 years old.\nThis is likely due, at least in part, to the algorithms being trained primarily on data an images of people who are not members of these groups.\nAlthough the algorithms might attain strong accuracy overall, it is inappropriate to evaluate them on this basis, without accounting for performance on subgroups in the population.\n\n\n7.9.4 Comments\nThe biases and assumptions noted above are not reasons to abandon predictive modeling, but rather flaws to be aware of and work to correct.\nPredictive algorithms, are only as good as the data on which they are trained and the societies in which they are developed, and will reflect inherent biases. Thus, they should be used cautiously and with with human judgment, just like any other statistical technique.\nBeware of statements like:\n“The data say this!”\n“The algorithm is objective.”\n“The numbers don’t lie.”\nAny data-driven analysis depends on assumptions, and sound judgment and awareness of context are required when assessing the validaty of conclusions drawn.\n\n\n7.9.5 Modeling for Prediction\n\nGoal is to make the most accurate predictions possible.\n\nNot concerned with understanding relationships between variables. Not worried model being to complicated to interpret, as long as it yields good predictions.\n\nAim for a model that best captures the signal in the data, without being thrown off by noise.\n\n\nLarge number of predictors is ok\n\nDon’t make model so complicated that it overfits the data.\n\n\nBe sure that model is predicting what you intend it to\n\nReflective of biases inherent in the data on which it was trained",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  }
]