[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 255 Notes",
    "section": "",
    "text": "Preface\nThese notes serve as the primary textual resource for Stat 255: Statistics for Data Science at Lawrence University.\nWhat is this course about?\nStat 255 provides an introduction to essential statistical tasks including modeling, inference, prediction, and computation. The course employs a modern approach, intended to equip students with skills needed for working with today’s complex data. Traditional concepts, like interval estimation and hypothesis testing, are introduced through the lens of multivariate models and simulation. Data computation in R plays a central role throughout the course.\nThe course’s overarching learning outcomes are:\n\nVisualize and wrangle data using statistical software R.\n\nBuild and assess multivariate models to predict future outcomes.\n\nUse statistics from samples to draw inferences about larger populations or processes.\n\nQuantify uncertainty associated with estimates and predictions.\n\nExplain the assumptions associated with statistical models, and evaluate whether these assumptions are reasonably satisfied in context.\n\nWrite reproducible analyses, using statistical software.\n\nMake ethical decisions based on data.\n\nMore specific learning tasks, related to these outcomes are provided in each chapter.\nWho is this course intended for?\nThis course is intended for students who are interested in learning statistical modeling and data computation skills that might prove useful in further courses, research, or career.\nStat 255 can serve as either:\n\na first course in statistics for students with a strong quantitative background, typically including calculus.\na second course in statistics, building on introductory topics taught in courses like Lawrence’s Stat 107: Principles of Statistics or AP Statistics.\n\nAt Lawrence, this course is required for the statistics track of the mathematics major, the economics and mathematics-economics majors, the business analytics track of the business and entrepreneurship major, and the statistics and data science minor. It also satisfies the statistics requirement for several other majors and minors.\nThe prerequisite for the course is either 1) a prior college-level course in statistics (i.e. STAT 107, BIOL 170 or 280, ANTH 207, AP Stats) OR 2) Calculus. (Math 140, AP Calculus, or equivalent).\nThe course does not assume any prior knowledge of statistics, but does move more rapidly than a typical introductory statistics course. Students engage rigorously in statistical thinking and computation, intended to equip them with essential skills for further study in statistics and data science.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Ch1.html",
    "href": "Ch1.html",
    "title": "1  Visualizing and Summarizing Data",
    "section": "",
    "text": "1.1 Getting Started in R\nWe’ll work with data on houses that sold in King County, WA, (home of Seattle) between 2014 and 2015.\nWe begin by loading the tidyverse package which can be used to create professional data graphics and summaries.\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch1.html#getting-started-in-r",
    "href": "Ch1.html#getting-started-in-r",
    "title": "1  Visualizing and Summarizing Data",
    "section": "",
    "text": "1.1.1 Previewing the Data\nhead()\nThe head() function displays the first 5 rows of the dataset.\n\nhead(Houses)\n\n# A tibble: 6 × 9\n     Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront\n  &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;     \n1     1 1225         4      4.5         5420   101930 average   No        \n2     2  885.        4      2.5         2830     5000 average   No        \n3     3  385.        4      1.75        1620     4980 good      No        \n4     4  253.        2      1.5         1070     9643 average   No        \n5     5  468.        2      1           1160     6000 good      No        \n6     6  310.        3      1           1430    19901 good      No        \n# ℹ 1 more variable: yr_built &lt;dbl&gt;\n\n\nThe rows of the dataset are called observations. In this case, the observations are the houses.\nThe columns of the dataset, which contain information about the houses, are called variables.\nglimpse\nThe glimpse() command shows the number of observations (rows), and the number of variables, (columns). We also see the name of each variable and its type. Variable types include\n\nCategorical variables, which take on groups or categories, rather than numeric values. In R, these might be coded as logical &lt;logi&gt;, character &lt;chr&gt;, factor &lt;fct&gt; and ordered factor &lt;ord&gt;.\nQuantitative variables, which take on meaningful numeric values. These include numeric &lt;num&gt;, integer &lt;int&gt;, and double &lt;dbl&gt;.\nDate and time variables take on values that are dates and times, and are denoted &lt;dttm&gt;\n\n\nglimpse(Houses)\n\nRows: 100\nColumns: 9\n$ Id          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ price       &lt;dbl&gt; 1225.00, 885.00, 385.00, 252.70, 468.00, 310.00, 550.00, 4…\n$ bedrooms    &lt;dbl&gt; 4, 4, 4, 2, 2, 3, 4, 4, 3, 3, 3, 4, 5, 3, 4, 4, 3, 4, 3, 3…\n$ bathrooms   &lt;dbl&gt; 4.50, 2.50, 1.75, 1.50, 1.00, 1.00, 1.00, 1.00, 1.00, 2.25…\n$ sqft_living &lt;dbl&gt; 5420, 2830, 1620, 1070, 1160, 1430, 1660, 1600, 960, 1660,…\n$ sqft_lot    &lt;dbl&gt; 101930, 5000, 4980, 9643, 6000, 19901, 34848, 4300, 6634, …\n$ condition   &lt;fct&gt; average, average, good, average, good, good, poor, good, a…\n$ waterfront  &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No…\n$ yr_built    &lt;dbl&gt; 2001, 1995, 1947, 1985, 1942, 1927, 1933, 1916, 1952, 1979…\n\n\nThere are 100 houses in the dataset, and 9 variables on each house.\nsummary\nsummary displays the mean, minimum, first quartile, median, third quartile, and maximum for each numeric variable, and the number of observations in each category, for categorical variables.\n\nsummary(Houses)\n\n       Id             price           bedrooms      bathrooms    \n Min.   :  1.00   Min.   : 180.0   Min.   :1.00   Min.   :0.750  \n 1st Qu.: 25.75   1st Qu.: 322.9   1st Qu.:3.00   1st Qu.:1.500  \n Median : 50.50   Median : 507.5   Median :3.00   Median :2.000  \n Mean   : 50.50   Mean   : 735.4   Mean   :3.39   Mean   :2.107  \n 3rd Qu.: 75.25   3rd Qu.: 733.8   3rd Qu.:4.00   3rd Qu.:2.500  \n Max.   :100.00   Max.   :5300.0   Max.   :6.00   Max.   :6.000  \n  sqft_living      sqft_lot          condition  waterfront    yr_built   \n Min.   : 440   Min.   :  1044   poor     : 1   No :85     Min.   :1900  \n 1st Qu.:1410   1st Qu.:  5090   fair     : 1   Yes:15     1st Qu.:1948  \n Median :2000   Median :  7852   average  :59              Median :1966  \n Mean   :2291   Mean   : 13205   good     :30              Mean   :1965  \n 3rd Qu.:2735   3rd Qu.: 12246   very_good: 9              3rd Qu.:1991  \n Max.   :8010   Max.   :101930                             Max.   :2014  \n\n\n\n\n1.1.2 Modifying the Data\nNext we’ll look at how to manipulate the data and create new variables.\n\nAdding a New Variable\nWe can use the mutate() function to create a new variable based on variables already in the dataset.\nLet’s add a variable giving the age of the house, as of 2015.\n\nHouses &lt;- Houses |&gt; mutate(age = 2015-yr_built)\nhead(Houses)\n\n# A tibble: 6 × 10\n     Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront\n  &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;     \n1     1 1225         4      4.5         5420   101930 average   No        \n2     2  885.        4      2.5         2830     5000 average   No        \n3     3  385.        4      1.75        1620     4980 good      No        \n4     4  253.        2      1.5         1070     9643 average   No        \n5     5  468.        2      1           1160     6000 good      No        \n6     6  310.        3      1           1430    19901 good      No        \n# ℹ 2 more variables: yr_built &lt;dbl&gt;, age &lt;dbl&gt;\n\n\n\n\nSelecting Columns\nIf the dataset contains a large number of variables, narrow down to the ones you are interested in working with. This can be done with the select() command. If there are not very many variables to begin with, or you are interested in all of them, then you may skip this step.\nLet’s create a smaller version of the dataset, with only the columns price, sqft_living, and waterfront. We’ll call this Houses_3var.\n\nHouses_3var &lt;- Houses |&gt; select(price, sqft_living, waterfront)\nhead(Houses_3var)\n\n# A tibble: 6 × 3\n  price sqft_living waterfront\n  &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n1 1225         5420 No        \n2  885.        2830 No        \n3  385.        1620 No        \n4  253.        1070 No        \n5  468.        1160 No        \n6  310.        1430 No        \n\n\n\n\nFiltering by Row\nThe filter() command narrows a dataset down to rows that meet specified conditions.\nWe’ll filter the data to include only houses built after 2000.\n\nNew_Houses &lt;- Houses |&gt; filter(yr_built&gt;=2000)\nhead(New_Houses)\n\n# A tibble: 6 × 10\n     Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront\n  &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;     \n1     1 1225         4      4.5         5420   101930 average   No        \n2    16 3075         4      5           4550    18641 average   Yes       \n3    23  862.        5      2.75        3595     5639 average   No        \n4    24  360.        4      2.5         2380     5000 average   No        \n5    25  625.        4      2.5         2570     5520 average   No        \n6    27  488.        3      2.5         3160    13603 average   No        \n# ℹ 2 more variables: yr_built &lt;dbl&gt;, age &lt;dbl&gt;\n\n\nNow, we’ll filter the data to include only houses on the waterfront.\n\nNew_Houses &lt;- Houses |&gt; filter(waterfront == \"Yes\")\nhead(New_Houses)\n\n# A tibble: 6 × 10\n     Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront\n  &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;     \n1    16 3075         4      5           4550    18641 average   Yes       \n2    19  995.        3      4.5         4380    47044 average   Yes       \n3    34  825.        2      1           1150    12775 good      Yes       \n4    40 2400.        4      2.5         3650     8354 average   Yes       \n5    42  290.        2      0.75         440     8313 good      Yes       \n6    46 5111.        5      5.25        8010    45517 average   Yes       \n# ℹ 2 more variables: yr_built &lt;dbl&gt;, age &lt;dbl&gt;",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch1.html#summary-statistics",
    "href": "Ch1.html#summary-statistics",
    "title": "1  Visualizing and Summarizing Data",
    "section": "1.2 Summary Statistics",
    "text": "1.2 Summary Statistics\n\n1.2.1 Measures of Center\nCommon ways to characterize the center of a distribution include mean, median, and mode.\nFor a set of \\(n\\) values \\(y_i, \\ldots, y_n\\):\n\nmean (\\(\\bar{y}\\)) represents the numerical average and is calculated by \\(\\bar{y} =\\frac{1}{n}\\displaystyle\\sum_{i=1}^n y_i\\).\nmedian represents the middle number when the values are arranged from least to greatest. If there are an even number of values in the dataset, then the median is given by the average of the middle two numbers.\n\nThe median of the upper half of the values is called the upper (or 3rd) quartile. This represents the 75th percentile in the distribution.\nThe median of the upper half of the values is called the lower (or 1st) quartile. This represents the 25th percentile in the distribution.\n\nmode is the most frequently occurring number in the data.\n\n\n\n1.2.2 Measures of Spread\nCommon ways of measuring the amount of spread, or variability, in a variable include:\n\nrange: the difference between the maximum and minimum values\ninterquartile range: the difference between the upper and lower quartiles (i.e. the range of the middle 50% of the values).\nstandard deviation (\\(s\\)): standard deviation is approximately the average deviation between an observation and the mean. It is calculated by\n\\(s =\\sqrt{\\displaystyle\\sum_{i=1}^n \\frac{(y_i-\\bar{y})^2}{n-1}}\\).\nThe square of the standard deviation, called the variance is denoted \\(s^2\\).\n\n\n\n1.2.3 Calcularing Summary Statistics in R\nLet’s calculate the mean, median, and standard deviation, in prices.\n\nHouses_Summary &lt;- Houses |&gt; summarize(Mean_Price = mean(price, na.rm=TRUE), \n                                          Median_Price = median(price, na.rm=TRUE), \n                                          StDev_Price = sd(price, na.rm = TRUE),\n                                          Number_of_Houses = n()) \nHouses_Summary\n\n# A tibble: 1 × 4\n  Mean_Price Median_Price StDev_Price Number_of_Houses\n       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;            &lt;int&gt;\n1       735.         507.        835.              100\n\n\nNotes:\n1. The n() command calculates the number of observations.\n2. The na.rm=TRUE command removes missing values, so that summary statistics can be calculated. It’s not needed here, since this dataset doesn’t include missing values, but if the dataset does include missing values, you will need to include this, in order to do the calculation.\nThe kable() function in the knitr() package creates tables with professional appearance.\n\nlibrary(knitr)\nkable(Houses_Summary)\n\n\n\n\nMean_Price\nMedian_Price\nStDev_Price\nNumber_of_Houses\n\n\n\n\n735.3525\n507.5\n835.1231\n100\n\n\n\n\n\n\n\n1.2.4 Grouped Summaries\ngroup_by()\nThe group_by() command allows us to calculate summary statistics, with the data broken down by by category.We’ll compare waterfront houses to non-waterfront houses.\n\nHouses_Grouped_Summary &lt;- Houses |&gt; group_by(waterfront) |&gt; \n                                      summarize(Mean_Price = mean(price, na.rm=TRUE),\n                                                Median_Price = median(price, na.rm=TRUE), \n                                                StDev_Price = sd(price, na.rm = TRUE),\n                                                Number_of_Houses = n()) \nkable(Houses_Grouped_Summary)\n\n\n\n\nwaterfront\nMean_Price\nMedian_Price\nStDev_Price\nNumber_of_Houses\n\n\n\n\nNo\n523.7595\n450\n295.7991\n85\n\n\nYes\n1934.3800\n1350\n1610.7959\n15\n\n\n\n\n\nNote: arrange(desc(Mean_Gross)) arranges the table in descending order of Mean_Gross. To arrange in ascending order, use arrange(Mean_Gross).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch1.html#data-visualization",
    "href": "Ch1.html#data-visualization",
    "title": "1  Visualizing and Summarizing Data",
    "section": "1.3 Data Visualization",
    "text": "1.3 Data Visualization\nNext, we’ll create graphics to help us visualize the distributions and relationships between variables. We’ll use the ggplot() function, which is part of the tidyverse package.\n\n1.3.1 Histogram\nHistograms are useful for displaying the distribution of a single quantitative variable. In a histogram, the x-axis breaks the variable into ranges of values, and the y-axis displays the number of observations with a value falling in that category (frequency).\nGeneral Template for Histogram\n\nggplot(data=DatasetName, aes(x=VariableName)) + \n  geom_histogram(fill=\"colorchoice\", color=\"colorchoice\") + \n  ggtitle(\"Plot Title\") +\n  xlab(\"x-axis label\") + \n  ylab(\"y-axis label\")\n\nHistogram of House Prices\n\nggplot(data=Houses, aes(x=price)) + \n  geom_histogram(fill=\"lightblue\", color=\"white\") + \n  ggtitle(\"Distribution of House Prices\") +\n  xlab(\"Price (in thousands)\") + \n  ylab(\"Frequency\")\n\n\n\n\n\n\n\n\nWe see that the distribution of house prices is right-skewed. Most houses cost less than $1,000,000, though there are a few houses that are much more expensive. The most common price range is around $400,000 to $500,000.\n\n\n1.3.2 Density Plot\nDensity plots show the distribution for a quantitative variable price. Scores can be compared across categories, like whether or not the house is on a waterfront.\nGeneral Template for Density Plot\n\nggplot(data=DatasetName, aes(x=QuantitativeVariable,\n                             color=CategoricalVariable, fill=CategoricalVariable)) + \n  geom_density(alpha=0.2) + \n  ggtitle(\"Plot Title\") +\n  xlab(\"Axis Label\") + \n  ylab(\"Frequency\") \n\nalpha, ranging from 0 to 1 dictates transparency.\nDensity Plot of House Prices\n\nggplot(data=Houses, aes(x=price, color=waterfront, fill=waterfront)) + \n  geom_density(alpha=0.2) + \n  ggtitle(\"Distribution of Prices\") +\n  xlab(\"House price (in thousands)\") + \n  ylab(\"Frequency\") \n\n\n\n\n\n\n\n\nWe see that on average, houses on the waterfront tend to be more expensive and have a greater price range than houses not on the waterfront.\n\n\n1.3.3 Boxplot\nBoxplots can be used to compare a quantitative variable with a categorical variable. The middle 50% of observations are contained in the “box”, with the upper and lower 25% of the observations in each tail.\nGeneral Template for Boxplot\n\nggplot(data=DatasetName, aes(x=CategoricalVariable, \n                             y=QuantitativeVariable)) + \n  geom_boxplot() + \n  ggtitle(\"Plot Title\") + \n  xlab(\"Variable Name\") + ylab(\"Variable Name\") \n\nYou can make the plot horizontal by adding + coordflip(). You can turn the axis text vertical by adding theme(axis.text.x = element_text(angle = 90)).\nBoxplot Comparing Price by Waterfront Status\n\nggplot(data=Houses, aes(x=waterfront, y=price)) + geom_boxplot() + \n  ggtitle(\"House Price by Waterfront Status\") + \n  xlab(\"Waterfront\") + ylab(\"Price (in thousands)\") + coord_flip()\n\n\n\n\n\n\n\n\nFor houses not on the waterfront, the median price is about $400,000, and the middle 50% of prices range from about $300,000 to $600,000.\nFor waterfront houses, the median price is about $1,500,000, and the middle 50% of prices range from about $900,000 to $1,900,000.\n\n\n1.3.4 Violin Plot\nViolin plots are an alternative to boxplots. The width of the violin tells us the density of observations in a given range.\nGeneral Template for Violin Plot\n\nggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable, \n                             fill=CategoricalVariable)) + \n  geom_violin() + \n  ggtitle(\"Plot Title\") + \n  xlab(\"Variable Name\") + ylab(\"Variable Name\") \n\nViolin Plot Comparing Prices by Waterfront\n\nggplot(data=Houses, aes(x=waterfront, y=price, fill=waterfront)) + \n  geom_violin() + \n  ggtitle(\"Price by Waterfront Status\") + \n  xlab(\"Waterfront\") + ylab(\"Price (in thousands)\") + \n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nAgain, we see that houses on the waterfront tend to be more expensive than those not on the waterfront, and have a wider range in prices.\n\n\n1.3.5 Scatterplot\nScatterplots are used to visualize the relationship between two quantitative variables.\nScatterplot Template\n\nggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable)) + \n  geom_point() +\n  ggtitle(\"Plot Title\") + \n  ylab(\"Axis Label\") + \n  xlab(\"Axis Label\")\n\nScatterplot Comparing Price and Square Feet of Living Space\n\nggplot(data=Houses, aes(x=sqft_living, y=price)) + \n  geom_point() +\n  ggtitle(\"Price and Living Space\") + \n  ylab(\"Price (in thousands)\") + \n  xlab(\"Living Space in sq. ft. \")\n\n\n\n\n\n\n\n\nWe see that there is an upward trend, indicating that houses with more living space tend to, on average, be higher priced than those with less living space. The relationship appears to be roughly linear, though there might be some curvature, as living space gets very large. There are some exceptions to this trend, most notably a house with more than 7,000 square feet, priced just over $1,000,000.\nWe can also add color, size, and shape to the scatterplot to display information about other variables.\nWe’ll use color to illustrate whether the house is on the waterfront, and size to represent the square footage of the entire lot (including the yard and the house).\n\nggplot(data=Houses, \n       aes(x=sqft_living, y=price, color=waterfront, size=sqft_lot)) + \n  geom_point() +\n  ggtitle(\"Price of King County Houses\") + \n  ylab(\"Price (in thousands)\") + \n  xlab(\"Living Space in sq. ft. \")\n\n\n\n\n\n\n\n\nWe notice that many of the largest and most expensive houses are on the waterfront.\n\n\n1.3.6 Bar Graph\nBar graphs can be used to visualize one or more categorical variables. A bar graph is similar to a histogram, in that the y-axis again displays frequency, but the x-axis displays categories, instead of ranges of values.\nBar Graph Template\n\nggplot(data=DatasetName, aes(x=CategoricalVariable)) + \n  geom_bar(fill=\"colorchoice\",color=\"colorchoice\")  + \n  ggtitle(\"Plot Title\") + \n  xlab(\"Variable Name\") + \n  ylab(\"Frequency\") \n\nBar Graph by Condition\n\nggplot(data=Houses, aes(x=condition)) + \n  geom_bar(fill=\"lightblue\",color=\"white\")  + \n  ggtitle(\"Number of Houses by Condition\") + \n  xlab(\"Condition\") + \n  ylab(\"Frequency\") +   \n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nWe see that the majority of houses are in average condition. Some are in good or very good condition, while very few are in poor or very poor condition.\n\n\n1.3.7 Stacked and Side-by-Side Bar Graphs\nStacked Bar Graph Template\n\nggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, \n                                         fill = CategoricalVariable2)) +\n    stat_count(position=\"fill\")  +\n  theme_bw() + ggtitle(\"Plot Title\") + \n  xlab(\"Variable 1\") + \n  ylab(\"Proportion of Variable 2\") +   \n  theme(axis.text.x = element_text(angle = 90)) \n\nStacked Bar Graph Example\nThe stat_count(position=\"fill\") command creates a stacked bar graph, comparing two categorical variables. Let’s explore whether waterfront status is related to condition.\n\nggplot(data = Houses, mapping = aes(x = waterfront, fill = condition)) +\n    stat_count(position=\"fill\")  +\n  theme_bw() + ggtitle(\"Condition by Waterfront Status\") + \n  xlab(\"Waterfront Status\") + \n  ylab(\"Condition\") +   \n  theme(axis.text.x = element_text(angle = 90)) \n\n\n\n\n\n\n\n\nWe see that a higher proportion of waterfront houses are in good or excellent condition than non-waterfront houses.\nSide-by-side Bar Graph Template\nWe can create a side-by-side bar graph, using position=dodge.\n\nggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, \n                                         fill = CategoricalVariable2)) +\n    geom_bar(position = \"dodge\") +\n  ggtitle(\"Plot Title\") + \n  xlab(\"Genre\") + \n  ylab(\"Frequency\") \n\nSide-by-side Bar Graph Example\n\nggplot(data = Houses, mapping = aes(x = waterfront, fill = condition)) +\n    geom_bar(position = \"dodge\") +\n  ggtitle(\"Condition by Waterfront Status\") + \n  xlab(\"Waterfront Status\") + \n  ylab(\"Condition\") +   \n  theme(axis.text.x = element_text(angle = 90)) \n\n\n\n\n\n\n\n\nIn this case, since there are so few waterfront houses, the graph is hard to read and not very useful.\nThe stacked bar graph is a better way to convey information in this instance, though you may find that for a different dataset, the side-by-side bar graph could be a better choice.\n\n\n1.3.8 Correlation Plot\nCorrelation plots can be used to visualize relationships between quantitative variables. Correlation is a number between -1 and 1, describing the strength of the linear relationship between two variables. Variables with strong positive correlations will have correlation close to +1, while variables with strong negative correlations will have correlations close to -1. Variables with little to no relationship will have correlation close to 0.\nThe cor() function calculates correlations between quantitative variables. We’ll use select_if to select only numeric variables. The `use=“complete.obs” command tells R to ignore observations with missing data.\n\ncor(select_if(Houses, is.numeric), use=\"complete.obs\") |&gt; round(2)\n\n               Id price bedrooms bathrooms sqft_living sqft_lot yr_built   age\nId           1.00  0.03    -0.06     -0.01       -0.03    -0.07    -0.02  0.02\nprice        0.03  1.00     0.40      0.67        0.81     0.42     0.17 -0.17\nbedrooms    -0.06  0.40     1.00      0.58        0.58     0.15     0.26 -0.26\nbathrooms   -0.01  0.67     0.58      1.00        0.85     0.45     0.50 -0.50\nsqft_living -0.03  0.81     0.58      0.85        1.00     0.54     0.36 -0.36\nsqft_lot    -0.07  0.42     0.15      0.45        0.54     1.00     0.14 -0.14\nyr_built    -0.02  0.17     0.26      0.50        0.36     0.14     1.00 -1.00\nage          0.02 -0.17    -0.26     -0.50       -0.36    -0.14    -1.00  1.00\n\n\nThe corrplot() function in the corrplot() package provides a visualization of the correlations. Larger, thicker circles indicate stronger correlations.\n\nlibrary(corrplot)\nCorr &lt;- cor(select_if(Houses, is.numeric), use=\"complete.obs\")\ncorrplot(Corr)\n\n\n\n\n\n\n\n\nWe see that price has a strong positive correlation with square feet of living space, and is also positively correlated with number of bedrooms and bathrooms. Living space, bedrooms, and bathrooms are all positively correlated, which makes sense, since we would expect bigger houses to have more bedrooms and bathrooms. Price does not show much correlation with the other variables. We notice that bathrooms is negatively correlated with age, which means older houses tend to have fewer bathrooms than newer ones. Not surprisingly, age is very strongly correlated with year built.\n\n\n1.3.9 Scatterplot Matrix\nA scatterplot matrix is a grid of plots. It can be created using the ggpairs() function in the GGally package.\nThe scatterplot matrix shows us:\n\nAlong the diagonal are density plots for quantitative variables, or bar graphs for categorical variables, showing the distribution of each variable.\n\nUnder the diagonal are plots showing the relationships between the variables in the corresponding row and column. Scatterplots are used when both variables are quantitative, bar graphs are used when both variables are categorical, and boxplots are used when one variable is categorical, and the other is quantitative.\n\nAbove the diagonal are correlations between quantitative variables.\n\nIncluding too many variables can make these hard to read, so it’s a good idea to use select to narrow down the number of variables.\n\nlibrary(GGally)\nggpairs(Houses |&gt; select(price, sqft_living, condition, age))\n\n\n\n\n\n\n\n\nThe scatterplot matrix is useful for helping us notice key trends in our data. However, the plot can hard to read as it is quite dense, especially when there are a large number of variables. These can help us look for trends from a distance, but we should then focus in on more specific plots.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch7.html",
    "href": "Ch7.html",
    "title": "7  Predictive Modeling",
    "section": "",
    "text": "7.1 Modeling for Prediction",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#modeling-for-prediction",
    "href": "Ch7.html#modeling-for-prediction",
    "title": "7  Predictive Modeling",
    "section": "",
    "text": "7.1.1 Overview\nWe’ve previously learned how to build models for the purpose of interpretation, when our primary focus is on understanding relationships between variables in the model. In this chapter, we’ll examine how to build models for situations when we are not interested in understanding relationships between variables, and instead care only about making the most accurate predictions possible.\nWe’ve seen that when we model for interpretation, we encounter a tradeoff between model complexity and interpretability. We wanted to choose a model that is complex enough to reasonably approximate the structure of the underlying data, but at the same time, not so complicated that it becomes hard to interpret. When modeling for prediction, we don’t need to worry about interpretability, which can sometimes make more complex models more desirable. Nevertheless, we’ll encounter a different kind of tradeoff, involving model complexity, that we’ll have to think about, and we’ll see that more complex models do not always lead to better predictions.\nPredictive Modeling Vocabulary\n\nThe new data on which we make predictions is called test data.\nThe data used to fit the model is called training data.\n\nIn the training data, we know the values of the explanatory and response variables. In the test data, we know only the values of the explanatory variables and want to predict the values of the response variable.\n\n\n7.1.2 Illustration of Predictive Modeling\nThe illustration shows observations from a simulated dataset consisting of 100 observations of a single explanatory variable \\(x\\), and response variable \\(y\\). We want to find a model that captures the trend in the data and will be best able to predict new values of y, for given x.\n\n\n\n\n\n\n\n\n\nWe’ll fit several different polynomial models to the data, increasing in complexity from the most simple model we could possibly use, a constant model, to a very complex eighth degree polynomial model.\nConstant Model to Sample Data\n\n\n\n\n\n\n\n\n\nLinear Model to Sample Data\n\n\n\n\n\n\n\n\n\nQuadratic Model\n\n\n\n\n\n\n\n\n\nDegree 3, 4, and 8 Models\nWe continue exploring higher order polynomial models. The blue curve represents a third degree (cubic) polynomial model, while the red curve represents a fourth degree (quartic) model and the green represents an eighth degree model.\n\n\n\n\n\n\n\n\n\nWe see that the flexibility of the model increases as we add higher-order terms. The curve is allowed to have more twists and bends. For higher-order, more complex models, individual points have more influence on the shape of the curve. This can be both a good and bad thing, as it allows the model to better bend and fit the data, but also makes it susceptible to the influence of outliers.\n\n\n7.1.3 Predicting New Data\nNow, suppose we have a new dataset of 100, x-values, and want to predict \\(y\\). The first 5 rows of the new dataset are shown\n\n\n\n\n\nx\nPrediction\n\n\n\n\n3.196237\n?\n\n\n1.475586\n?\n\n\n5.278882\n?\n\n\n5.529299\n?\n\n\n7.626731\n?\n\n\n\n\n\nWe fit polynomial models of degree 0 through 8 to the data. Note that although we did not show the 5th through 7th degree models in our illustrations, we’ll still fit these to the data.\n\nSim_M0 &lt;-lm(data=Sampdf, y~1)\nSim_M1 &lt;-lm(data=Sampdf, y~x)\nSim_M2 &lt;- lm(data=Sampdf, y~x+I(x^2))\nSim_M3 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3))\nSim_M4 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4))\nSim_M5 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5))\nSim_M6 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6))\nSim_M7 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7))\nSim_M8 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8))\n\nWe predict the values of the new observations, using each of the 9 models.\n\nNewdf$Deg0Pred &lt;- predict(Sim_M0, newdata=Newdf)\nNewdf$Deg1Pred &lt;- predict(Sim_M1, newdata=Newdf)\nNewdf$Deg2Pred &lt;- predict(Sim_M2, newdata=Newdf)\nNewdf$Deg3Pred &lt;- predict(Sim_M3, newdata=Newdf)\nNewdf$Deg4Pred &lt;- predict(Sim_M4, newdata=Newdf)\nNewdf$Deg5Pred &lt;- predict(Sim_M5, newdata=Newdf)\nNewdf$Deg6Pred &lt;- predict(Sim_M6, newdata=Newdf)\nNewdf$Deg7Pred &lt;- predict(Sim_M7, newdata=Newdf)\nNewdf$Deg8Pred &lt;- predict(Sim_M8, newdata=Newdf)\n\nIn fact, since these data were simulated, we know the true value of \\(y\\), so we can compare the predicted values to the true ones.\n\nkable(Newdf %&gt;% dplyr::select(-c(samp)) %&gt;% round(2) %&gt;% head(5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\ny\nDeg0Pred\nDeg1Pred\nDeg2Pred\nDeg3Pred\nDeg4Pred\nDeg5Pred\nDeg6Pred\nDeg7Pred\nDeg8Pred\n\n\n\n\n108\n5.53\n5.49\n1.17\n1.05\n0.41\n-0.14\n-0.30\n0.10\n0.40\n0.25\n0.31\n\n\n4371\n2.05\n3.92\n1.17\n1.89\n2.02\n3.66\n3.95\n4.26\n3.82\n3.37\n3.49\n\n\n4839\n3.16\n1.46\n1.17\n1.63\n1.29\n3.24\n3.35\n2.78\n2.24\n2.15\n2.07\n\n\n6907\n2.06\n6.79\n1.17\n1.89\n2.02\n3.66\n3.95\n4.25\n3.80\n3.36\n3.47\n\n\n7334\n2.92\n-1.03\n1.17\n1.68\n1.42\n3.43\n3.60\n3.14\n2.51\n2.31\n2.25\n\n\n\n\n\n\n\n7.1.4 Evaluating Predictions - RMSPE\nFor quantitative response variables, we can evaluate the predictions by calculating the average of the squared differences between the true and predicted values. Often, we look at the square root of this quantity. This is called the Root Mean Square Prediction Error (RMSPE).\n[ = , ]\nwhere \\(n'\\) represents the number of new cases being predicted.\nWe calcuate RMSPE for each of the 9 models.\n\nRMSPE0 &lt;- sqrt(mean((Newdf$y-Newdf$Deg0Pred)^2))\nRMSPE1 &lt;- sqrt(mean((Newdf$y-Newdf$Deg1Pred)^2))\nRMSPE2 &lt;- sqrt(mean((Newdf$y-Newdf$Deg2Pred)^2))\nRMSPE3 &lt;- sqrt(mean((Newdf$y-Newdf$Deg3Pred)^2))\nRMSPE4 &lt;- sqrt(mean((Newdf$y-Newdf$Deg4Pred)^2))\nRMSPE5 &lt;- sqrt(mean((Newdf$y-Newdf$Deg5Pred)^2))\nRMSPE6 &lt;- sqrt(mean((Newdf$y-Newdf$Deg6Pred)^2))\nRMSPE7 &lt;- sqrt(mean((Newdf$y-Newdf$Deg7Pred)^2))\nRMSPE8 &lt;- sqrt(mean((Newdf$y-Newdf$Deg8Pred)^2))\n\n\n\n\n\n\nDegree\nRMSPE\n\n\n\n\n0\n4.051309\n\n\n1\n3.849624\n\n\n2\n3.726767\n\n\n3\n3.256592\n\n\n4\n3.283513\n\n\n5\n3.341336\n\n\n6\n3.346908\n\n\n7\n3.370821\n\n\n8\n3.350198\n\n\n\n\n\nThe third degree model did the best at predicting the new data.\nNotice that making the model more complex beyond third degree not only didn’t help, but actually hurt prediction accuracy.\n\n\n7.1.5 Training Data Error\nNow, let’s examine the behavior if we had fit the models to the data, instead of the test data.\n\nRMSE0 &lt;- sqrt(mean(Sim_M0$residuals^2))\nRMSE1 &lt;- sqrt(mean(Sim_M1$residuals^2))\nRMSE2 &lt;- sqrt(mean(Sim_M2$residuals^2))\nRMSE3 &lt;- sqrt(mean(Sim_M3$residuals^2))\nRMSE4 &lt;- sqrt(mean(Sim_M4$residuals^2))\nRMSE5 &lt;- sqrt(mean(Sim_M5$residuals^2))\nRMSE6 &lt;- sqrt(mean(Sim_M6$residuals^2))\nRMSE7 &lt;- sqrt(mean(Sim_M7$residuals^2))\nRMSE8 &lt;- sqrt(mean(Sim_M8$residuals^2))\n\n\nDegree &lt;- 0:8\nTest &lt;- c(RMSPE0, RMSPE1, RMSPE2, RMSPE3, RMSPE4, RMSPE5, RMSPE6, RMSPE7, RMSPE8)\nTrain &lt;- c(RMSE0, RMSE1, RMSE2, RMSE3, RMSE4, RMSE5, RMSE6, RMSE7, RMSE8)\nRMSPEdf &lt;- data.frame(Degree, Test, Train)\nRMSPEdf\n\n  Degree     Test    Train\n1      0 4.051309 3.431842\n2      1 3.849624 3.366650\n3      2 3.726767 3.296821\n4      3 3.256592 2.913233\n5      4 3.283513 2.906845\n6      5 3.341336 2.838738\n7      6 3.346908 2.809928\n8      7 3.370821 2.800280\n9      8 3.350198 2.799066\n\n\nNotice that the most complex model achieves the best performance on the training data, but not on the test data.\nAs the model complexity grows, the model will always fit the training data better, but that does not mean it will perform better on new data. It is possible to start modeling noise, rather than true signal in the training data, which hurts the accuracy of the model when applied to new data.\n\n\n7.1.6 Graph of RMSPE\n\n\n\n\n\n\n\n\n\n\nTraining error decreases as model becomes more complex\n\nTesting error is lowest for the 3rd degree model, then starts to increase again\n\n\n\n7.1.7 Best Model\nOf the models we looked at, the third degree model does the best. The estimates of its coefficients are shown below.\n\nsummary(Sim_M3)\n\n\nCall:\nlm(formula = y ~ x + I(x^2) + I(x^3), data = Sampdf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9451 -1.7976  0.1685  1.3988  6.8064 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept) -0.54165    1.26803  -0.427   0.670221    \nx            4.16638    1.09405   3.808   0.000247 ***\nI(x^2)      -1.20601    0.25186  -4.788 0.00000610 ***\nI(x^3)       0.08419    0.01622   5.191 0.00000117 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.973 on 96 degrees of freedom\nMultiple R-squared:  0.2794,    Adjusted R-squared:  0.2569 \nF-statistic: 12.41 on 3 and 96 DF,  p-value: 0.0000006309\n\n\nIn fact, the data were generated from the model \\(y_i = 4.5x  - 1.4x^2 +  0.1x^3 + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,3)\\)\nWe compare the true expected response curve (in yellow) to the estimates from the various polynomial models.\n\n\n\n\n\n\n\n\n\nThe 8th degree model performs worse than the cubic. The extra terms cause the model to be “too flexible,” and it starts to model random fluctuations (noise) in the training data, that do not capture the true trend for the population. This is called overfitting.\n\n\n7.1.8 Model Complexity, Training Error, and Test Error",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#variance-bias-tradeoff",
    "href": "Ch7.html#variance-bias-tradeoff",
    "title": "7  Predictive Modeling",
    "section": "7.2 Variance-Bias Tradeoff",
    "text": "7.2 Variance-Bias Tradeoff\n\n7.2.1 What Contributes to Prediction Error?\nSuppose \\(Y_i = f(x_i) + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\).\nLet \\(\\hat{f}\\) represent the function of our explanatory variable(s) \\(x^*\\) used to predict the value of response variable \\(y^*\\). Thus \\(\\hat{y}^* = f(x^*)\\).\nThere are three factors that contribute to the expected value of \\(\\left(y^* - \\hat{y}\\right)^2 = \\left(y^* - \\hat{f}(x^*)\\right)^2\\).\n\nBias associated with fitting model: Model bias pertains to the difference between the true response function value \\(f(x^*)\\), and the average value of \\(\\hat{f}(x^*)\\) that would be obtained in the long run over many samples.\n\n\nfor example, if the true response function \\(f\\) is cubic, then using a constant, linear, or quadratic model would result in biased predictions for most values of \\(x^*\\).\n\nVariance associated with fitting model: Individual observations in the training data are subject to random sampling variability. The more flexible a model is, the more weight is put on each individual observation increasing the variance associated with the model.\nVariability associated with prediction: Even if we knew the true value \\(f(x^*)\\), which represents the expected value of \\(y^*\\) given \\(x=x^*\\), the actual value of \\(y^*\\) will vary due to random noise (i.e. the \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\) term).\n\n\n\n7.2.2 Variance and Bias\nThe third source of variability cannot be controlled or eliminated. The first two, however are things we can control. If we could figure out how to minimize bias while also minimizing variance associated with a prediction, that would be great! But…\nThe constant model suffers from high bias. Since it does not include a linear, quadratic, or cubic term, it cannot accurately approximate the true regression function.\nThe Eighth degree model suffers from high variance. Although it could, in theory, approximate the true regression function correctly, it is too flexible, and is thrown off because of the influence of individual points with high degrees of variability.\n\n\n\n\n\n\n\n\n\n\n\n7.2.3 Variance-Bias Tradeoff\nAs model complexity (flexibility) increases, bias decreases. Variance, however, increases.\n\n\n\n\n\n\n\n\n\nIn fact, it can be shown that:\n\\(\\text{Expected RMSPE} = \\text{Variance} + \\text{Bias}^2\\)\nOur goal is the find the “sweetspot” where expected RMSPE is minimized.\n\n\n7.2.4 Modeling for Prediction\n\nWhen our purpose is purely prediction, we don’t need to worry about keeping the model simple enough to interpret.\n\nGoal is to fit data well enough to make good predictions on new data without modeling random noise in the training (overfitting)\n\nA model that is too simple suffers from high bias\n\nA model that is too complex suffers from high variance and is prone to overfitting\n\nThe right balance is different for every dataset\n\nMeasuring error on data used to fit the model (training data) does not accurately predict how well model will be able to predict new data (test data)\n\n\n\n7.2.5 Cross-Validation\nWe’ve seen that training error is not an accurate approximation of test error. Instead, we’ll approximate test error, by setting aside a set of the training data, and using it as if it were a test set. This process is called cross-validation, and the set we put aside is called the validation set.\n\nPartition data into disjoint sets (folds). Approximately 5 folds recommended.\n\nBuild a model using 4 of the 5 folds.\n\nUse model to predict responses for remaining fold.\nCalculate root mean square error \\(RMSPE=\\displaystyle\\sqrt{\\frac{\\sum((\\hat{y}_i-y_i)^2)}{n'}}\\).\n\nRepeat for each of 5 folds.\n\nAverage RMSPE values across folds.\n\nIf computational resources permit, it is often beneficial to perform CV multiple times, using different sets of folds.\n\n\n7.2.6 Cross-Validation Illustration\n\n\n\n\n\nhttps://www.researchgate.net/figure/A-schematic-illustration-of-K-fold-cross-validation-for-K-5-Original-dataset-shown_fig5_311668395\n\n\n\n\n\n\n7.2.7 CV in R\nThe train function in the caret R package performs cross validation automatically. We’ll use it to compare five different models for house prices among a dataset of 1,000 houses sold in Ames, IA between 2006 and 2010.\nWe’ll consider six different models of (mostly) increasing complexity.\n\nlibrary(tidyverse)\nTrain_Data &lt;- read_csv(\"Ames_Train_Data.csv\")  # Load data\nlibrary(caret)   # load caret package\n\n\n# set cross-validation settings - use 10 repeats of 10-fold CV\ncontrol &lt;- trainControl(method=\"repeatedcv\", number=10, repeats=10, savePredictions = \"all\" )\n\n# define models\n# set same random seed before each model to ensure same partitions are used in CV, making them comparable\n\nset.seed(10302023)   \nmodel1 &lt;- train(data=Train_Data, \n                SalePrice ~ `Overall Qual` ,  \n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel2 &lt;- train(data=Train_Data, \n                SalePrice ~ `Overall Qual` +  `Gr Liv Area` + `Garage Area`,  \n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel3 &lt;- train(data=Train_Data, SalePrice ~ `Overall Qual` + \n                  `Gr Liv Area` + `Garage Area` + \n                  `Neighborhood` + `Bldg Type`,  \n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel4 &lt;- train(data=Train_Data, SalePrice ~ `Overall Qual` \n                + `Gr Liv Area`  + `Garage Area` \n                + `Neighborhood` + `Bldg Type` + `Year Built`,  \n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel5 &lt;- train(data=Train_Data, SalePrice ~ `Overall Qual` + \n                  `Gr Liv Area` + `Garage Area` + `Neighborhood` + \n                  `Bldg Type` + `Year Built` + I(`Overall Qual`^2) + \n                  I(`Gr Liv Area`^2) + I(`Garage Area`^2) + \n                  I(`Year Built`^2),  method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel6 &lt;- train(data=Train_Data, SalePrice ~ .,  method=\"lm\", trControl=control)  # include everything linearly\n\n\n# Calculate RMSPE for each model\nRMSPE1 &lt;- sqrt(mean((model1$pred$obs-model1$pred$pred)^2))\nRMSPE2 &lt;- sqrt(mean((model2$pred$obs-model2$pred$pred)^2))\nRMSPE3 &lt;- sqrt(mean((model3$pred$obs-model3$pred$pred)^2))\nRMSPE4 &lt;- sqrt(mean((model4$pred$obs-model4$pred$pred)^2))\nRMSPE5 &lt;- sqrt(mean((model5$pred$obs-model5$pred$pred)^2))\nRMSPE6 &lt;- sqrt(mean((model6$pred$obs-model6$pred$pred)^2))\n\n\nRMSPE1\n\n[1] 51710.58\n\nRMSPE2\n\n[1] 44192.34\n\nRMSPE3\n\n[1] 38212.96\n\nRMSPE4\n\n[1] 38016.67\n\nRMSPE5\n\n[1] 38245.46\n\nRMSPE6\n\n[1] 40586.14\n\n\nWe see that in this case, model M4 performed the best on the hold-out data. We should use Model 4 to make predictions on new data over the other models seen here. It is likely that there are better models out there than model 4, likely with complexity somewhere between that of model 4 and models 5 and 6. Perhaps you can find one.\nOnce we have our preferred model, we can read in our test data and make predictions, and display the first 10 predicted values.\n\nTestData &lt;- read_csv(\"Ames_Test_Data.csv\")\npredictions &lt;- predict(model4, newdata=TestData)  # substitute your best model\nhead(data.frame(predictions), 10)\n\n   predictions\n1    156767.46\n2    252017.22\n3    222084.65\n4    247418.75\n5    116156.38\n6    161242.97\n7    114106.22\n8     46470.89\n9    344009.51\n10   190402.70\n\n\nWe create a csv file containing the predictions, using the code below.\n\nwrite.csv(predictions, file = \"predictions.csv\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#ridge-regression",
    "href": "Ch7.html#ridge-regression",
    "title": "7  Predictive Modeling",
    "section": "7.3 Ridge Regression",
    "text": "7.3 Ridge Regression\n\n7.3.1 Complexity in Model Coefficients\nWe’ve thought about complexity in terms of the number of terms we include in a model, as well as whether we include quadratic terms and higher order terms and interactions. We can also think about model complexity in terms of the coefficients \\(b_1, \\ldots, b_p\\). Larger values of \\(b_1, \\ldots, b_p\\) are associated with more complex models. Smaller values of \\(b_1, \\ldots, b_p\\) are associated with less complex models. When \\(b_j=0\\), this mean variable \\(j\\) is not used in the model.\nTo illustrate, we fit a regression model to the Ames housing dataset, which includes 71 possible explanatory variables, in addition to price.\n\nset.seed(10302021)\nsamp &lt;- sample(1:nrow(ames_raw), 1000)\nTrain_Data &lt;- ames_raw[samp,]\n\nThe full list of coefficient estimates is shown below.\n\nM_OLS &lt;- lm(data=Train_Data, SalePrice ~ .)\nM_OLS$coefficients\n\n            (Intercept)          `Overall Qual`            `Year Built` \n      -14136063.8944780            6975.9249580             494.5819845 \n         `Mas Vnr Area`          `Central Air`Y           `Gr Liv Area` \n             33.3816934           -3745.3629147              37.2290416 \n         `Lot Frontage`            `1st Flr SF`         `Bedroom AbvGr` \n            -14.9594204              13.3690338           -2353.0379012 \n        `TotRms AbvGrd`                   Order                     PID \n            914.5594852               9.0730317               0.7938731 \n       `MS SubClass`030        `MS SubClass`040        `MS SubClass`045 \n           1747.3877587            5867.2885074            7182.7066393 \n       `MS SubClass`050        `MS SubClass`060        `MS SubClass`070 \n          -1330.0097462           -7400.8589386            -953.6999925 \n       `MS SubClass`075        `MS SubClass`080        `MS SubClass`085 \n         -10585.7402995           -6516.1314579           -6527.2337681 \n       `MS SubClass`090        `MS SubClass`120        `MS SubClass`160 \n         -21891.2260114          -20296.4699853          -34837.6663542 \n       `MS SubClass`180        `MS SubClass`190      `MS Zoning`C (all) \n         -18822.1874094          -12421.4283083          -38785.6865317 \n          `MS Zoning`FV      `MS Zoning`I (all)           `MS Zoning`RH \n         -23114.7182938          -17213.3331514          -13306.2219908 \n          `MS Zoning`RL           `MS Zoning`RM              `Lot Area` \n         -17643.5478889          -24783.4465053               0.7407590 \n             StreetPave               AlleyPave                 AlleyNA \n          37262.6393526            2527.1840221              29.0268338 \n         `Lot Shape`IR2          `Lot Shape`IR3          `Lot Shape`Reg \n           8575.5874307           10397.8001841            2676.2691253 \n      `Land Contour`HLS       `Land Contour`Low       `Land Contour`Lvl \n          11318.1617440          -22452.1871122           12230.7109977 \n    `Lot Config`CulDSac         `Lot Config`FR2         `Lot Config`FR3 \n          10427.6521788          -12317.5423512          -14557.2814204 \n     `Lot Config`Inside         `Land Slope`Mod         `Land Slope`Sev \n          -1168.2308323           10550.3766984          -24213.7593573 \n    NeighborhoodBlueste      NeighborhoodBrDale     NeighborhoodBrkSide \n          19001.1401867           20558.0541049           14314.5790198 \n    NeighborhoodClearCr     NeighborhoodCollgCr     NeighborhoodCrawfor \n           7379.3465594             -46.6307558           28775.4384686 \n    NeighborhoodEdwards     NeighborhoodGilbert      NeighborhoodGreens \n          -7491.2919144            1951.6205024            4667.7592928 \n     NeighborhoodIDOTRR     NeighborhoodMeadowV     NeighborhoodMitchel \n          12473.1492830           18891.1264802           -9274.3206260 \n      NeighborhoodNAmes     NeighborhoodNoRidge     NeighborhoodNPkVill \n           1781.9098984           32109.4882191           19601.4137971 \n    NeighborhoodNridgHt      NeighborhoodNWAmes     NeighborhoodOldTown \n          34407.2004203           -3367.8475918            9857.0794600 \n     NeighborhoodSawyer     NeighborhoodSawyerW     NeighborhoodSomerst \n           6761.6510470            3945.8508858           20453.1722509 \n    NeighborhoodStoneBr       NeighborhoodSWISU      NeighborhoodTimber \n          44556.0799048            8518.9310629            1455.9163924 \n    NeighborhoodVeenker      `Condition 1`Feedr       `Condition 1`Norm \n           5759.5517197            -315.5372848           10041.9749570 \n      `Condition 1`PosA       `Condition 1`PosN       `Condition 1`RRAe \n          49498.8746234           15873.9313786          -11599.3284625 \n      `Condition 1`RRAn       `Condition 1`RRNn      `Condition 2`Feedr \n           7894.0460905            6045.1253614            7639.7933280 \n      `Condition 2`Norm       `Condition 2`PosA       `Condition 2`PosN \n           6804.6578647            1496.1296933         -224695.9129765 \n      `Condition 2`RRNn          `Overall Cond`        `Year Remod/Add` \n          20826.2553462            5652.4500122             115.1029835 \n      `Roof Style`Gable     `Roof Style`Gambrel         `Roof Style`Hip \n           -983.4965913           -3775.4699187           -1020.9768277 \n    `Roof Style`Mansard        `Roof Style`Shed      `Roof Matl`CompShg \n          18711.8071355           -9552.0308918          671041.0670584 \n     `Roof Matl`Membran      `Roof Matl`Tar&Grv      `Roof Matl`WdShngl \n         738250.2085208          653237.7170668          757954.9462501 \n  `Mas Vnr Type`BrkFace      `Mas Vnr Type`None     `Mas Vnr Type`Stone \n         -14297.1433973           -5178.2753573          -11764.3736400 \n         `Exter Qual`Fa          `Exter Qual`Gd          `Exter Qual`TA \n         -21903.6214492          -37435.0319282          -40396.2768748 \n         `Exter Cond`Fa          `Exter Cond`Gd          `Exter Cond`Po \n          -2499.1726078            9625.5181637           -9090.3355705 \n         `Exter Cond`TA        FoundationCBlock         FoundationPConc \n          10976.5039644            2136.6596942            4341.3771422 \n         FoundationSlab         FoundationStone          FoundationWood \n          -8011.2100540            5209.4079083          -21499.8677282 \n          `Bsmt Qual`Fa           `Bsmt Qual`Gd           `Bsmt Qual`Po \n         -19118.1141774          -13338.1127108           58476.8071349 \n          `Bsmt Qual`TA           `Bsmt Qual`NA       `Bsmt Exposure`Gd \n         -14904.6825425           29237.6694620            8445.8558434 \n      `Bsmt Exposure`Mn       `Bsmt Exposure`No       `Bsmt Exposure`NA \n          -5636.1528491           -5726.8655933          -28286.6797423 \n         `BsmtFin SF 1`          `BsmtFin SF 2`           `Bsmt Unf SF` \n             33.8127701              24.9524651              13.2017599 \n            HeatingGasW             HeatingWall          `Heating QC`Fa \n           3554.5996405           16477.8270033           -7277.0668485 \n         `Heating QC`Gd          `Heating QC`Po          `Heating QC`TA \n           -760.5324990          -17982.8274261           -5710.2836368 \n        ElectricalFuseF         ElectricalFuseP           ElectricalMix \n            728.4351710           25345.9011297           51715.0815893 \n        ElectricalSBrkr            `2nd Flr SF`        `Bsmt Full Bath` \n          -2872.3538220              12.4886691            -498.4665953 \n       `Bsmt Half Bath`             `Full Bath`             `Half Bath` \n           3026.8853409            3781.8172133            3333.8342115 \n        `Kitchen AbvGr`        `Kitchen Qual`Fa        `Kitchen Qual`Gd \n         -12956.8475513           -7789.8981158          -13554.1557391 \n       `Kitchen Qual`TA          FunctionalMaj2          FunctionalMin1 \n         -13298.5375890          -24570.4189413           -8664.3164291 \n         FunctionalMin2           FunctionalMod           FunctionalTyp \n         -12709.0598893          -14394.7176699            4206.1036385 \n             Fireplaces        `Fireplace Qu`Fa        `Fireplace Qu`Gd \n          11467.2013154          -12766.1232883          -14204.2309228 \n       `Fireplace Qu`Po        `Fireplace Qu`TA        `Fireplace Qu`NA \n         -23140.9430388          -17684.2617792           -5555.0116338 \n    `Garage Type`Attchd    `Garage Type`Basment    `Garage Type`BuiltIn \n          -1098.5463523            -698.4291295           -4596.8780051 \n   `Garage Type`CarPort     `Garage Type`Detchd         `Garage Yr Blt` \n         -10478.5627774            -138.9775476             -61.6633788 \n     `Garage Finish`RFn      `Garage Finish`Unf           `Garage Cars` \n          -4343.2684170           -1482.5096317            2990.4177680 \n          `Garage Area`         `Garage Qual`Fa         `Garage Qual`Gd \n             24.3013657          -70420.9028312          -51114.4969380 \n        `Garage Qual`Po         `Garage Qual`TA         `Garage Cond`Fa \n        -134052.9764560          -67768.2607876           73986.1047213 \n        `Garage Cond`Gd         `Garage Cond`Po         `Garage Cond`TA \n          64104.4268776          112712.4021077           71259.2488661 \n         `Paved Drive`P          `Paved Drive`Y          `Wood Deck SF` \n           5257.0317668            2077.1863821               7.6414867 \n        `Open Porch SF`        `Enclosed Porch`            `3Ssn Porch` \n            -16.2882422             -11.3738346              16.1217441 \n         `Screen Porch`             `Pool Area`             `Pool QC`TA \n             38.2573038            -106.8682816           24551.0371793 \n            `Pool QC`NA               FenceGdWo              FenceMnPrv \n        -100661.2221692               3.7225602            1567.2382267 \n              FenceMnWw                 FenceNA      `Misc Feature`Gar2 \n           -880.4035636            1297.2148680          545161.5427966 \n     `Misc Feature`Othr      `Misc Feature`Shed        `Misc Feature`NA \n         569599.2024080          551469.2569906          547892.2454606 \n             `Misc Val`               `Mo Sold`               `Yr Sold` \n              1.6361319            -262.1972861            5937.2902795 \n         `Sale Type`Con        `Sale Type`ConLD        `Sale Type`ConLI \n           7985.9754307           23972.1096774            9525.1931028 \n       `Sale Type`ConLw          `Sale Type`CWD          `Sale Type`New \n          11056.3865076           -9551.0936247           23118.1311002 \n         `Sale Type`Oth          `Sale Type`VWD          `Sale Type`WD  \n          39733.0338449          -11729.6976216            4819.9397983 \n`Sale Condition`AdjLand  `Sale Condition`Alloca  `Sale Condition`Family \n          41276.9513339           24461.6278440             404.8499384 \n `Sale Condition`Normal `Sale Condition`Partial \n           1592.2625631           -4641.7581345 \n\n\nLet’s focus on the first 10 rows.\n\nhead(coef(M_OLS),10) %&gt;% round(3)\n\n    (Intercept)  `Overall Qual`    `Year Built`  `Mas Vnr Area`  `Central Air`Y \n  -14136063.894        6975.925         494.582          33.382       -3745.363 \n  `Gr Liv Area`  `Lot Frontage`    `1st Flr SF` `Bedroom AbvGr` `TotRms AbvGrd` \n         37.229         -14.959          13.369       -2353.038         914.559 \n\n\nIf all coefficients in the model were 0, then we would be using the most simple constant model, and the prediction for the price of each house would be exactly the same as the overall mean. As \\(b_j's\\) get farther from 0, predictions begin move away from the overall mean and depend more and more on the values or categories of the explanatory variable(s) associated with individual houses. This creates a risk, however, of overfitting.\nA way to combat this, other than dropping variables from the model, is to shrink some or all of the regression coefficients closer to 0, pushing predictions closer to the overall mean.\nA statistical technique for doing this is called ridge regression.\n\n\n7.3.2 Ridge Regression Penalty\nWe’ve seen that in ordinary least-squares regression, \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that to minimizes\n[ ^n (y_i -i)^2 =* ^n (y_i -(b_0 + b_1x{i1} + b_2{x_i2} + +b_px_{ip}))^2 ]\nWhen \\(p\\) is large and we want to be careful of overfitting, a common approach is to add a “penalty term” to this function, to incentive choosing values of \\(b_1, \\ldots, b_p\\) that are closer to 0, thereby “shrinking” the predictions toward the overall mean house price.\nSpecifically, we minimize:\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\n\\end{aligned}\n\\]\nwhere \\(\\lambda\\) is a pre-determined positive constant.\nLarger values of \\(b_j\\) typically help the model better fit the training data, thereby making the first term smaller, but also make the second term larger. The idea is the find optimal values of \\(b_0, b_1, \\ldots, b_p\\) that are large enough to allow the model to fit the data well, thus keeping the first term (SSR) small, while also keeping the penalty term small as well.\n\n\n7.3.3 Choosing \\(\\lambda\\)\nThe value of \\(\\lambda\\) is predetermined by the user. The larger the value of \\(\\lambda\\), the more heavily large \\(b_j's\\) are penalized. A value of \\(\\lambda=0\\) corresponds to ordinary least-squares.\n\\[\n\\begin{aligned}\nQ=& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\n\\end{aligned}\n\\]\n\nSmall values of \\(\\lambda\\) lead to more complex models, with larger \\(|b_j|\\)’s.\n\nAs \\(\\lambda\\) increases, \\(|b_j|\\)’s shrink toward 0. The model becomes less complex, thus bias increases, but variance decreases.\n\nWe can use cross validation to determine the optimal value of \\(\\lambda\\)\n\n\n\n\n\n\n\n\n\n\nWhen using ridge regression, it is important to standardize each explanatory variable (i.e. subtract the mean and divide by the standard deviation). This ensures each variable has mean 0 and standard deviation 1. Without standardizing the optimal choice of \\(b_j\\)’s would depend on scale, with variables with larger absolute measurements having more influence. We’ll standardize the response variable too. Though this is not strictly necessary, it doesn’t hurt. We can always transform back if necessary.\nStandardization is performed using the scale command in R.\n\nTrain_sc &lt;- Train_Data %&gt;% mutate_if(is.numeric, scale)\n\n\n\n7.3.4 Ridge Regression on Housing Dataset\nWe’ll use the caret package to perform cross validation in order to find the optimal value of \\(\\lambda\\). To use ridge regression, we specify method = \"glmnet\", and tuneGrid=expand.grid(alpha=0, lambda=l_vals). Note the alpha value can be changed to use other types of penalized regression sometimes used in predictive modeling, such as lasso or elastic net.\n\ncontrol = trainControl(\"repeatedcv\", number = 10, repeats=10)\nl_vals = 10^seq(-3, 3, length = 100)  # test values between 1/1000 and 1000\n\nset.seed(11162020)\nHousing_ridge &lt;- train(SalePrice ~ .,\n                       data = Train_sc, method = \"glmnet\", trControl=control , \n                      tuneGrid=expand.grid(alpha=0, lambda=l_vals))\n\nValue of \\(\\lambda\\) minimizing RMSPE:\n\nHousing_ridge$bestTune$lambda\n\n[1] 0.6135907\n\n\nWe examine RMSPE on the withheld data as a function of \\(\\lambda\\).\n\n\n\n\n\n\n\n\n\nUsing \\(\\lambda\\) = 0.6135907, obtain the following set of ridge regression coefficients. Notice how the ridge coefficients are typically closer to 0 than the ordinary least squares coefficients, indicating a less complex model.\n\nM_OLS_sc &lt;- lm(data=Train_sc, SalePrice ~ .)\nOLS_coef &lt;- M_OLS_sc$coefficients\nRidge_coef &lt;- coef(Housing_ridge$finalModel, Housing_ridge$bestTune$lambda)[,1]\ndf &lt;- data.frame(OLS_coef[2:10], Ridge_coef[2:10])\nnames(df) &lt;-c(\"OLS Coeff\", \"Ridge Coeff\")\ndf\n\n                   OLS Coeff Ridge Coeff\n`Overall Qual`   0.121728754  0.10435284\n`Year Built`     0.187102422  0.03451303\n`Mas Vnr Area`   0.080212607  0.06202880\n`Central Air`Y  -0.046191694  0.04289126\n`Gr Liv Area`    0.237623291  0.00000000\n`Lot Frontage`  -0.004290945  0.07967743\n`1st Flr SF`     0.069910650  0.01020597\n`Bedroom AbvGr` -0.022457937  0.07194208\n`TotRms AbvGrd`  0.017574153  0.01342224\n\n\nPredictions and residuals for the first six houses in the traning data, using ordinary least squares and ridge regression, are shown below.\n\nlibrary(glmnet)\nMAT &lt;- model.matrix(SalePrice~., data=Train_sc)\nridge_mod &lt;- glmnet(x=MAT, y=Train_sc$SalePrice, alpha = 0, lambda=Housing_ridge$bestTune$lambda )\n\n\ny &lt;- Train_sc$SalePrice\nPred_OLS &lt;- predict(M_OLS_sc)\nPred_Ridge &lt;- predict(ridge_mod, newx=MAT)\nOLS_Resid &lt;- y - Pred_OLS\nRidge_Resid &lt;- y - Pred_Ridge\nResdf &lt;- data.frame(y, Pred_OLS, Pred_Ridge, OLS_Resid, Ridge_Resid)\nnames(Resdf) &lt;- c(\"y\", \"OLS Pred\", \"Ridge Pred\", \"OLS Resid\", \"Ridge Resid\")\nkable(head(Resdf))\n\n\n\n\n\ny\nOLS Pred\nRidge Pred\nOLS Resid\nRidge Resid\n\n\n\n\n859\n-0.6210832\n-0.4637429\n-0.4651589\n-0.1573403\n-0.1559243\n\n\n1850\n0.6800520\n1.1897467\n1.0528536\n-0.5096947\n-0.3728016\n\n\n1301\n-0.4545873\n-0.4527781\n-0.4958630\n-0.0018092\n0.0412758\n\n\n981\n-0.6408161\n-0.6626212\n-0.7711186\n0.0218051\n0.1303025\n\n\n2694\n-0.7937457\n-0.8679455\n-0.7543093\n0.0741997\n-0.0394365\n\n\n2209\n-0.7906625\n-0.6955254\n-0.6449779\n-0.0951370\n-0.1456845\n\n\n\n\n\n\n\n7.3.5 Ridge vs OLS\nIn OLS, we choose \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that minimizes\n\\[\n\\displaystyle\\sum*{i=1}\\^n (y_i -\\hat{y}i)\\^2 =\\* \\displaystyle\\sum{i=1}\\^n (y_i -(b_0 + b_1x{i1} + b_2x\\_{i2} + \\ldots + b_px\\_{ip}))\\^2\n\\]\nOLS: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\)\n\nsum((y-Pred_OLS)^2)\n\n[1] 56.94383\n\n\nRidge: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\)\n\nsum((y-Pred_Ridge)^2)\n\n[1] 127.1331\n\n\nNot surprisingly the OLS model achieves smaller \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\). This has to be true, since the OLS coefficients are chosen to minimize this quantity.\nIn ridge regression, \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that minimizes\n\\[\n\\begin{aligned}\nQ=& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\n\\end{aligned}\n\\]\nOLS: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\)\n\nsum((y-Pred_OLS)^2) + 0.6136*sum(coef(M_OLS_sc)[-1]^2) \n\n[1] 373.1205\n\n\nRidge: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\)\n\nsum((y-Pred_Ridge)^2) + 0.6136*sum((Ridge_coef)[-1]^2)\n\n[1] 130.3375\n\n\nWe see that the ridge coefficients achieve a lower value of Q than the OLS ones.\n\n\n7.3.6 Lasso and Elastic Net\nTwo other techniques that are similar to ridge regression are lasso and elastic net. Both also aim to avoid overfitting by shrinking regression coefficients toward 0 in a manner similar to ridge regression.\nLasso regression is very similar to ridge regression. Coefficients \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that to minimizes\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^p|b_j|\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^p|b_j|\n\\end{aligned}\n\\] Regression with an elastic net uses both ridge and lasso penalty terms and determines the values of \\(b_0, b_1, \\ldots, b_p\\) by minimizing\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^p|b_j|\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda_1\\displaystyle\\sum_{j=1}^pb_j^2+ \\lambda_2\\displaystyle\\sum_{j=1}^p|b_j|\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#decision-trees",
    "href": "Ch7.html#decision-trees",
    "title": "7  Predictive Modeling",
    "section": "7.4 Decision Trees",
    "text": "7.4 Decision Trees\n\n7.4.1 Basics of Decision Trees\nA decision tree is a flexible alternative to a regression model. It is said to be nonparametric because it does not involve parameters like \\(\\beta_0, \\beta_1, \\ldots \\beta_p\\). A tree makes no assumption about the nature of the relationship between the response and explanatory variables, and instead allows us to learn this relationship from the data. A tree makes prediction by repeatedly grouping together like observations in the training data. We can make predictions for a new case, by tracing it through the tree, and averaging responses of training cases in the same terminal node.\nDecision Tree Example:\nWe fit a decision tree to the Ames Housing dataset, using the rpart function in a package by the same name.\n\nlibrary(rpart)\nlibrary(rpart.plot)\ntree &lt;- rpart(SalePrice~., data=Train_Data, cp=0.04)\nrpart.plot(tree, box.palette=\"RdBu\", shadow.col=\"gray\", nn=TRUE, cex=1, extra=1)\n\n\n\n\n\n\n\n\nWe see that the houses are first split based on whether or not their overall quality rating was less than 8. Each of the resulting nodes are then split again, using information from other explanatory variables. Each split partitions the data further, so that houses in the same node can be thought of as being similar to one another.\n\nThe predicted price of a House with overall quality 7, and was built in 1995 is $200,000.\nThe predicted price of a House overall quality 8 and 1,750 sq. ft. on the first floor is $370,000.\n\n\n\n7.4.2 Partitioning in A Decision Tree\nFor a quantitative response variable, data are split into two nodes so that responses in the same node are as similar as possible, while responses in the different nodes are as different as possible.\nLet L and R represent the left and right nodes from a possible split. Let \\(n_L\\) and \\(n_R\\) represent the number of observations in each node, and \\(\\bar{y}_L\\) and \\(\\bar{y}_R\\) represent the mean of the training data responses in each node.\nFor each possible split, involving an explanatory variable, we calculate:\n\\[\n\\displaystyle\\sum\\_{i=1}\\^{n_L} (y_i -\\bar{y}*L)\\^2 +* \\displaystyle\\sum{i=1}\\^{n_R} (y_i -\\bar{y}\\_R)\\^2\n\\]\nWe choose the split that minimizes this quantity.\nPartitioning Example\nConsider a dataset with two explanatory variables, \\(x_1\\) and \\(x_2\\), and a response variable \\(y\\), whose values are shown numerically in the graph.\n\n\n   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\nx1    8    2    8    1    8    6    2    5    1     8     4    10     9     8\nx2    5    3    1    1    4    3    8    1   10     8     6     5     0     2\ny   253   64  258   21  257  203  246  114  331   256   213   406   326   273\n   [,15]\nx1     6\nx2     1\ny    155\n\n\n\n\n\n\n\n\n\n\n\nThe goal is to split up the data, using information about \\(x_1\\) and \\(x_2\\) in a way that makes the \\(y\\) values grouped together as similar as possible.\n1. One Possible Split (\\(x_1 &lt; 5.5\\))\nWe could split the data into 2 groups depending on whether \\(x_1 &lt; 5.5\\).\n\n\n\n\n\n\n\n\n\nWe calcuate the mean y-value in each resulting node:\n\n\\(\\bar{y}_L = (331+246+213+21+64+114)/6 \\approx 164.84\\)\n\n\\(\\bar{y}_R = (203+155+256+253+257+273+258+326+406)/9 \\approx 265.22\\)\n\nTo measure measure the amount of deviation in the node, we calculate the sum of the squared difference between each individual value and the overall mean in each node.\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2  \\\\\n& =(331-164.83)^2+(246-164.33)^2 + \\ldots+(114-164.33)^2 \\\\\n& =69958.83\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\\\\n& =(203-265.22)^2+(155-265.22)^2 + \\ldots+(406-265.22)^2 \\\\\n& =39947.56\n\\end{aligned}\n\\]\nAdding together these two quantities, we obtain an overall measure of the squared deviations between observations in the same node.\n\n69958.83 + 39947.56 = 109906.4\n\n2.Second Possible Split (\\(x_1 &lt; 6.5\\))\nWe could alternatively split the data into 2 groups depending on whether \\(x_1 &lt; 6.5\\).\n\n\n\n\n\n\n\n\n\nUsing this split,\n\n\\(\\bar{y}_L = (331+246+213+21+64+114 + 203+155)/8 \\approx 168.375\\)\n\n\\(\\bar{y}_R = (256+253+257+273+258+326+406)/7 \\approx 289.857\\)\n\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2  \\\\\n& =(331-168.375)^2+(246-168.375)^2 + \\ldots+(203-168.375)^2 \\\\\n& =71411.88\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\\\\n& =(203-289.857)^2+(155-289.857)^2 + \\ldots+(406-289.857)^2 \\\\\n& =19678.86\n\\end{aligned}\n\\]\nThe total squared deviation is:\n\n71411.88 + 19678.86 = 91090.74\n\nThe split at \\(x1 &lt; 6.5\\) is better than \\(x_1&lt;5.5\\)\n3. Third Possible Split (\\(x_2 &lt; 5.5\\))\nWe could also split the data into 2 groups depending on whether \\(x_2 &lt; 5.5\\).\n\n\n\n\n\n\n\n\n\nUsing this split,\n\n\\(\\bar{y}_L = (331+246+213+256)/4 \\approx 261.5\\)\n\n\\(\\bar{y}_R = (21 + 64 + \\ldots + 406)/11 \\approx 211.82\\)\n\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2  \\\\\n& =(331-261.5)^2+(246-261.5)^2 + (213-261.5)^2+(256-261.5)^2 \\\\\n& =7453\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\\\\n& =(21-211.82)^2+(64-211.82)^2 + \\ldots+(406-211.82)^2 \\\\\n& =131493.6\n\\end{aligned}\n\\]\nThe sum of squared deviations is:\n\n7453 + 131493.6 = 138946.6\n\nComparison of Splits\n\nOf the three split’s we’ve calculated, \\(\\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 + \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2\\) is minimized using \\(x_1 &lt; 6.5\\).\nIn fact, if we calculate all possible splits over \\(x_1\\) and \\(x_2\\), \\(\\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 + \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2\\) is minimized by splitting on \\(x_1 &lt; 6.5\\)\n\nThus, we perform the first split in the tree, using \\(x_1 &lt; 6.5\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.3 Next Splits\nNext, we find the best splits on the resulting two nodes. It turns out that the left node is best split on \\(x_2 &lt; 4.5\\), and the right node is best split on \\(x_1 &lt; 8.5\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.4 Recursive Partitioning\nSplitting continues until nodes reach a certain predetermined minimal size, or until change improvement in model fit drops below a predetermined value\n\n\n\n\n\n\n\n\n\n\n\n7.4.5 Model Complexity in Trees\nThe more we partition data into smaller nodes, the more complex the model becomes. As we continue to partition, bias decreases, as cases are grouped with those that are more similar to themselves. On the other hand, variance increases, as there are fewer cases in each node to be averaged, putting more weight on each individual observation.\nSplitting into too small of nodes can lead to drastic overfitting. In the extreme case, if we split all the way to nodes of size 1, we would get RMSE of 0 on the training data, but should certainly not expect RMSPE of 0 on the test data.\nThe optimal depth of the tree, or minimal size for terminal nodes can be determined using cross-validation. The rpart package uses a complexity parameter cp, which determines how much a split must improve model fit in order to be made. Smaller values of cp are associated with more complex tree models, since they allow splits even when model fit only improves by a little.\n\n\n7.4.6 Cross-Validation on Housing Data\nWe’ll use caret to determine the optimal value of the cp parameter. We use method=\"rpart\" to grow decision trees.\n\ncp_vals = 10^seq(-8, 1, length = 100) # test values between 1/10^8 and 1\ncolnames(Train_sc) &lt;- make.names(colnames(Train_sc))\n\nset.seed(11162020)\nHousing_Tree &lt;- train(data=Train_sc, SalePrice ~ .,  method=\"rpart\", trControl=control, \n                     tuneGrid=expand.grid(cp=cp_vals))\n\nThe optimal value of cp is:\n\nHousing_Tree$bestTune\n\n             cp\n52 0.0004328761\n\n\nWe plot RMSPE on the holdout data as a function of cp.\n\ncp &lt;- Housing_Tree$results$cp\nRMSPE &lt;- Housing_Tree$results$RMSE\nggplot(data=data.frame(cp, RMSPE), aes(x=cp, y=RMSPE))+geom_line() + xlim(c(0,0.001)) + ylim(c(0.475,0.485))  + \n  ggtitle(\"Regression Tree Cross Validation Results\")\n\n\n\n\n\n\n\n\n\n\n7.4.7 Comparing OLS, Lasso, Ridge, and Tree\n\nset.seed(11162020)\nHousing_OLS &lt;- train(data=Train_sc, SalePrice ~ .,  method=\"lm\", trControl=control)\nset.seed(11162020)\nHousing_lasso &lt;- train(SalePrice ~., data = Train_sc, method = \"glmnet\", trControl=control, \n                      tuneGrid=expand.grid(alpha=1, lambda=l_vals))\n\nRMSPE on the standardized version of the response variable is displayed below for ordinary least squares, ridge regression, lasso regression, and a decision tree.\n\nmin(Housing_OLS $results$RMSE)\n\n[1] 0.5634392\n\nmin(Housing_ridge$results$RMSE)\n\n[1] 0.4570054\n\nmin(Housing_lasso$results$RMSE)\n\n[1] 0.4730672\n\nmin(Housing_Tree$results$RMSE)\n\n[1] 0.477414\n\n\nIn this situation, the tree outperforms OLS, but does not do as well as lasso or ridge. The best model will vary depending on the nature of the data. We can use cross-validation to determine which model is likely to perform best in prediction.\n\n\n7.4.8 Random Forest\nA popular extension of a decision tree is a random forest. A random forest consists of many (often ~10,000) trees. Predictions are made by averaging predictions from individual trees.\n\nIn order to ensure the trees are different from each other:\n\neach tree is grown from a different bootstrap sample of the training data.\n\nwhen deciding on a split, only a random subset of explanatory variables are considered.\n\n\nGrowing deep trees ensures low bias. In a random forest, averaging across many deep trees decreases variance, while maintaining low bias.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#regression-splines",
    "href": "Ch7.html#regression-splines",
    "title": "7  Predictive Modeling",
    "section": "7.5 Regression Splines",
    "text": "7.5 Regression Splines\n\n7.5.1 Regression Splines\nWe’ve seen that we can use polynomial regression to capture nonlinear trends in data.\n\nA regression spline is a piecewise function of polynomials.\n\nHere we’ll keep thing simple by focusing on a spline with a single explanatory variable. Splines can also be used for multivariate data.\nWe’ll examine the use of splines on the car price prediction dataset.\nWe divide the data into a set of 75 cars, which we’ll use to train the model, and 35 cars, on which we’ll make and evaluate predictions.\nThe 75 cars in the training set are shown below.\n\n\n\n\n\n\n\n\n\n\n\n7.5.2 Two Models with High Bias\n\n\n\n\n\n\n\n\n\nThe constant and linear models have high bias, as they are not complex enough to capture the apparent curvature in the relationship between price and acceleration time.\nA cubic model, on the other hand might better capture the trend.\n\n\n\n\n\n\n\n\n\n\n\n7.5.3 Cubic Splines\nIt’s possible that the behavior of the response variable might differ in different regions of the x-axis. A cubic spline allows us to fit different models in different regions of the x-axis.\n\n\n\n\n\n\n\n\n\nThe region boundaries are called knots\nCubic Spline with 5 Knots\n\n\n\n\n\n\n\n\n\nCubic Spline with 10 Knots\n\n\n\n\n\n\n\n\n\nCubic Spline with 20 Knots\n\n\n\n\n\n\n\n\n\nNotice that as the number of knots increases, the model becomes more and more complex. We would not expect the relationship between price and acceleration time to look like it does in these more complicated pictures. It is likely that as the number of knots gets big, the model overfits the training data.\n\n\n7.5.4 Predicting Test Data\nShown below is a plot of RMSPE when predictions are made on the new test data.\n\n\n\n\n\n\n\n\n\nWe see that RMSPE is minimized using the model with three knots.\n\n\n7.5.5 Implementation of Splines\nImportant Considerations:\n\nhow many knots\n\nwhere to place knots\n\ndegree of polynomial\n\nThe best choices for all of these will vary between datasets and can be assessed through cross-validation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#summary-and-comparision",
    "href": "Ch7.html#summary-and-comparision",
    "title": "7  Predictive Modeling",
    "section": "7.6 Summary and Comparision",
    "text": "7.6 Summary and Comparision\nIn the previous sections, we’ve applied various predictive modeling techniques to predict house prices in Ames, IA. In each section, we’ve focused on an individual predictive technique (OLS, ridge/lasso regression, trees, splines), but in practice, we often test out these techniques together to find which is likely to perform best on a set of data. Here, we’ll go through the steps to test out and evaluate these techniques on the Ames Housing dataset.\nThere are no new statistical ideas presented in this section, just a synthesis of the preceding material. We leave out splines, since we did not discuss using splines in a multivariate setting, but we compare OLS, ridge and decision trees.\nWe use a subset of variables for illustrative purposes.\n\nset.seed(10302021)\nsamp &lt;- sample(1:nrow(ames_raw), 1000)\nAmes_Houses &lt;- ames_raw[samp,]\n\n\nNew_Houses &lt;- ames_raw &lt;- ames_raw[-samp,]\nNew_Houses &lt;- New_Houses[1:5, ]\n\nWe’ll begin by doing some data preparation.\nWe standardize all explanatory variables in the training and new data. We do not standardize the response variable, price, so we can interpret predicted values more easily.\n\nHouses_Combined &lt;- rbind(Ames_Houses, New_Houses)\nHouses_sc &lt;- Houses_Combined %&gt;% mutate_if(is.numeric, scale)\nHouses_sc$SalePrice &lt;- as.numeric(Houses_Combined$SalePrice)\nHouses_sc_Train &lt;- Houses_sc[1:1000, ]\nHouses_sc_New &lt;- Houses_sc[1001:1005, ]\n\nThe Houses_sc_Train dataset contains standardized values for the 1000 houses in the training data. The first six rows are shown below.\n\nhead(Houses_sc_Train)\n\n     Overall.Qual  Year.Built Central.Air  Gr.Liv.Area X1st.Flr.SF\n2771  -0.07619084  0.79212207           Y -0.003620472  -0.9340706\n2909  -0.77229808  0.18658251           Y -0.392399656  -1.3891530\n2368  -0.07619084  0.01837707           Y -0.938684253  -1.6993209\n2604  -2.16451257 -1.89916487           Y -0.571836202  -1.1908489\n669   -0.07619084 -0.14982836           Y -0.918746859  -0.3111924\n1427   2.01213088  1.22945620           Y  0.528707949   1.5345608\n     Bedroom.AbvGr TotRms.AbvGrd    Lot.Area Lot.Shape Land.Contour\n2771     0.1632018    -0.2910546  0.28570669       IR1          Lvl\n2909     0.1632018    -0.9160758 -0.88703104       Reg          Lvl\n2368     0.1632018    -0.2910546 -0.98002927       Reg          Lvl\n2604     0.1632018    -0.2910546  0.04235132       Reg          Lvl\n669      0.1632018    -0.2910546  0.17314883       IR1          Lvl\n1427     0.1632018     0.9589877  0.20650820       Reg          Lvl\n     Overall.Cond Exter.Qual Heating.QC Paved.Drive SalePrice\n2771   -0.4680319         Gd         Ex           Y    187000\n2909    0.4279149         TA         TA           Y    104500\n2368    1.3238618         TA         Ex           Y    116000\n2604   -1.3639788         TA         TA           Y    105000\n669    -1.3639788         Fa         Gd           Y    163000\n1427   -0.4680319         Ex         Ex           Y    395039\n\n\nThe Houses_sc_New displays standardized values for the new houses that we’re trying to predict.\n\nhead(Houses_sc_New)\n\n  Overall.Qual Year.Built Central.Air Gr.Liv.Area X1st.Flr.SF Bedroom.AbvGr\n2  -0.77229808 -0.3516749           Y  -1.2058453  -0.6772922    -1.0608118\n4   0.61991640 -0.1161873           Y   1.2145543   2.4091326     0.1632018\n6  -0.07619084  0.8930453           Y   0.2057222  -0.6010214     0.1632018\n7   1.31602364  0.9939686           Y  -0.3246125   0.4464308    -1.0608118\n8   1.31602364  0.6911988           Y  -0.4402494   0.2989739    -1.0608118\n  TotRms.AbvGrd   Lot.Area Lot.Shape Land.Contour Overall.Cond Exter.Qual\n2    -0.9160758  0.1877886       Reg          Lvl    0.4279149         TA\n4     0.9589877  0.1323496       Reg          Lvl   -0.4680319         Gd\n6     0.3339665 -0.0094877       IR1          Lvl    0.4279149         TA\n7    -0.2910546 -0.6164362       Reg          Lvl   -0.4680319         Gd\n8    -0.9160758 -0.6062364       IR1          HLS   -0.4680319         Gd\n  Heating.QC Paved.Drive SalePrice\n2         TA           Y    105000\n4         Ex           Y    244000\n6         Ex           Y    195500\n7         Ex           Y    213500\n8         Ex           Y    191500\n\n\nSince the glmnet command requires training data to be entered as a matrix, we create versions of the datasets in matrix form.\n\nHouses_sc$SalePrice[is.na(Houses$SalePrice)] &lt;- 0 #can't take NA's when fitting model matrix, doesn't matter since only need x-coeffs\nHouses_sc_Combined_MAT &lt;- model.matrix(SalePrice~., data=rbind(Houses_sc))\nHouses_sc_Train_MAT &lt;- Houses_sc_Combined_MAT[1:1000, ]\nHouses_sc_New_MAT &lt;- Houses_sc_Combined_MAT[1001:1005, ]\n\n\n7.6.1 Modeling with OLS\nWe first fit an ordinary least squares regression model to the data.\n\nHousing_OLS &lt;- lm(data=Houses_sc_Train, SalePrice~ .)\ncoef(Housing_OLS)\n\n    (Intercept)    Overall.Qual      Year.Built    Central.AirY     Gr.Liv.Area \n    238908.0614      23368.4152      14036.2549      -4497.1153      29640.4606 \n    X1st.Flr.SF   Bedroom.AbvGr   TotRms.AbvGrd        Lot.Area    Lot.ShapeIR2 \n      8320.1472      -5011.0485       1554.7785       7566.9377       1570.1676 \n   Lot.ShapeIR3    Lot.ShapeReg Land.ContourHLS Land.ContourLow Land.ContourLvl \n     19082.7508      -4566.5111      44704.8906      22406.5959      19096.4163 \n   Overall.Cond    Exter.QualFa    Exter.QualGd    Exter.QualTA    Heating.QCFa \n      7965.6704     -68750.2773     -62800.5804     -74028.3841      -3972.4036 \n   Heating.QCGd    Heating.QCPo    Heating.QCTA    Paved.DriveP    Paved.DriveY \n     -4478.6876     -23000.4394      -5272.7136      -1901.9235        709.1532 \n\n\n\n\n7.6.2 Ridge Regression with Housing Data\nNow, we’ll use ridge regression to predict insurance costs.\nWe use cross validation to determine the optimal value of lamba. We perform 10 repeats of 10-fold cross-validation. We test 100 lambda-values ranging from \\(10^-5\\) to \\(10^5\\).\n\ncontrol = trainControl(\"repeatedcv\", number = 10, repeats=10)\nl_vals = 10^seq(-5, 5, length = 100)\n\nset.seed(2022)\nHousing_ridge &lt;- train( SalePrice ~ ., data = Houses_sc_Train, method = \"glmnet\", trControl=control , tuneGrid=expand.grid(alpha=0, lambda=l_vals))\n\n\nHousing_ridge$bestTune$lambda\n\n[1] 6135.907\n\n\nWe fit a model to the full training dataset using the optimal value of \\(lambda\\) .\n\nridge_mod &lt;- glmnet(x=Houses_sc_Train_MAT, y=Houses_sc_Train$SalePrice, alpha = 0, lambda=Housing_ridge$bestTune$lambda )\ncoef(ridge_mod)\n\n26 x 1 sparse Matrix of class \"dgCMatrix\"\n                         s0\n(Intercept)     206079.5442\n(Intercept)          .     \nOverall.Qual     24716.6502\nYear.Built       12909.8224\nCentral.AirY     -1693.4832\nGr.Liv.Area      24137.8185\nX1st.Flr.SF      10707.5001\nBedroom.AbvGr    -5034.2266\nTotRms.AbvGrd     5394.5170\nLot.Area          7086.7613\nLot.ShapeIR2      3322.3720\nLot.ShapeIR3     18987.3176\nLot.ShapeReg     -5345.7478\nLand.ContourHLS  41239.7682\nLand.ContourLow  15011.2269\nLand.ContourLvl  12784.0351\nOverall.Cond      6560.4987\nExter.QualFa    -29042.5581\nExter.QualGd    -24942.6445\nExter.QualTA    -35102.6069\nHeating.QCFa     -8118.6371\nHeating.QCGd     -6380.1279\nHeating.QCPo    -19693.6611\nHeating.QCTA     -7645.0855\nPaved.DriveP      -613.3798\nPaved.DriveY      1324.2704\n\n\nThe regression coefficients are displayed together with the OLS coefficients in a data.frame.\n\nRidge_coef &lt;- as.vector(ridge_mod$beta)[-1] #leave off intercept using [-1]\nOLS_coef &lt;- coef(Housing_OLS)[-1]\ndata.frame(OLS_coef, Ridge_coef)\n\n                   OLS_coef  Ridge_coef\nOverall.Qual     23368.4152  24716.6502\nYear.Built       14036.2549  12909.8224\nCentral.AirY     -4497.1153  -1693.4832\nGr.Liv.Area      29640.4606  24137.8185\nX1st.Flr.SF       8320.1472  10707.5001\nBedroom.AbvGr    -5011.0485  -5034.2266\nTotRms.AbvGrd     1554.7785   5394.5170\nLot.Area          7566.9377   7086.7613\nLot.ShapeIR2      1570.1676   3322.3720\nLot.ShapeIR3     19082.7508  18987.3176\nLot.ShapeReg     -4566.5111  -5345.7478\nLand.ContourHLS  44704.8906  41239.7682\nLand.ContourLow  22406.5959  15011.2269\nLand.ContourLvl  19096.4163  12784.0351\nOverall.Cond      7965.6704   6560.4987\nExter.QualFa    -68750.2773 -29042.5581\nExter.QualGd    -62800.5804 -24942.6445\nExter.QualTA    -74028.3841 -35102.6069\nHeating.QCFa     -3972.4036  -8118.6371\nHeating.QCGd     -4478.6876  -6380.1279\nHeating.QCPo    -23000.4394 -19693.6611\nHeating.QCTA     -5272.7136  -7645.0855\nPaved.DriveP     -1901.9235   -613.3798\nPaved.DriveY       709.1532   1324.2704\n\n\n\n\n7.6.3 Decision Tree\nNow, we’ll predict house prices using using a decision tree.\nFirst, we grow and display a small decision tree, by setting the cp parameter equal to 0.05.\n\ntree &lt;- rpart(SalePrice~., data=Houses_sc_Train, cp=0.05)\nrpart.plot(tree, box.palette=\"RdBu\", shadow.col=\"gray\", nn=TRUE, cex=1, extra=1)\n\n\n\n\n\n\n\n\nNow we use cross-validation to determine the optimal value of the cp parameter. We use 10 repeats of 10-fold cross-validation. We test 1000 cp-values ranging from \\(10^-5\\) to \\(10^5\\).\n\ncp_vals = 10^seq(-5, 5, length = 100)\ncolnames(Houses_sc_Train) &lt;- make.names(colnames(Houses_sc_Train))\n\nset.seed(2022)\nHousing_Tree &lt;- train(data=Houses_sc_Train, SalePrice ~ .,  method=\"rpart\", trControl=control,tuneGrid=expand.grid(cp=cp_vals))\nHousing_Tree$bestTune\n\n             cp\n4 0.00002009233\n\n\nWe grow a full tree using the optimal cp value.\n\nHousing_Best_Tree &lt;- rpart(SalePrice~., data=Houses_sc_Train, cp=Housing_Tree$bestTune)\n\n\n\n7.6.4 Comparing Performance\nWe use cross-validation to compare the performance of the linear model, ridge regression model, and decision tree.\n\nset.seed(2022)\nHousing_OLS &lt;- train(data=Houses_sc_Train, SalePrice ~ .,  method=\"lm\", trControl=control)\n\n\nmin(Housing_OLS$results$RMSE)\n\n[1] 35313.3\n\n\n\nmin(Housing_ridge$results$RMSE)\n\n[1] 35747.22\n\n\n\nmin(Housing_Tree$results$RMSE)\n\n[1] 33675.46\n\n\nThe tree predictions give slightly lower RMSPE.\n\n\n7.6.5 Predictions on New Data\nWe now predict the sale price of the five new houses using each technique.\nOrdinary Least-Squares model:\n\nOLS_pred &lt;- predict(Housing_OLS, newdata=Houses_sc_New)\nhead(OLS_pred)\n\n       2        4        6        7        8 \n114709.4 253695.8 195078.1 222117.6 242493.9 \n\n\nRidge regression model:\nWe use the Customers_sc_New_MAT dataset, since the glmnet package requires inputs in matrix form.\n\nridge_pred &lt;- predict(ridge_mod, newx=Houses_sc_New_MAT)\nhead(ridge_pred)\n\n        s0\n2 114950.4\n4 259359.8\n6 195288.0\n7 226841.6\n8 249064.8\n\n\nDecision tree:\n\ntree_pred &lt;- predict(Housing_Best_Tree, newdata=Houses_sc_New)\nhead(tree_pred)\n\n       2        4        6        7        8 \n132926.5 287045.1 183978.6 207079.4 207079.4",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#assessing-a-classifiers-performance",
    "href": "Ch7.html#assessing-a-classifiers-performance",
    "title": "7  Predictive Modeling",
    "section": "7.7 Assessing a Classifier’s Performance",
    "text": "7.7 Assessing a Classifier’s Performance\n\n7.7.1 Measuring Prediction Accuracy\nJust as we’ve done for models with quantitative variables, we’ll want to compare and assess the performance of models for predicting categorical responses. This might involve comparing llogistic regression models with different explanatory variables, or comparing a regression model to another technique such as a decision tree.\nJust as we did before, we’ll divide the data so that we can evaluate predictions on a subset of the data that was not used to fit the model.\nWe’ll divide the credit card dataset into a set of 9,000 observations, on which we’ll fit our models and assess predictions on the remaining 1,000.\n\nset.seed(08172022)\nsamp &lt;- sample(1:nrow(Default), 1000)\nDefault_Test &lt;- Default[samp, ]\nDefault_Train &lt;- Default[-samp, ]\n\nWe fit the model with interaction to the training data:\n\nLR_Default_M_Int &lt;- glm(data=Default_Train, default ~ balance * student, family = binomial(link = \"logit\"))\nsummary(LR_Default_M_Int)\n\n\nCall:\nglm(formula = default ~ balance * student, family = binomial(link = \"logit\"), \n    data = Default_Train)\n\nCoefficients:\n                      Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        -11.2714061   0.5188284 -21.725 &lt;0.0000000000000002 ***\nbalance              0.0060696   0.0003273  18.547 &lt;0.0000000000000002 ***\nstudentYes           0.0924588   0.8606304   0.107               0.914    \nbalance:studentYes  -0.0004749   0.0005142  -0.924               0.356    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2617.1  on 8999  degrees of freedom\nResidual deviance: 1385.5  on 8996  degrees of freedom\nAIC: 1393.5\n\nNumber of Fisher Scoring iterations: 8\n\n\nWe then use the model to estimate the probability of a person defaulting on their credit card payment.\nInformation about 10 different credit card users, as well as the logistic regression estimate of their probability of default are shown below. The table also shows whether or not the user really defaulted on their payment.\n\nLR_Prob &lt;- predict(LR_Default_M_Int, newdata=Default_Test, type=\"response\") %&gt;% round(2)\nActual_Default &lt;- factor(ifelse(Default_Test$default==1, \"Yes\", \"No\"))\nstudent &lt;- Default_Test$student\nbalance &lt;- Default_Test$balance\nLR_Res_df &lt;- data.frame(student, balance, LR_Prob, Actual_Default)\nkable(head(LR_Res_df, 50)%&gt;% arrange(desc(LR_Prob)) %&gt;% head(10))\n\n\n\n\n\nstudent\nbalance\nLR_Prob\nActual_Default\n\n\n\n\n2465\nYes\n2026.864\n0.54\nNo\n\n\n1228\nNo\n1682.201\n0.26\nNo\n\n\n6656\nNo\n1551.028\n0.14\nNo\n\n\n1185\nNo\n1541.813\n0.13\nNo\n\n\n9963\nYes\n1635.175\n0.12\nNo\n\n\n6635\nNo\n1434.128\n0.07\nYes\n\n\n9691\nNo\n1391.318\n0.06\nNo\n\n\n5921\nYes\n1513.542\n0.06\nNo\n\n\n9755\nNo\n1233.619\n0.02\nNo\n\n\n7569\nYes\n1294.286\n0.02\nNo\n\n\n\n\n\n\n\n7.7.2 Decision Tree Classifier\nFor comparison, let’s use a decision tree to predict whether a person will default.\nIn a binary classification problem, we can treat a default as \\(y=1\\) and non-default as \\(y=0\\), and grow the tree as we would in regression.\nThe mean response in a node \\(\\bar{Y}\\), which is equivalent to the proportion of people in the node who defaulted, can be interpreted as the probability of default.\nThe first few splits of the tree are shown.\n\nlibrary(rpart)\nlibrary(rpart.plot)\n# grow shorter tree for illustration\ntree &lt;- rpart(data=Default_Train, default~balance + student, cp=0.005)\nrpart.plot(tree, box.palette=\"RdBu\", shadow.col=\"gray\", nn=TRUE, cex=1, extra=1)\n\n\n\n\n\n\n\n\n\n# grow full tree\ntree &lt;- rpart(data=Default_Train, default~balance + student)\n\n\nTree_Prob &lt;- predict(tree, newdata = Default_Test) %&gt;% round(2)\n\nWe add the decision tree probabilities to the table seen previously.\n\nLR_Res_df &lt;- data.frame(student, balance, LR_Prob, Tree_Prob, Actual_Default)\nkable(head(LR_Res_df, 50)%&gt;% arrange(desc(LR_Prob)) %&gt;% head(10))\n\n\n\n\n\nstudent\nbalance\nLR_Prob\nTree_Prob\nActual_Default\n\n\n\n\n2465\nYes\n2026.864\n0.54\n0.77\nNo\n\n\n1228\nNo\n1682.201\n0.26\n0.16\nNo\n\n\n6656\nNo\n1551.028\n0.14\n0.16\nNo\n\n\n1185\nNo\n1541.813\n0.13\n0.16\nNo\n\n\n9963\nYes\n1635.175\n0.12\n0.16\nNo\n\n\n6635\nNo\n1434.128\n0.07\n0.01\nYes\n\n\n9691\nNo\n1391.318\n0.06\n0.01\nNo\n\n\n5921\nYes\n1513.542\n0.06\n0.16\nNo\n\n\n9755\nNo\n1233.619\n0.02\n0.01\nNo\n\n\n7569\nYes\n1294.286\n0.02\n0.01\nNo\n\n\n\n\n\nWe see that the tree estimates that the first person has a 0.77 probability of defaulting on the payment, compared to an estimate of 0.54, given by the logistic regression model. On the other hand, the tree estimates only a 0.16 probability of the second person defaulting, compared to 0.26 for the logistic regression model.\n\n\n7.7.3 Assessing Classifier Accuracy\nWe’ve seen \\(\\text{RMSPE} = \\sqrt{\\displaystyle\\sum_{i=1}^{n}{(\\hat{y}_i-y_i)^2}}\\) used as a measure of predictive accuracy in a regression problem.\nSince our outcome is not numeric, this is not a good measure of predictive accuracy in a classification problem. We’ll examine some alternatives we can use instead.\nClassification Accuracy\nOne simple approach is calculate the proportion of credit card users classified correctly. If a person has model estimates a predicted probability of default greater than 0.5, the person is predicted to default, while if the probability estimate is less than 0.5, the person is predicted to not default.\nThe table shows the prediction for each of the 10 users, using both logistic regression and the decision tree.\n\nLR_Pred &lt;- factor(ifelse(LR_Prob &gt; 0.5, \"Yes\", \"No\"))\nTree_Pred &lt;- factor(ifelse(Tree_Prob &gt; 0.5, \"Yes\", \"No\"))\nLR_Res_df &lt;- data.frame(student, balance, LR_Prob, Tree_Prob, LR_Pred,Tree_Pred, Actual_Default)\nkable(head(LR_Res_df, 50)%&gt;% arrange(desc(LR_Prob)) %&gt;% head(10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudent\nbalance\nLR_Prob\nTree_Prob\nLR_Pred\nTree_Pred\nActual_Default\n\n\n\n\n2465\nYes\n2026.864\n0.54\n0.77\nYes\nYes\nNo\n\n\n1228\nNo\n1682.201\n0.26\n0.16\nNo\nNo\nNo\n\n\n6656\nNo\n1551.028\n0.14\n0.16\nNo\nNo\nNo\n\n\n1185\nNo\n1541.813\n0.13\n0.16\nNo\nNo\nNo\n\n\n9963\nYes\n1635.175\n0.12\n0.16\nNo\nNo\nNo\n\n\n6635\nNo\n1434.128\n0.07\n0.01\nNo\nNo\nYes\n\n\n9691\nNo\n1391.318\n0.06\n0.01\nNo\nNo\nNo\n\n\n5921\nYes\n1513.542\n0.06\n0.16\nNo\nNo\nNo\n\n\n9755\nNo\n1233.619\n0.02\n0.01\nNo\nNo\nNo\n\n\n7569\nYes\n1294.286\n0.02\n0.01\nNo\nNo\nNo\n\n\n\n\n\nNotice that although the probabilities differ, the logistic regression model and classification tree give the same predictions for these ten cases. Both correctly predict 8 out of the 10 cases, but mistakenly predict the first person to default, when they didn’t, and mistakenly predict that the sixth person would not default when they did.\nWe’ll check the classification accuracy for the model and the tree.\n\nsum(LR_Pred == Actual_Default)/1000\n\n[1] 0.972\n\n\n\nsum(Tree_Pred == Actual_Default)/1000\n\n[1] 0.971\n\n\nWe see that the two techniques are each right approximately 97% of the time.\nThis may not really be as good as it sounds. Can you think of a very simple classification strategy that would achieve a similarly impressive predictive accuracy on these data?\n\n\n7.7.4 Confusion Matrix\nIn addition to assessing overall accuracy, it is sometimes helpful to assess how well models are able to predict outcomes in each class. For example, how accurately can a model detect people who do actually default on their payments?\nA confusion matrix is a two-by-two table displaying the number of cases predicted in each category as columns, and the number of cases actually in each category as rows\n\n\n\n\nActually Negative\nActually Positive\n\n\n\n\nPredicted Negative\n# True Negative\n# False Negative\n\n\nPredicted Positive\n# False Positive\n# True Positive\n\n\n\nThe confusionMatrix matrix command in R returns the confusion matrix for all 1,000 test cases.\nLet’s look at the confusion matrix for all 1,000 test cases. The data argument is the predicted outcome, and the reference argument is the true outcome. The positive argument is the category that we’ll classify as a positive.\nLogistic Regression Confusion Matrix\n\nconfusionMatrix(data=LR_Pred, reference=factor(Actual_Default) , positive=\"Yes\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  957  20\n       Yes   8  15\n                                          \n               Accuracy : 0.972           \n                 95% CI : (0.9598, 0.9813)\n    No Information Rate : 0.965           \n    P-Value [Acc &gt; NIR] : 0.12988         \n                                          \n                  Kappa : 0.5035          \n                                          \n Mcnemar's Test P-Value : 0.03764         \n                                          \n            Sensitivity : 0.4286          \n            Specificity : 0.9917          \n         Pos Pred Value : 0.6522          \n         Neg Pred Value : 0.9795          \n             Prevalence : 0.0350          \n         Detection Rate : 0.0150          \n   Detection Prevalence : 0.0230          \n      Balanced Accuracy : 0.7101          \n                                          \n       'Positive' Class : Yes             \n                                          \n\n\nOut of 965 people who did not default, the logistic regression model correctly predicted 957 of them.\nOut of 35 people that did default, the model correctly predicted 15 of them.\nTree Confusion Matrix\n\n# data is predicted class\n# reference is actual class\nconfusionMatrix( data = Tree_Pred , reference= Actual_Default, \"Yes\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  960  24\n       Yes   5  11\n                                          \n               Accuracy : 0.971           \n                 95% CI : (0.9586, 0.9805)\n    No Information Rate : 0.965           \n    P-Value [Acc &gt; NIR] : 0.1724819       \n                                          \n                  Kappa : 0.4186          \n                                          \n Mcnemar's Test P-Value : 0.0008302       \n                                          \n            Sensitivity : 0.3143          \n            Specificity : 0.9948          \n         Pos Pred Value : 0.6875          \n         Neg Pred Value : 0.9756          \n             Prevalence : 0.0350          \n         Detection Rate : 0.0110          \n   Detection Prevalence : 0.0160          \n      Balanced Accuracy : 0.6546          \n                                          \n       'Positive' Class : Yes             \n                                          \n\n\nOut of 965 people who did not default, the logistic regression model correctly predicted 960 of them.\nOut of 35 people that did default, the model correctly predicted 11 of them.\nNotice that the tree was less likely to predict a person to default in general, returning only 16 positive predictions, compared to 23 for the logistic regression model.\n\n\n7.7.5 Sensitivity and Specificity\nThe sensitivity of a classifier is the proportion of all positive cases that the model correctly identifies as positive. (i.e. probability model says “positive” given actually is positive.)\n\\[\n\\text{Sensitivity} = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Negative}} = \\frac{\\text{Correctly Predicted Positives}}{\\text{Total Number of Actual Positives}}\n\\]\nLR Sensitivity\n\\[\n\\frac{15}{15+20} \\approx 0.4286\n\\]\nTree Sensitivity\n\\[\n\\frac{11}{11+24} \\approx 0.3143\n\\]\nThe specificity of a classifier is the proportion of all negative cases that the model correctly identifies as negative (i.e probabiltiy model says “negative” given truly is negative.)\n\\[\\text{Specificity} = \\frac{\\text{True Negative}}{\\text{True Negative} + \\text{False Positive}}= \\frac{\\text{Correctly Predicted Negatives}}{\\text{Total Number of Actual Negatives}}\n\\]\nLR Specificity\n\\[\\frac{957}{957+8} \\approx 0.9917\\]\nTree Specificity\n\\[\\frac{960}{960+5} \\approx 0.9948 \\]\nIn a given situation, we should think about the cost of a false negative vs a false positive when determining whether to place more weight on sensitivity or specificity. For example, “is it worse to tell a patient they tested positive for a disease when they really don’t have it, or to not tell them they tested positive when they really do have it?”",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#receiver-operating-characteristic-curve",
    "href": "Ch7.html#receiver-operating-characteristic-curve",
    "title": "7  Predictive Modeling",
    "section": "7.8 Receiver Operating Characteristic Curve",
    "text": "7.8 Receiver Operating Characteristic Curve\n\n7.8.1 Separating +’s and -’s\nThe prediction accuracy, sensitivity, and specificity measures, seen in the previous section are based only on the predicted outcome, without considering the probability estimates themselves. These techniques treat a 0.49 estimated probability of default the same as a 0.01 estimated probability.\nWe would hope to see more defaults among people with high estimated default probabilities than low ones. To assess this, we can list the people in order from highest to lowest probability estimates and see where the true defaults lie.\nFor example, consider the following fictional probability estimates produced by two different classifiers (models) for eight credit card users:\nClassifier 1\n\n\n  Classifier1_Probability_Estimate True_Outcome\n1                             0.90          Yes\n2                             0.75          Yes\n3                             0.60           No\n4                             0.40          Yes\n5                             0.30           No\n6                             0.15           No\n7                             0.05           No\n8                             0.01           No\n\n\nClassifier 2\n\n\n  Classifier2_Probability_Estimate True_Outcome\n1                             0.80          Yes\n2                             0.70           No\n3                             0.55           No\n4                             0.40          Yes\n5                             0.35           No\n6                             0.15           No\n7                             0.10          Yes\n8                             0.02           No\n\n\nClassifier 1 is better able to separate the “Yes’s” from “No’s” as the three true “Yes’s” are among the four highest probabilities. Classifier 2 is less able to separate the true “Yes’s” from true “No’s.”\n\n\n7.8.2 ROC Curve\nA receiver operating characteristic (ROC) curve tells us how well a predictor is able to separate positive cases from negative cases.\nThe blog (Toward Data Science) [https://towardsdatascience.com/applications-of-different-parts-of-an-roc-curve-b534b1aafb68] writes\n“Receiver Operating Characteristic (ROC) curve is one of the most common graphical tools to diagnose the ability of a binary classifier, independent of the inherent classification algorithm. The ROC analysis has been used in many fields including medicine, radiology, biometrics, natural hazards forecasting, meteorology, model performance assessment, and other areas for many decades and is increasingly used in machine learning and data mining research [1]. If you are a Data Scientist, you might be using it on a daily basis.”\nThe ROC curve plots the true positive (or hit) rate against the false positive rate (false alarm) rate, as the cutoff for a positive classification varies.\n\n\n\n\n\n\n\n\n\nThe higher the curve, the better the predictor is able to separate positive cases from negative ones.\nPredictions made totally at random would be expected to yield a diagonal ROC curve.\n\n\n7.8.3 Constructing ROC Curve\n\nOrder the probabilities from highest to lowest.\n\nAssume only the case with the highest probability is predicted as a positive.\n\nCalculate the true positive rate (hit rate) \\[\\frac{\\text{\\# True Positives}}{\\text{\\# Actual Positives}}\\] and false positive (false alarm) \\[\\frac{\\text{\\# False Positives}}{\\text{\\# Actual Negatives}}\\]rate.\nPlot the point \\[\\left( \\frac{\\text{\\# False Positives}}{\\text{\\# Actual Negatives}}, \\frac{\\text{\\# True Positives}}{\\text{\\# Actual Positives}} \\right)\\] in the coordinate plane.\n\nNow assume the cases with the two highest probabilities are predicted as positives, and repeat steps 3-4.\n\nContinue, by classifiying one more case as positive in each step.\n\n\n\n7.8.4 Construct ROC Example\nLet’s practice constructing an ROC curve for a small set of probability estimates.\n\nprob &lt;- c(0.9, 0.8, 0.7, 0.65, 0.45, 0.3, 0.2, 0.15, 0.1, 0.05)\nActual &lt;- c(\"+\", \"-\", \"+\", \"+\", \"-\", \"-\", \"-\", \"-\", \"+\", \"-\")\nHit_Rate &lt;- c(\"1/4\", \"1/4\", \"2/4\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")\nFA_Rate &lt;- c(\"0/6\", \"1/6\", \"1/6\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")\nkable(data.frame(prob, Actual, Hit_Rate, FA_Rate))\n\n\n\n\nprob\nActual\nHit_Rate\nFA_Rate\n\n\n\n\n0.90\n+\n1/4\n0/6\n\n\n0.80\n-\n1/4\n1/6\n\n\n0.70\n+\n2/4\n1/6\n\n\n0.65\n+\n\n\n\n\n0.45\n-\n\n\n\n\n0.30\n-\n\n\n\n\n0.20\n-\n\n\n\n\n0.15\n-\n\n\n\n\n0.10\n+\n\n\n\n\n0.05\n-\n\n\n\n\n\n\n\nFinish filling in the table and sketch a graph of the resulting ROC curve.\nQuestion: If the probability estimate of 0.45 were instead 0.5 or 0.55, would this change the ROC curve? Why or why not?\n\n\n7.8.5 AUC\nThe area under the ROC curve, (AUC) provides a measure of the model’s predictive strength.\nWhile there is no standard for what constitutes a good\" AUC, higher is better, andAUC” is useful for comparing models.\nA model that can perfectly separate successes from failures will have an AUC of 1.\nA model that assigns probabilities at random is expected to have an AUC of 0.5.\n\n\n7.8.6 LR and Tree ROC Curves\n\nlibrary(pROC)\nlibrary(verification)\nroc.plot(x=Default_Test$default, pred = LR_Prob)\n\n\n\n\n\n\n\n\n\nauc(response=Default_Test$default, predictor = LR_Prob)\n\nArea under the curve: 0.8953\n\n\n\nroc.plot(x=Default_Test$default, pred = Tree_Prob)\n\n\n\n\n\n\n\n\n\nauc(response=Default_Test$default, predictor = Tree_Prob)\n\nArea under the curve: 0.8176\n\n\n\nRandProb &lt;- runif(1000, 0, 1)\n\n\nroc.plot(x=Default_Test$default, pred = RandProb)\n\n\n\n\n\n\n\n\n\nauc(response=Default_Test$default, predictor = RandProb)\n\nArea under the curve: 0.563\n\n\nEven though a model that assigns predictions randomly, with 97% predicted as negatives will have a high accuracy rate, it will yield a poor ROC curve indicating an inability to separate positive cases from negative ones.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#ethical-considerations-in-predictive-modeling",
    "href": "Ch7.html#ethical-considerations-in-predictive-modeling",
    "title": "7  Predictive Modeling",
    "section": "7.9 Ethical Considerations in Predictive Modeling",
    "text": "7.9 Ethical Considerations in Predictive Modeling\n\n7.9.1 Assumptions in Predictive Models\nLike any other statistical technique, predictive inference (sometimes done through machine learning algorithms) depends on the validity of assumptions.\n\nThe response variable observed in the data is actually the thing we want to predict\n\nTraining/Test data representative of population of interest\nPrediction accuracy is appropriate metric\n\nBelow are some examples of real uses of predictive inference in which some of these assumptions were violated, leading to inappropriate and unethical conclusions.\n\n\n7.9.2 Amazon Hiring Algorithm\nIn 2014, Amazon began working on an algorithm to predict whether a job applicant would be suitable for hire for software developer positions, based on characteristics of their job application.\nresponse variable: rating of candidate’s strength (1-5) explanatory variables: many variables based on information included on the resume (e.g. highest degree, major, GPA, college/university, prior job experiences, internships, frequency of certain words on resume, etc.)\nThe algorithm was trained using data from past applications, rated by humans, over the past 10 years. It could then be used to predict ratings of future job applicants.\nAccording to [Reuters])(https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G),\n“In effect, Amazon’s system taught itself that male candidates were preferable. It penalized resumes that included the word “women’s,” as in “women’s chess club captain.” And it downgraded graduates of two all-women’s colleges, according to people familiar with the matter.”\nWhile the algorithm was intended to predict candidate quality, the response variable on the training data actually reflected biases in past hiring decisions, leading the algorithm to do the same.\n\n\n7.9.3 Facial Recognition\nFacial recognition technology is used by law enforcement surveillance, airport passenger screening, and employment and housing decisions. It has, however, been banned for use by police in some cities, including San Francisco and Boston, due to concerns about inequity and privacy.\nResearch has shown that although certain facial recognition algorithms achieve over 90% accuracy overall, accuracy rate is lower among subjects who are female, Black, or 18-30 years old.\nThis is likely due, at least in part, to the algorithms being trained primarily on data an images of people who are not members of these groups.\nAlthough the algorithms might attain strong accuracy overall, it is inappropriate to evaluate them on this basis, without accounting for performance on subgroups in the population.\n\n\n7.9.4 Comments\nThe biases and assumptions noted above are not reasons to abandon predictive modeling, but rather flaws to be aware of and work to correct.\nPredictive algorithms, are only as good as the data on which they are trained and the societies in which they are developed, and will reflect inherent biases. Thus, they should be used cautiously and with with human judgment, just like any other statistical technique.\nBeware of statements like:\n“The data say this!”\n“The algorithm is objective.”\n“The numbers don’t lie.”\nAny data-driven analysis depends on assumptions, and sound judgment and awareness of context are required when assessing the validaty of conclusions drawn.\n\n\n7.9.5 Modeling for Prediction\n\nGoal is to make the most accurate predictions possible.\n\nNot concerned with understanding relationships between variables. Not worried model being to complicated to interpret, as long as it yields good predictions.\n\nAim for a model that best captures the signal in the data, without being thrown off by noise.\n\n\nLarge number of predictors is ok\n\nDon’t make model so complicated that it overfits the data.\n\n\nBe sure that model is predicting what you intend it to\n\nReflective of biases inherent in the data on which it was trained",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  }
]