[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 255 Notes",
    "section": "",
    "text": "Preface\nThese notes serve as the primary textual resource for Stat 255: Statistics for Data Science at Lawrence University.\nWhat is this course about?\nStat 255 provides an introduction to essential statistical tasks including modeling, inference, prediction, and computation. The course employs a modern approach, intended to equip students with skills needed for working with today’s complex data. Traditional concepts, like interval estimation and hypothesis testing, are introduced through the lens of multivariate models and simulation. Data computation in R plays a central role throughout the course.\nThe course’s overarching learning outcomes are:\n\nVisualize and wrangle data using statistical software R.\n\nBuild and assess multivariate models to predict future outcomes.\n\nUse statistics from samples to draw inferences about larger populations or processes.\n\nQuantify uncertainty associated with estimates and predictions.\n\nExplain the assumptions associated with statistical models, and evaluate whether these assumptions are reasonably satisfied in context.\n\nWrite reproducible analyses, using statistical software.\n\nMake ethical decisions based on data.\n\nMore specific learning tasks, related to these outcomes are provided in each chapter.\nWho is this course intended for?\nThis course is intended for students who are interested in learning statistical modeling and data computation skills that might prove useful in further courses, research, or career.\nStat 255 can serve as either:\n\na first course in statistics for students with a strong quantitative background, typically including calculus.\na second course in statistics, building on introductory topics taught in courses like Lawrence’s Stat 107: Principles of Statistics or AP Statistics.\n\nAt Lawrence, this course is required for the statistics track of the mathematics major, the economics and mathematics-economics majors, the business analytics track of the business and entrepreneurship major, and the statistics and data science minor. It also satisfies the statistics requirement for several other majors and minors.\nThe prerequisite for the course is either 1) a prior college-level course in statistics (i.e. STAT 107, BIOL 170 or 280, ANTH 207, AP Stats) OR 2) Calculus. (Math 140, AP Calculus, or equivalent).\nThe course does not assume any prior knowledge of statistics, but does move more rapidly than a typical introductory statistics course. Students engage rigorously in statistical thinking and computation, intended to equip them with essential skills for further study in statistics and data science.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Ch1.html",
    "href": "Ch1.html",
    "title": "1  Visualizing and Summarizing Data",
    "section": "",
    "text": "Learning Outcomes\nConceptual Learning Outcomes\n1. Interpret and draw connections between data graphics (histogram, density plot, box plot, violin plot, scatterplot, bar graph) and summary statistics (mean, median, standard deviation, and IQR).\nComputational Learning Outcomes\nA. Calculate summary statistics R.\nB. Create data visualizations in R.\nC. Create reproducible reports in R.*\nD. Use GitHub for version control.*",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch1.html#getting-started-in-r",
    "href": "Ch1.html#getting-started-in-r",
    "title": "1  Visualizing and Summarizing Data",
    "section": "1.1 Getting Started in R",
    "text": "1.1 Getting Started in R\nWe’ll work with data on houses that sold in King County, WA, (home of Seattle) between 2014 and 2015.\nWe begin by loading the tidyverse package which can be used to create professional data graphics and summaries.\n\nlibrary(tidyverse)\n\n\n1.1.1 Previewing the Data\nhead()\nThe head() function displays the first 5 rows of the dataset.\n\nhead(Houses)\n\n# A tibble: 6 × 9\n     ID price sqft_living waterfront condition       sqft_lot bedrooms bathrooms\n  &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;fct&gt;              &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    90  335.        1030 No         average or bel…     1066        2      1.75\n2   126 1450.        2750 No         average or bel…    17789        4      2.75\n3   300  268.        1590 No         average or bel…    11914        3      1.75\n4   541 2125         5403 Yes        good               24069        3      2.5 \n5   932  545         2520 No         average or bel…     7863        5      2.5 \n6  1063  790.        2840 No         good               11900        4      2.75\n# ℹ 1 more variable: yr_built &lt;dbl&gt;\n\n\nThe rows of the dataset are called observations. In this case, the observations are the houses.\nThe columns of the dataset, which contain information about the houses, are called variables.\nglimpse\nThe glimpse() command shows the number of observations (rows), and the number of variables, (columns). We also see the name of each variable and its type. Variable types include\n\nCategorical variables, which take on groups or categories, rather than numeric values. In R, these might be coded as logical &lt;logi&gt;, character &lt;chr&gt;, factor &lt;fct&gt; and ordered factor &lt;ord&gt;.\nQuantitative variables, which take on meaningful numeric values. These include numeric &lt;num&gt;, integer &lt;int&gt;, and double &lt;dbl&gt;.\nDate and time variables take on values that are dates and times, and are denoted &lt;dttm&gt;\n\n\nglimpse(Houses)\n\nRows: 200\nColumns: 9\n$ ID          &lt;int&gt; 90, 126, 300, 541, 932, 1063, 1147, 1313, 1535, 1715, 1718…\n$ price       &lt;dbl&gt; 335.00, 1450.00, 267.50, 2125.00, 545.00, 790.00, 510.00, …\n$ sqft_living &lt;dbl&gt; 1030, 2750, 1590, 5403, 2520, 2840, 1860, 2590, 1620, 3120…\n$ waterfront  &lt;chr&gt; \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ condition   &lt;fct&gt; average or below, average or below, average or below, good…\n$ sqft_lot    &lt;dbl&gt; 1066, 17789, 11914, 24069, 7863, 11900, 3658, 9530, 7560, …\n$ bedrooms    &lt;dbl&gt; 2, 4, 3, 3, 5, 4, 3, 4, 4, 4, 2, 4, 3, 4, 3, 5, 3, 2, 4, 2…\n$ bathrooms   &lt;dbl&gt; 1.75, 2.75, 1.75, 2.50, 2.50, 2.75, 2.50, 2.50, 1.75, 3.25…\n$ yr_built    &lt;dbl&gt; 2006, 1914, 1957, 1976, 1981, 1961, 1994, 1978, 1947, 2005…\n\n\nThere are 200 houses in the dataset, and 9 variables on each house.\nsummary\nsummary displays the mean, minimum, first quartile, median, third quartile, and maximum for each numeric variable, and the number of observations in each category, for categorical variables.\n\nsummary(Houses)\n\n       ID            price         sqft_living    waterfront       \n Min.   :   90   Min.   : 105.0   Min.   : 700   Length:200        \n 1st Qu.: 5162   1st Qu.: 329.5   1st Qu.:1498   Class :character  \n Median :10490   Median : 457.8   Median :2100   Mode  :character  \n Mean   :10743   Mean   : 710.2   Mean   :2316                     \n 3rd Qu.:15605   3rd Qu.: 748.8   3rd Qu.:2815                     \n Max.   :21536   Max.   :4500.0   Max.   :7620                     \n            condition      sqft_lot         bedrooms       bathrooms    \n average or below:142   Min.   :   720   Min.   :1.000   Min.   :0.500  \n good            : 47   1st Qu.:  5000   1st Qu.:3.000   1st Qu.:1.750  \n very_good       : 11   Median :  8236   Median :3.000   Median :2.500  \n                        Mean   : 14093   Mean   :3.335   Mean   :2.246  \n                        3rd Qu.: 12258   3rd Qu.:4.000   3rd Qu.:2.500  \n                        Max.   :215186   Max.   :5.000   Max.   :5.500  \n    yr_built   \n Min.   :1901  \n 1st Qu.:1955  \n Median :1978  \n Mean   :1974  \n 3rd Qu.:1998  \n Max.   :2014  \n\n\n\n\n1.1.2 Modifying the Data\n\nSelecting Columns\nIf the dataset contains a large number of variables, narrow down to the ones you are interested in working with. This can be done with the select() command. If there are not very many variables to begin with, or you are interested in all of them, then you may skip this step.\nLet’s create a smaller version of the dataset, with only the columns ID, price, sqft_living, and waterfront. We’ll call this Houses_3var.\n\nHouses_4var &lt;- Houses |&gt; select(ID, price, sqft_living, waterfront)\nhead(Houses_4var)\n\n# A tibble: 6 × 4\n     ID price sqft_living waterfront\n  &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;     \n1    90  335.        1030 No        \n2   126 1450.        2750 No        \n3   300  268.        1590 No        \n4   541 2125         5403 Yes       \n5   932  545         2520 No        \n6  1063  790.        2840 No        \n\n\n\n\nFiltering by Row\nThe filter() command narrows a dataset down to rows that meet specified conditions.\nWe’ll filter the data to include only houses built after 2000.\n\nNew_Houses &lt;- Houses |&gt; filter(yr_built&gt;=2000)\nhead(New_Houses)\n\n# A tibble: 6 × 9\n     ID price sqft_living waterfront condition       sqft_lot bedrooms bathrooms\n  &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;fct&gt;              &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    90  335.        1030 No         average or bel…     1066        2      1.75\n2  1715  799         3120 No         average or bel…     5000        4      3.25\n3  2445 3278.        6840 Yes        average or bel…    10000        2      1.75\n4  2627 4500.        6640 Yes        average or bel…    40014        5      5.5 \n5  2748  726         2970 No         average or bel…    10335        5      3   \n6  2878  195         1180 No         average or bel…     2553        2      2   \n# ℹ 1 more variable: yr_built &lt;dbl&gt;\n\n\nNow, we’ll filter the data to include only houses on the waterfront.\n\nNew_Houses &lt;- Houses |&gt; filter(waterfront == \"Yes\")\nhead(New_Houses)\n\n# A tibble: 6 × 9\n     ID price sqft_living waterfront condition       sqft_lot bedrooms bathrooms\n  &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;fct&gt;              &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   541 2125         5403 Yes        good               24069        3      2.5 \n2  2141 1700         2500 Yes        very_good          15250        5      2   \n3  2445 3278.        6840 Yes        average or bel…    10000        2      1.75\n4  2627 4500.        6640 Yes        average or bel…    40014        5      5.5 \n5  4219 1875         3280 Yes        average or bel…    29111        3      2.5 \n6  4487 2000         4580 Yes        average or bel…     4443        5      4   \n# ℹ 1 more variable: yr_built &lt;dbl&gt;\n\n\nNext we’ll look at how to manipulate the data and create new variables.\n\n\nAdding a New Variable\nWe can use the mutate() function to create a new variable based on variables already in the dataset.\nLet’s add a variable giving the age of the house, as of 2015.\n\nHouses &lt;- Houses |&gt; mutate(age = 2015-yr_built)\nhead(Houses) |&gt; select(ID, price, sqft_living, waterfront, condition, yr_built, age)\n\n# A tibble: 6 × 7\n     ID price sqft_living waterfront condition        yr_built   age\n  &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;fct&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1    90  335.        1030 No         average or below     2006     9\n2   126 1450.        2750 No         average or below     1914   101\n3   300  268.        1590 No         average or below     1957    58\n4   541 2125         5403 Yes        good                 1976    39\n5   932  545         2520 No         average or below     1981    34\n6  1063  790.        2840 No         good                 1961    54",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch1.html#summary-statistics",
    "href": "Ch1.html#summary-statistics",
    "title": "1  Visualizing and Summarizing Data",
    "section": "1.2 Summary Statistics",
    "text": "1.2 Summary Statistics\n\n1.2.1 Measures of Center\nCommon ways to characterize the center of a distribution include mean, median, and mode.\nFor a set of \\(n\\) values \\(y_i, \\ldots, y_n\\):\n\nmean (\\(\\bar{y}\\)) represents the numerical average and is calculated by \\(\\bar{y} =\\frac{1}{n}\\displaystyle\\sum_{i=1}^n y_i\\).\nmedian represents the middle number when the values are arranged from least to greatest. If there are an even number of values in the dataset, then the median is given by the average of the middle two numbers.\n\nThe median of the upper half of the values is called the upper (or 3rd) quartile. This represents the 75th percentile in the distribution.\nThe median of the lower half of the values is called the lower (or 1st) quartile. This represents the 25th percentile in the distribution.\n\nmode is the most frequently occurring number in the data.\n\n\n\n1.2.2 Measures of Spread\nCommon ways of measuring the amount of spread, or variability, in a variable include:\n\nrange: the difference between the maximum and minimum values\ninterquartile range: the difference between the upper and lower quartiles (i.e. the range of the middle 50% of the values).\nstandard deviation (\\(s\\)): standard deviation is approximately the average deviation between an observation and the mean. It is calculated by\n\\(s =\\sqrt{\\displaystyle\\sum_{i=1}^n \\frac{(y_i-\\bar{y})^2}{n-1}}\\).\nThe square of the standard deviation, called the variance is denoted \\(s^2\\).\n\n\n\n1.2.3 Calcularing Summary Statistics in R\nLet’s calculate the mean, median, and standard deviation, in prices.\n\nHouses_Summary &lt;- Houses |&gt; summarize(Mean_Price = mean(price, na.rm=TRUE), \n                                          Median_Price = median(price, na.rm=TRUE), \n                                          StDev_Price = sd(price, na.rm = TRUE),\n                                          Number_of_Houses = n()) \nHouses_Summary\n\n# A tibble: 1 × 4\n  Mean_Price Median_Price StDev_Price Number_of_Houses\n       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;            &lt;int&gt;\n1       710.         458.        679.              200\n\n\nNotes:\n1. The n() command calculates the number of observations.\n2. The na.rm=TRUE command removes missing values, so that summary statistics can be calculated. It’s not needed here, since this dataset doesn’t include missing values, but if the dataset does include missing values, you will need to include this, in order to do the calculation.\nThe kable() function in the knitr() package creates tables with professional appearance.\n\nlibrary(knitr)\nkable(Houses_Summary)\n\n\n\n\nMean_Price\nMedian_Price\nStDev_Price\nNumber_of_Houses\n\n\n\n\n710.1975\n457.8125\n678.6034\n200\n\n\n\n\n\n\n\n1.2.4 Grouped Summaries\ngroup_by()\nThe group_by() command allows us to calculate summary statistics, with the data broken down by by category.We’ll compare waterfront houses to non-waterfront houses.\n\nHouses_Grouped_Summary &lt;- Houses |&gt; group_by(waterfront) |&gt; \n                                      summarize(Mean_Price = mean(price, na.rm=TRUE),\n                                                Median_Price = median(price, na.rm=TRUE), \n                                                StDev_Price = sd(price, na.rm = TRUE),\n                                                Number_of_Houses = n()) \nkable(Houses_Grouped_Summary)\n\n\n\n\nwaterfront\nMean_Price\nMedian_Price\nStDev_Price\nNumber_of_Houses\n\n\n\n\nNo\n552.6609\n430\n409.7904\n175\n\n\nYes\n1812.9540\n1700\n1073.8480\n25\n\n\n\n\n\nWe see that waterfront house tend to be more expensive, on average, and also have more variability (higher standard deviation) in price than non-waterfront houses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch1.html#data-visualization",
    "href": "Ch1.html#data-visualization",
    "title": "1  Visualizing and Summarizing Data",
    "section": "1.3 Data Visualization",
    "text": "1.3 Data Visualization\nNext, we’ll create graphics to help us visualize the distributions and relationships between variables. We’ll use the ggplot() function, which is part of the tidyverse package.\n\n1.3.1 Histogram\nHistograms are useful for displaying the distribution of a single quantitative variable. In a histogram, the x-axis breaks the variable into ranges of values, and the y-axis displays the number of observations with a value falling in that category (frequency).\nGeneral Template for Histogram\n\nggplot(data=DatasetName, aes(x=VariableName)) + \n  geom_histogram(fill=\"colorchoice\", color=\"colorchoice\") + \n  ggtitle(\"Plot Title\") +\n  xlab(\"x-axis label\") + \n  ylab(\"y-axis label\")\n\nHistogram of House Prices\n\nggplot(data=Houses, aes(x=price)) + \n  geom_histogram(fill=\"lightblue\", color=\"white\") + \n  ggtitle(\"Distribution of House Prices\") +\n  xlab(\"Price (in thousands)\") + \n  ylab(\"Frequency\")\n\n\n\n\n\n\n\n\nWe see that the distribution of house prices is right-skewed. Most houses cost less than $1,000,000, though there are a few houses that are much more expensive. The most common price range is around $400,000 to $500,000.\n\n\n1.3.2 Density Plot\nDensity plots show the distribution for a quantitative variable price. Scores can be compared across categories, like whether or not the house is on a waterfront.\nGeneral Template for Density Plot\n\nggplot(data=DatasetName, aes(x=QuantitativeVariable,\n                             color=CategoricalVariable, fill=CategoricalVariable)) + \n  geom_density(alpha=0.2) + \n  ggtitle(\"Plot Title\") +\n  xlab(\"Axis Label\") + \n  ylab(\"Frequency\") \n\nalpha, ranging from 0 to 1 dictates transparency.\nDensity Plot of House Prices\n\nggplot(data=Houses, aes(x=price, color=waterfront, fill=waterfront)) + \n  geom_density(alpha=0.2) + \n  ggtitle(\"Distribution of Prices\") +\n  xlab(\"House price (in thousands)\") + \n  ylab(\"Frequency\") \n\n\n\n\n\n\n\n\nWe see that on average, houses on the waterfront tend to be more expensive and have a greater price range than houses not on the waterfront.\n\n\n1.3.3 Boxplot\nBoxplots can be used to compare a quantitative variable with a categorical variable. The middle 50% of observations are contained in the “box”, with the upper and lower 25% of the observations in each tail.\nGeneral Template for Boxplot\n\nggplot(data=DatasetName, aes(x=CategoricalVariable, \n                             y=QuantitativeVariable)) + \n  geom_boxplot() + \n  ggtitle(\"Plot Title\") + \n  xlab(\"Variable Name\") + ylab(\"Variable Name\") \n\nYou can make the plot horizontal by adding + coordflip(). You can turn the axis text vertical by adding theme(axis.text.x = element_text(angle = 90)).\nBoxplot Comparing Price by Waterfront Status\n\nggplot(data=Houses, aes(x=waterfront, y=price)) + geom_boxplot() + \n  ggtitle(\"House Price by Waterfront Status\") + \n  xlab(\"Waterfront\") + ylab(\"Price (in thousands)\") + coord_flip()\n\n\n\n\n\n\n\n\nFor houses not on the waterfront, the median price is about $400,000, and the middle 50% of prices range from about $300,000 to $700,000.\nFor waterfront houses, the median price is about $1,700,000, and the middle 50% of prices range from about $900,000 to $2,200,000.\n\n\n1.3.4 Violin Plot\nViolin plots are an alternative to boxplots. The width of the violin tells us the density of observations in a given range.\nGeneral Template for Violin Plot\n\nggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable, \n                             fill=CategoricalVariable)) + \n  geom_violin() + \n  ggtitle(\"Plot Title\") + \n  xlab(\"Variable Name\") + ylab(\"Variable Name\") \n\nViolin Plot Comparing Prices by Waterfront\n\nggplot(data=Houses, aes(x=waterfront, y=price, fill=waterfront)) + \n  geom_violin() + \n  ggtitle(\"Price by Waterfront Status\") + \n  xlab(\"Waterfront\") + ylab(\"Price (in thousands)\") + \n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nAgain, we see that houses on the waterfront tend to be more expensive than those not on the waterfront, and have a wider range in prices.\n\n\n1.3.5 Scatterplot\nScatterplots are used to visualize the relationship between two quantitative variables.\nScatterplot Template\n\nggplot(data=DatasetName, aes(x=QuantitativeVariable, y=QuantitativeVariable)) + \n  geom_point() +\n  ggtitle(\"Plot Title\") + \n  ylab(\"Axis Label\") + \n  xlab(\"Axis Label\")\n\nScatterplot Comparing Price and Square Feet of Living Space\n\nggplot(data=Houses, aes(x=sqft_living, y=price)) + \n  geom_point() +\n  ggtitle(\"Price and Living Space\") + \n  ylab(\"Price (in thousands)\") + \n  xlab(\"Living Space in sq. ft. \")\n\n\n\n\n\n\n\n\nWe see that there is an upward trend, indicating that houses with more living space tend to, on average, be higher priced than those with less living space. The relationship appears to be roughly linear, though there might be some curvature, as living space gets very large. There are some exceptions to this trend, most notably a house with more than 7,000 square feet, priced over $1,500,000.\nWe can also add color, size, and shape to the scatterplot to display information about other variables.\nWe’ll use color to illustrate whether the house is on the waterfront, and size to represent the square footage of the entire lot (including the yard and the house).\n\nggplot(data=Houses, \n       aes(x=sqft_living, y=price, color=waterfront, size=sqft_lot)) + \n  geom_point() +\n  ggtitle(\"Price of King County Houses\") + \n  ylab(\"Price (in thousands)\") + \n  xlab(\"Living Space in sq. ft. \")\n\n\n\n\n\n\n\n\nWe notice that many of the largest and most expensive houses are on the waterfront.\n\n\n1.3.6 Bar Graph\nBar graphs can be used to visualize one or more categorical variables. A bar graph is similar to a histogram, in that the y-axis again displays frequency, but the x-axis displays categories, instead of ranges of values.\nBar Graph Template\n\nggplot(data=DatasetName, aes(x=CategoricalVariable)) + \n  geom_bar(fill=\"colorchoice\",color=\"colorchoice\")  + \n  ggtitle(\"Plot Title\") + \n  xlab(\"Variable Name\") + \n  ylab(\"Frequency\") \n\nBar Graph by Condition\n\nggplot(data=Houses, aes(x=condition)) + \n  geom_bar(fill=\"lightblue\",color=\"white\")  + \n  ggtitle(\"Number of Houses by Condition\") + \n  xlab(\"Condition\") + \n  ylab(\"Frequency\") +   \n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nWe see that the majority of houses are in average or below condition. Some are in good or very good condition, while a few are in very good condition.\n\n\n1.3.7 Stacked and Side-by-Side Bar Graphs\nStacked Bar Graph Template\n\nggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, \n                                         fill = CategoricalVariable2)) +\n    stat_count(position=\"fill\")  +\n  theme_bw() + ggtitle(\"Plot Title\") + \n  xlab(\"Variable 1\") + \n  ylab(\"Proportion of Variable 2\") +   \n  theme(axis.text.x = element_text(angle = 90)) \n\nStacked Bar Graph Example\nThe stat_count(position=\"fill\") command creates a stacked bar graph, comparing two categorical variables. Let’s explore whether waterfront status is related to condition.\n\nggplot(data = Houses, mapping = aes(x = waterfront, fill = condition)) +\n    stat_count(position=\"fill\")  +\n  theme_bw() + ggtitle(\"Condition by Waterfront Status\") + \n  xlab(\"Waterfront Status\") + \n  ylab(\"Condition\") +   \n  theme(axis.text.x = element_text(angle = 90)) \n\n\n\n\n\n\n\n\nWe see that a slightly higher proportion of waterfront houses are in good or very condition than non-waterfront houses.\nSide-by-side Bar Graph Template\nWe can create a side-by-side bar graph, using position=dodge.\n\nggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, \n                                         fill = CategoricalVariable2)) +\n    geom_bar(position = \"dodge\") +\n  ggtitle(\"Plot Title\") + \n  xlab(\"Genre\") + \n  ylab(\"Frequency\") \n\nSide-by-side Bar Graph Example\n\nggplot(data = Houses, mapping = aes(x = waterfront, fill = condition)) +\n    geom_bar(position = \"dodge\") +\n  ggtitle(\"Condition by Waterfront Status\") + \n  xlab(\"Waterfront Status\") + \n  ylab(\"Condition\") +   \n  theme(axis.text.x = element_text(angle = 90)) \n\n\n\n\n\n\n\n\nThe side by side bar graph conveys similar information to the stacked bar graph. It has the advantage of letting us see how many houses are in each category. We can tell that there are far more non-waterfront houses than waterfront ones. On the other hand, it is harder to compare the ratio of average, good, and very good houses with a side-by-side bar graph than it is with a stacked bar graph.\n\n\n1.3.8 Correlation Plot\nCorrelation plots can be used to visualize relationships between quantitative variables. Correlation is a number between -1 and 1, describing the strength of the linear relationship between two variables. Variables with strong positive correlations will have correlation close to +1, while variables with strong negative correlations will have correlations close to -1. Variables with little to no relationship will have correlation close to 0.\nThe cor() function calculates correlations between quantitative variables. We’ll use select_if to select only numeric variables. The `use=“complete.obs” command tells R to ignore observations with missing data.\n\ncor(select_if(Houses, is.numeric), use=\"complete.obs\") |&gt; round(2)\n\n               ID price sqft_living sqft_lot bedrooms bathrooms yr_built   age\nID           1.00  0.05        0.06     0.02    -0.07      0.11     0.21 -0.21\nprice        0.05  1.00        0.79     0.16     0.28      0.60     0.09 -0.09\nsqft_living  0.06  0.79        1.00     0.36     0.47      0.72     0.27 -0.27\nsqft_lot     0.02  0.16        0.36     1.00     0.14      0.17     0.00  0.00\nbedrooms    -0.07  0.28        0.47     0.14     1.00      0.48     0.07 -0.07\nbathrooms    0.11  0.60        0.72     0.17     0.48      1.00     0.42 -0.42\nyr_built     0.21  0.09        0.27     0.00     0.07      0.42     1.00 -1.00\nage         -0.21 -0.09       -0.27     0.00    -0.07     -0.42    -1.00  1.00\n\n\nThe corrplot() function in the corrplot() package provides a visualization of the correlations. Larger, thicker circles indicate stronger correlations.\n\nlibrary(corrplot)\nCorr &lt;- cor(select_if(Houses, is.numeric), use=\"complete.obs\")\ncorrplot(Corr)\n\n\n\n\n\n\n\n\nWe see that price has a strong positive correlation with square feet of living space, and is also positively correlated with number of bedrooms. Living space, bedrooms, and bathrooms are also positively correlated, which makes sense, since we would expect bigger houses to have more bedrooms and bathrooms. Price does not show much correlation with the other variables. We notice that square feet of living space is negatively correlated with age, which means older houses tend to be smaller than newer ones. Not surprisingly, age is very strongly correlated with year built.\n\n\n1.3.9 Scatterplot Matrix\nA scatterplot matrix is a grid of plots. It can be created using the ggpairs() function in the GGally package.\nThe scatterplot matrix shows us:\n\nAlong the diagonal are density plots for quantitative variables, or bar graphs for categorical variables, showing the distribution of each variable.\n\nUnder the diagonal are plots showing the relationships between the variables in the corresponding row and column. Scatterplots are used when both variables are quantitative, bar graphs are used when both variables are categorical, and boxplots are used when one variable is categorical, and the other is quantitative.\n\nAbove the diagonal are correlations between quantitative variables.\n\nIncluding too many variables can make these hard to read, so it’s a good idea to use select to narrow down the number of variables.\n\nlibrary(GGally)\nggpairs(Houses |&gt; select(price, sqft_living, condition, age))\n\n\n\n\n\n\n\n\nThe scatterplot matrix is useful for helping us notice key trends in our data. However, the plot can hard to read as it is quite dense, especially when there are a large number of variables. These can help us look for trends from a distance, but we should then focus in on more specific plots.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch1.html#practice-questions",
    "href": "Ch1.html#practice-questions",
    "title": "1  Visualizing and Summarizing Data",
    "section": "1.4 Practice Questions",
    "text": "1.4 Practice Questions\n\n1)\nThe Coasters contains information on roller coasters around the world. More information about the variables are available here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoaster\nState\nType\nHeight\nSpeed\nInversions\nDuration\nOpened\nAgeGroup\n\n\n\n\nTop Thrill Dragster\nOH\nSteel\n128\n194\nNo\n30\n2003\nNewest\n\n\nSuperman The Escape\nCA\nSteel\n126\n162\nNo\n28\n1997\nNewer\n\n\nMillennium Force\nOH\nSteel\n94\n151\nNo\n140\n2000\nNewest\n\n\nTitan\nTX\nSteel\n75\n138\nNo\n210\n2001\nNewest\n\n\nSilver Star\nD\nSteel\n73\n127\nNo\n240\n2002\nNewest\n\n\nGoliath\nCA\nSteel\n72\n138\nNo\n180\n2000\nNewest\n\n\n\n\n\n\n\n\nWhat are the observations in the dataset? What are the variables?\n\n\n\n\n\nState whether each variable is categorical or quantitative.\n\n\n\n\n2)\nThe graphic shows the distribution of US state populations in 2015.\n\n\n\n\n\n\n\n\n\n\na)\nA common mistake among introductory statistics students is to say that the largest state population is 14 million. Why might someone think this? Why is it incorrect?\n\n\nb)\nApproximately what is the median state population?\n\n\nc)\nHow will the mean state population compare to the median?\n\n\nd)\nHow many states had populations under 10 million?\n\n\n\n3)\nThe following histograms and boxplots provide information on the average monthly temperatures, in degrees Celsius, for the 24 months in 2015-16, in the cities of Moscow (Russia), Melbourne (Australia), and San Francisco (United States). Use the graphs to answer the following questions.\n\n\n\n\n\n\n\n\n\n\na)\nApproximately what was the median monthly temperature in each city?\n\n\nb)\nApproximately what is the range of the middle 50% of monthly temperatures in San Francisco?\n\n\nc)\nApproximately what percentage of monthly temperatures in Moscow were below freezing (0 deg. Celsius)?\n\n\nd)\nWhich of these cities would you expect to have the largest standard deviation among temperatures? Which would you expect to have the smallest standard deviation? Explain your answer in a way that makes clear that you understand what standard deviation represents in this context.\n\n\ne)\nHow would the mean monthly temperature in Melbourne compare to the median temperature? Would it be substantially higher, substantially lower, or about the same? Explain your answer.\n\n\nf)\nShown below are histograms of temperatures in each of the three cities. State which histogram corresponds to each city.\n\n\n\n\n\n\n\n\n\n\n\n\n4)\nThe American Cancer Society and National Cancer Institute distribute pamphlets to inform patients with cancer abut the nature of the disease and possible treatments. It is important that pamphlets be written at a reading level that patients will be able to understand.\nThe data represent reading levels of 63 adult cancer patients at the Philadelphia Veterans Affairs Medical Center, and the reading levels of 30 pamphlets available to them. (Source:https://ww2.amstat.org/publications/jse/v3n2/datasets.short.html)\n\n\n\n\n\n\n\n\n\nThe graph shows reading levels of patients and pamphlets along the horizontal axis and proportion of patients or pamphlets of that level on the vertical axis.\n\n\n\n\n\n\n\n\n\n\na)\nWhat is the median reading level among the cancer patients? What is the median reading level of the pamphlets?\n\n\nb)\nHow would the standard deviation in patients’ reading levels compare to the standard deviation in the pamphlet levels?\n\nc)\nAre pamphlets being written at the appropriate reading levels? Justify your answer.\n\n\n\n\n5)\nOne person stands outside the Warch Center (Lawrence University’s Student Center) at noon on a Monday and records the ages of every fifth person who walks in. Another person does the same thing at the Appleton airport. Which location is likely to have the larger mean age? Which is likely to have a larger standard deviation in age?\n\n\n6)\nFor each of the following, state whether the mean or median would likely be greater, or whether they would likely be about the same. Explain your reasoning.\n\na)\ndistribution of house prices in Wisconsin\n\n\n\nb)\ndistribution of heights of women in United States\n\n\n\nc)\ndistribution of age at time of death in United States\n\n\n\n7)\nA dataset contains the following numbers:\n3, 35, 37, 38, 41, 45, 46, 49, 52\nAfter inspecting the data, the recorder realizes there is a typo and the 3 is supposed to be a 33. How will the following statistics change when the typo is fixed (will they increase, decrease, or stay the same)?\n\na)\nMean\n\n\nb)\nMedian\n\n\nc)\nIQR\n\n\nd)\nStandard Deviation\n\n\n\n8)\nThe histograms show the distribution of quiz scores in four different statistics classes.\n\n\n\n\n\n\n\n\n\nThe following questions can be answered based on information in the graphs, without actually calculating mean or standard deviation from the formulas.\n\na)\nRank the classes from highest mean score to lowest. If any classes are tied, say this.\n\n\nb)\nWould the median score differ from the mean for any of the classes? If so, which one(s)?\n\n\nc)\nRank the classes from highest standard deviation in scores to lowest. If any classes are tied, say this.\n\n\n\n9)\nSuppose that a city is interested in estimating average household size among its residents. The city considers two different approaches for conducting this study.\nApproach 1: Select a random sample of 100 people and ask each person how many people live in their household.\nApproach 2: Select a random sample of 100 households and ask one person from each household how many people live in the household?\nHow will mean household size estimates will compare for the two approaches? Will one approach yield a larger estimate of household size? Why or why not?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizing and Summarizing Data</span>"
    ]
  },
  {
    "objectID": "Ch2.html",
    "href": "Ch2.html",
    "title": "2  Introduction to Statistical Models",
    "section": "",
    "text": "Learning Outcomes\nConceptual Learning Outcomes\n2. Use regression models to calculate estimates and predictions.\n3. Interpret coefficient estimates in regression models in context.\n4. Calculate and interpret regression sums of squares SSR, SSM, and SST, and coefficient of determination, \\(R^2\\), and F-statistics. \n5. Draw conclusions about model estimates, \\(R^2\\), and F-statistics using graphical representations of data. \n6. Explain the least squares estimation process.\nComputational Learning Outcomes\nE. Fit statistical models in R and interpret results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch2.html#fitting-models-to-data",
    "href": "Ch2.html#fitting-models-to-data",
    "title": "2  Introduction to Statistical Models",
    "section": "2.1 Fitting Models to Data",
    "text": "2.1 Fitting Models to Data\n\n2.1.1 Terminology\nIn this section, we’ll use statistical models to predict the prices of houses in King County, WA.\nIn a statistical model,\n\nThe variable we are trying to predict (price) is called the response variable (denoted \\(Y\\)).\nVariable(s) we use to help us make the prediction is(are) called explanatory variables (denoted \\(X\\)). These are also referred to as predictor variables or covariates.\n\nIn this section, we’ll attempt to predict the price of a house, using information about its size (in square feet), and whether or not it is on the waterfront. The price is our response variable, while size and waterfront location are explanatory variables.\n\nCategorical variables are variables that take on groups or categories, rather than numeric values, for example, whether or not the house is on the waterfront.\nQuantitative variables take on meaningful numeric values, for example the number of square feet in the house.\n\n\n\n2.1.2 Quantitative Explanatory Variable\nWe’ll first predict the price of the house, using the number of square feet of living space as our explanatory variable.\nA model with a single quantitative explanatory variable is called a simple linear regression (SLR) model.\nWe’ll assume that price changes linearly with square feet, and fit a trend line to the data.\n\nggplot(data=Houses, aes(x=sqft_living, y=price)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", se=FALSE) +\n  ggtitle(\"Price and Living Space\") + \n  ylab(\"Price\") + \n  xlab(\"Living Space in sq. ft. \") + \n  theme_bw()\n\n\n\n\n\n\n\n\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = b_0 + b_1\\times\\text{Sq.Ft.}\n\\]\nThe symbol over the response variable (Price) is read as “hat”, and means “predicted price”.\nWe fit the model in R, using the lm (linear model) command. The output gives the estimates of \\(b_0\\) and \\(b_1\\).\n\nM_House_price_sqft &lt;- lm(data=Houses, price~sqft_living)\nM_House_price_sqft\n\n\nCall:\nlm(formula = price ~ sqft_living, data = Houses)\n\nCoefficients:\n(Intercept)  sqft_living  \n  -364.8497       0.4641  \n\n\nThe estimates are \\(b_0=\\) -364.85 and \\(b_1=\\) 0.46.\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = -364.85  + 0.46\\times\\text{Sq.Ft.}\n\\]\nInterpretations\nThe intercept \\(b_0\\) represents the expected (or average) value of the response variable, when the explanatory variable is equal to 0. This is not always a meaningful interpretation in context.\nThe slope \\(b_1\\) represents the expected (or average) change in the response variable for each one-unit increase in the explanatory variable.\n\nOn average, a house with 0 square feet is expected to cost -365 thousand dollars. This is not a sensible interpretation, as there are no houses with 0 square feet.\nFor each additional square foot in living space, the price of the house is expected to increase by 0.46 thousand dollars (or $460).\n\n\nSince a 1 square ft. increase is very small, it makes more sense to give the interpretation in terms of a 100-square foot increase. For each additional 100 square feet in living space, the price of the house is expected to increase by 46 thousand dollars.\n\n\nPrediction\nWe can predict the price of a house with a given number of square feet by plugging the square feet into the model equation.\nThe predicted price of a house with 2,000 square feet is\n\\[\n\\widehat{\\text{Price}} = -364.85 + 0.46\\times 2000 = 563.41{ \\text{ thousand dollars}}\n\\]\nWe can calculate this directly in R using the predict command.\n\npredict(M_House_price_sqft, newdata=data.frame(sqft_living=2000))\n\n       1 \n563.4116 \n\n\nWe should only try to make predictions on houses within the range of the observed data. Since the largest house in the dataset is 8,000 square feet we should not try to predict the price of house with 10,000 square feet.\n\n\n2.1.3 Categorical Explanatory Variable\nNext, we’ll predict the price of a house based on whether or not it is on the waterfront.\nThe boxplot shows the distribution of prices for waterfront and nonwaterfront houses. The red dots indicate the mean.\n\nggplot(data=Houses, aes(x=waterfront, y=price)) + geom_boxplot() + \n  ggtitle(\"House Price by Waterfront Status\") + \n  xlab(\"Waterfront\") + \n  stat_summary(fun.y=\"mean\", geom=\"point\", shape=20, color=\"red\", fill=\"red\")  + \n  ylab(\"Price\") + \n  coord_flip() + \n  theme_bw() \n\n\n\n\n\n\n\n\nThe table displays the price summary by waterfront status.\n\nHouses_Grouped_Summary &lt;- Houses %&gt;% group_by(waterfront) %&gt;% \n                                      summarize(Mean_Price = mean(price, na.rm=TRUE),\n                                                Median_Price = median(price, na.rm=TRUE), \n                                                StDev_Price = sd(price, na.rm = TRUE),\n                                                Number_of_Houses = n()) \nkable(Houses_Grouped_Summary)\n\n\n\n\nwaterfront\nMean_Price\nMedian_Price\nStDev_Price\nNumber_of_Houses\n\n\n\n\nNo\n552.6609\n430\n409.7904\n175\n\n\nYes\n1812.9540\n1700\n1073.8480\n25\n\n\n\n\n\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = b_0 + b_1\\times\\text{Waterfront}\n\\]\nThe waterfront variable takes on value of 1 if the house is on the waterfront, and 0 otherwise.\n\nM_House_price_wf &lt;- lm(data=Houses, price~waterfront)\nM_House_price_wf\n\n\nCall:\nlm(formula = price ~ waterfront, data = Houses)\n\nCoefficients:\n  (Intercept)  waterfrontYes  \n        552.7         1260.3  \n\n\nThe estimates are \\(b_0=\\) 552.66 and \\(b_1=\\) 1260.29.\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = 552.66 + 1260.29\\times \\text{Waterfront}\n\\]\nInterpretations\nThe intercept \\(b_0\\) represents the expected (or average) value of the response variable in the “baseline” category (in this case non-waterfront).\nThe coefficient \\(b_1\\) represents the expected (or average) difference in response between the a category and the “baseline” category.\n\nOn average, a house that is not on the waterfront is expected to cost rM_House_price_wf_b0` thousand dollars.\nOn average a house that is on the waterfront is expected to cost 1260.29 thousand (or 1.26029 million) dollars more than a house that is not on the waterfront.\n\nPrediction\nWe can predict the price of a house with a given number of square feet by plugging in either 1 or 0 for the waterfront variable.\nThe predicted price of a house on the waterfront is:\n\\[\n\\widehat{\\text{Price}} = 552.66 + 552.66\\times 1 = 1812.95{ \\text{ thousand (or 553.92 million)}}\n\\]\nThe predicted price of a house not on the waterfront is:\n\\[\n\\widehat{\\text{Price}} = 552.66 + 552.66\\times 0 = 552.66{ \\text{ thousand}}\n\\]\nCalculations in R:\n\npredict(M_House_price_wf, newdata=data.frame(waterfront=\"Yes\"))\n\n       1 \n1812.954 \n\n\n\npredict(M_House_price_wf, newdata=data.frame(waterfront=\"No\"))\n\n       1 \n552.6609 \n\n\nNotice that the predicted prices for each category correspond to the average price for that category.\n\n\n2.1.4 Multiple Explanatory Variables\nWe’ve used square feet and waterfront status as explanatory variables individually. We can also build a model that uses both of these variables at the same time.\nA model with two or more explanatory variables is called a multiple regression model.\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = b_0 + b_1\\times\\text{Sq. Ft} + b_2\\times\\text{Waterfront}\n\\]\nFor a house not on the waterfront, the \\(\\text{Waterfront} = 0\\) so the model equation is:\n\\[\n\\widehat{\\text{Price}} = b_0  + b_1\\text{Sq. Ft}\n\\]\nFor a house on the waterfront, \\(\\text{Waterfront} = 1\\), so the model equation is:\n\\[\n\\widehat{\\text{Price}} = (b_0 + b_2) + b_1\\times\\text{Sq. Ft}\n\\]\nNotice that the slope is the same, regardless of whether the house is on the waterfront (\\(b_1\\)). The intercept, however, is different (\\(b_0\\) for houses not on the waterfront, and \\(b_0 + b_2\\) for houses on the waterfront). Thus, the model assumes that price increases at the same rate, with respect to square feet, regardless of whether or not it is on the waterfront, but allows the predicted price for a waterfront house to differ from a non-waterfront house of the same size.\nThe geom_parallel_slopes(se=FALSE) command adds regression lines with a common slope. This function is part of the moderndive R package.\n\nPlot_House_price_sqft_wf &lt;- ggplot(data=Houses, aes(x=sqft_living, y=price, color=waterfront)) + \n  geom_point() + \n  theme_bw() + \n  geom_parallel_slopes(se=FALSE) \n\nPlot_House_price_sqft_wf\n\n\n\n\n\n\n\n\nWe fit the model in R.\n\nM_House_price_sqft_wf &lt;- lm(data=Houses, price~sqft_living+waterfront)\nM_House_price_sqft_wf\n\n\nCall:\nlm(formula = price ~ sqft_living + waterfront, data = Houses)\n\nCoefficients:\n  (Intercept)    sqft_living  waterfrontYes  \n    -269.5118         0.3819       760.2965  \n\n\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = -269.51 + 0.38\\times\\text{Sq. Ft} + 760.3\\times\\text{Waterfront}\n\\]\nInterpretations\nThe intercept \\(b_0\\) represents the expected (or average) value of the response variable, when all quantitative explanatory variables are equal to 0, and all categorical variables are in the “baseline” category. This interpretation is not always sensible.\nWe interpret coefficients \\(b_j\\) for categorical or quantitative variables, the same way we would in a regression model with only one variable, but we need to state that all other explanatory variables are being held constant.\n\nOn average, a house that is not on the waterfront with 0 square feet is expected to cost -269.51 thousand dollars. This is not a sensible interpretation, since there are no houses with 0 square feet.\n\nFor each 1-square foot increase in size, the price of a house is expected to increase by 0.38 thousand (or 380) dollars, assuming waterfront status is the same. Equivalently, for each 100-square foot increase in size, the price of a house is expected to increase by 38 thousand dollars, assuming waterfront status is the same.\nOn average, a house on the waterfront is expected to cost 760.3 thousand dollars more than a house that is not on the waterfront, assuming square footage is the same.\n\nQuestion for thought: Notice that the estimated difference in price between waterfront and non-waterfront houses, after accounting for size, is 760.3, which is less than the estimated difference when we did not account for size, which was 1260.29. Why is this? Hint: look at the scatterplot displaying house price, size, and waterfront status.\nPrediction\nThe predicted price of a 2,000 square foot house on the waterfront is:\n\\[\n\\widehat{\\text{Price}} = -269.51 + 0.38\\times2000 + 760.3\\times1 = 1254.66{ \\text{ thousand (or 1.25 million)}}\n\\]\nThe predicted price of a 2,000 square foot house not on the waterfront is:\n\\[\n\\widehat{\\text{Price}} = -269.51 + 0.38\\times2000 = 494.37{ \\text{ thousand}}\n\\]\nCalculations in R:\n\npredict(M_House_price_sqft_wf, newdata=data.frame(waterfront=\"Yes\", sqft_living=2000))\n\n       1 \n1254.665 \n\n\n\npredict(M_House_price_sqft_wf, newdata=data.frame(waterfront=\"No\", sqft_living=2000))\n\n       1 \n494.3681 \n\n\n\n\n2.1.5 No Explanatory Variable\nFinally, we’ll consider a model that makes use of no explanatory variables at all. Although this might seem silly, its relevance will be seen in the next section.\nThe histogram shows the distribution of prices, without any information about explanatory variables. The mean price is indicated in red.\n\nggplot(data=Houses, aes(x=price)) + \n  geom_histogram(fill=\"lightblue\", color=\"white\") + \n  ggtitle(\"Distribution of House Prices\") + \n  theme_bw() + \n  xlab(\"Price\") + \n  ylab(\"Frequency\") + \n  geom_point(aes(x=mean(Houses$price), y=0), color=\"red\", shape=24, fill=\"red\")\n\n\n\n\n\n\n\n\nThe mean, median, and standard deviation in prices is shown below.\n\nlibrary(knitr)\nkable(Houses_Summary)\n\n\n\n\nMean_Price\nMedian_Price\nStDev_Price\nNumber_of_Houses\n\n\n\n\n710.1975\n457.8125\n678.6034\n200\n\n\n\n\n\nSuppose we know that a house sold in King County during this time, and want to predict the price, without knowing anything else about the house.\nThe best we can do is to use the mean price for our prediction. (We’ll define what we mean by “best” later in the chapter.)\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = b_0\n\\]\nWe fit a statistical model in R using the lm command.\n\nM_House_price_0 &lt;- lm(data=Houses, price ~ 1) # when there are no explanatory variables, use ~1\nM_House_price_0\n\n\nCall:\nlm(formula = price ~ 1, data = Houses)\n\nCoefficients:\n(Intercept)  \n      710.2  \n\n\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = 710.2\n\\]\nInterpretation\nThe expected price of a house in King County is 710.2 thousand dollars.\nPredictions\nWithout knowing anything about the house other than that it is in King County, WA we would expect it to cost 710.2 thousand dollars.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch2.html#variability-explained-by-a-model",
    "href": "Ch2.html#variability-explained-by-a-model",
    "title": "2  Introduction to Statistical Models",
    "section": "2.2 Variability Explained by a Model",
    "text": "2.2 Variability Explained by a Model\nWe’ve seen four different models for predicting house price. We want to be able to quantify how well the models are predicting prices, and determine which best explain variability in price.\nOf course we won’t know the price of the house we are trying to predict, so we can’t be sure how close or far our prediction is. We do, however, know the prices of the original 200 houses in our dataset. We can assess the models by measuring how far the actual prices of the 200 houses differ from the predicted (mean) price, and by calculating the proportion of total variation in sale price explained by each model.\n\n2.2.1 Total Variability\nLet’s start with our most basic model, which uses no explanatory variables and predicts the price of each simply using the average of all houses in the dataset.\nWe measure the total variability in the response variable by calculating the square difference between each individual response value and the overall average. This quantity is called the total sum of squares (SST).\n\\[\n\\text{SST} = \\displaystyle\\sum_{i=1}^n (y_i - \\bar{y})^2\n\\]\nThe plot below shows a horizontal line at the mean sale price (710.2 thousand). The points represent prices of individual houses, and the red lines represent the differences between the price of each house and the overall average.\n\n\n\n\n\n\n\n\n\nWe see that the majority of the houses are priced below the overall average, but some houses are much more expensive than average. A property of averages is that the positive and negative differences from the average will always cancel out and sum to zero.\nThe first three houses in the dataset are shown below.\n\nFirst3Houses &lt;- Houses %&gt;% select(ID, price, waterfront, sqft_living) %&gt;% head(3)\nkable(First3Houses)\n\n\n\n\nID\nprice\nwaterfront\nsqft_living\n\n\n\n\n90\n335.0\nNo\n1030\n\n\n126\n1450.0\nNo\n2750\n\n\n300\n267.5\nNo\n1590\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\text{SST} & = \\displaystyle\\sum_{i=1}^{100} (y_i - \\bar{y})^2 \\\\\n& = (335- 710)^2 + (1450- 710)^2 + (1450- 710)^2 + \\ldots\n\\end{aligned}\n\\]\nWe could calculate SST by hand for small datasets. For larger datasets, we’ll use R to perform the calculation.\n\nmeanprice &lt;- mean(Houses$price)  #calculate mean price\nSST &lt;- sum((Houses$price - meanprice)^2)  ## calculate SST\nSST\n\n[1] 91640004\n\n\nOr, equivalently,\n\nSST_M_House_price &lt;- sum((M_House_price_0$resid)^2)\nSST_M_House_price\n\n[1] 91640004\n\n\nBy itself, the size of SST does not have much meaning. We cannot say whether a SST value like the one we see here is large or small, since it depends on the size and scale of the variable being measured. An SST value that is very large in one context might be very small in another.\nSST does, however, give us a baseline measure of the total variability in the response variable. We’ll assess the performance of a model with a given explanatory variable by measuring how much of this variability the model accounts for.\n\n\n2.2.2 Residuals\nNow let’s consider our model that uses the size of the house in square feet as the explanatory variable. The figure on the left shows difference between actual and predicted prices, using this linear model. We compare the size of the differences to those resulting from the basic model that does not use any explanatory variables, and predicts each price using the overall average (shown on the right).\n\nP_Resid_M_House_price_sqft &lt;- ggplot(data=Houses, aes(x = sqft_living, y = price)) +  geom_point() +\n                geom_segment(aes(xend = sqft_living, yend = M_House_price_sqft$fitted.values), color=\"red\") +\n                geom_point(aes(y = M_House_price_sqft$fitted.values), shape = 1) +\n                stat_smooth(method=\"lm\", se=FALSE) + ylim(c(0,5000)) +\n                theme_bw() \n\n\n\n\n\n\n\n\n\n\nNotice that the red lines are shorter in the figure on the left, indicating the predictions are closer to the actual values.\nThe difference between the actual and predicted values is called the residual. The residual for the \\(ith\\) case is\n\\[\nr_i = (y_i-\\hat{y}_i)\n\\]\nWe’ll calculate the residuals for the first three houses in the dataset, shown below.\n\nkable(First3Houses)\n\n\n\n\nID\nprice\nwaterfront\nsqft_living\n\n\n\n\n90\n335.0\nNo\n1030\n\n\n126\n1450.0\nNo\n2750\n\n\n300\n267.5\nNo\n1590\n\n\n\n\n\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = -364.85 + 0.46\\times \\text{Sq. Ft}\n\\]\nThe predicted prices for these three houses are:\n\\[\n\\widehat{\\text{Price}_1} = -364.85 + 0.46\\times 1030 = 113.2 \\text{ thousand dollars}\n\\]\n\\[\n\\widehat{\\text{Price}_2} = -364.85 + 0.46\\times 2750 = 911.5 \\text{ thousand dollars}\n\\]\n\\[\n\\widehat{\\text{Price}_3} = -364.85 + 0.46\\times 1590 = 373.1 \\text{ thousand dollars}\n\\]\nTo calculate the residuals, we subtract the predicted price from the actual price.\n\\[r_1 = y_1-\\hat{y}_1 = 335 - 113.2 = 222  \\text{ thousand dollars}\\]\n\\[r_2 = y_2-\\hat{y}_2 = 1450 - 911.5 = 538 \\text{ thousand dollars}\\]\n\\[r_3 = y_3-\\hat{y}_2 = 267.5 - 373.1 = -106 \\text{ thousand dollars}\\]\nPositive residuals indicate that the house sold for more than the model predicted, while negative residuals mean the house sold for less than the model predicted.\nThe predicted values and residuals from a model can be calculated automatically in R. The predicted values and residuals for the first 3 houses are shown below.\n\nPredicted &lt;- predict(M_House_price_sqft)\nhead(Predicted, 3)\n\n       1        2        3 \n113.2049 911.5096 373.1180 \n\n\n\nResidual &lt;- M_House_price_sqft$residuals\nhead(Residual, 3)\n\n        1         2         3 \n 221.7951  538.4904 -105.6180 \n\n\n\n\n2.2.3 Variability Explained by Sq. Ft. Model\nThe sum of squared residuals (SSR) measures the amount of unexplained variability in the response variable after accounting for all explanatory variables in the model.\n\\[\n\\text{SSR} = \\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2.  \n\\]\nNote that SSR is similar to SST, except we subtract the model’s predicted values, rather than the overall average. In the special case of a model with no explanatory variables, the predicted values are equal to the overall average, so SSR is equal to SST.\nWe calculate SSR for the model using square feet as the explanatory variable.\n\\[\n\\begin{aligned}\n\\text{SSR} & = \\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2.  \\\\\n& = (335 - 113.2)^2 + (1450 - 911.5)^2 + (267.5 - 373.1)^2 + \\ldots\n\\end{aligned}\n\\]\nWe can calculate the model’s SSR directly in R.\n\nSSR_M_House_price_sqft &lt;- sum(M_House_price_sqft$residuals^2)\nSSR_M_House_price_sqft\n\n[1] 34576162\n\n\nSSR represents the amount of total variability in saleprice remaining after accounting for the house’s size in square feet.\nThe SSR=34576162.0524397 value is about one third of the SST value of 91640003.8131156. This means that about 2/3 of the total variability in sale price is explained by the model that accounts for sale price.\nThe difference (SST-SSR) represents the variability in the response variable that is explained by the model. This quantity is called the model sum of squares (SSM).\n\\[ \\text{SSM} = \\text{SST} - \\text{SSR} \\]\nIt can be shown that \\(\\text{SSM}=\\displaystyle\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2\\).\nThe proportion of total variability in the response variable explained by the model is called the coefficient of determination, denoted \\(R^2\\). We calculate this by dividing SSM by SST.\n\\[\nR^2=\\frac{SSM}{SST}= \\frac{SST-SSR}{SST}\n\\]\nExample: For the model with square feet as the explanatory variable,\n\\[\nSSM = SST-SSR = 91640004 - 34576162 = 57063842 .\n\\]\n\\[\nR^2 = \\frac{34576162}{91640004}=0.62.\n\\]\nApproximately 62.3% of the total variability in sale price is explained by the model using square feet as the explanatory variable.\n\n\n\n\n\n\n\n\n\nWe calculate \\(R^2\\) directly in R.\n\nsummary(M_House_price_sqft)$r.squared\n\n[1] 0.6226958\n\n\n\n\n2.2.4 Linear Correlation Coefficient\nFor models with a single quantiative explanatory varible, the coefficient of determination is equal to the square of the correlation coefficient \\(r\\), discussed in Chapter 1.\n\nggplot(data=Houses, aes(x=sqft_living, y=price)) + \n  geom_point() + \n  theme_bw() + \n  stat_smooth(method=\"lm\", se=FALSE) + \n  xlab(\"Square Feet\") + \n  ylab(\"Price (in thousands)\")\n\n\n\n\n\n\n\n\n\nFor linear models with a single quantitative variable, the linear correlation coefficient \\(r=\\sqrt{R^2}\\), or \\(r=-\\sqrt{R^2}\\) (with sign matching the sign on the slope of the line), provides information about the strength and direction of the linear relationship between the variables.\n\\(-1 \\leq r \\leq 1\\), and \\(r\\) close to \\(\\pm1\\) provides evidence of strong linear relationship, while \\(r\\) close to 0 suggests linear relationship is weak.\n\n\ncor(Houses$price, Houses$sqft_living)\n\n[1] 0.7891107\n\n\nThere is a positive and fairly strong relationship between price and square feet of a house.\nWhile the correlation coefficient, \\(r\\) is equal to the square root of \\(R^2\\) in the simple case of a model with one quantitative explanatory variable and a quantitative response variable, \\(R^2\\) has much broader application, as \\(r\\) is defined only for models with one quantitative explanatory variable and a quantitative response, while \\(R^2\\) is relevant for any linear model with a quantitative response variable.\n\n\n2.2.5 Variability Explained by Waterfront Model\nWe can similarly calculate the proportion of variability explained by the model using waterfront as an explanatory variable.\nRecall that in this model, the predicted price of a house with a waterfront is given by the average price of all waterfront houses, and the predicted price of a non-waterfront house is given by the average price of all non-waterfront houses.\nWe can calculate residuals using these predicted values, and compare them to the residuals resulting from a model with no explanatory variables, which uses the overall average price for all predictions.\nThe left two figures show the residuals resulting from a model that accounts for waterfront status. The figure on the right shows the residuals resulting from the model with no explanatory variables.\n\n\n\n\n\n\n\n\n\nNotice that after accounting for waterfront status, the differences between observed and predicted values are bigger than they were in the model that accounted for square feet, though not as big as for the model that doesn’t use any explanatory variables.\nWe use R to calculate SSR for the waterfront model.\n\nSSR_M_House_price_wf &lt;- sum(M_House_price_wf$residuals^2)\nSSR_M_House_price_wf\n\n[1] 56895095\n\n\n\\[\nSSM = SST-SSR = 91640004 - 56895095 = 34744909.\n\\]\n\\[\nR^2 = \\frac{34744909}{91640004}=0.38.\n\\]\nApproximately 37.9145647% of the total variability in sale price is explained by the model using waterfront status as the explanatory variable.\n\n\n\n\n\n\n\n\n\nWe calculate \\(R^2\\) directly in R.\n\nsummary(M_House_price_wf)$r.squared\n\n[1] 0.3791456\n\n\n\n\n2.2.6 Variability Explained by Multiple Regression Model\nWe’ve seen at the model using square feet accounts for about 2/3 of the total variability in house prices, while the model using waterfront status accounts for about 1/3 of the total variability. Let’s see if we can do better by using both variables together.\nThe left figure shows the residuals resulting from a model that accounts for both waterfront status and square feet. The figure on the right shows the residuals resulting from the model with no explanatory variables.\n\n\n\n\n\n\n\n\n\nWe use R to calculate SSR for the waterfront model.\n\nSSR_M_House_price_sqft_wf &lt;- sum(M_House_price_sqft_wf$residuals^2)\nSSR_M_House_price_sqft_wf\n\n[1] 23720774\n\n\n\\[\nSSM = SST-SSR = 91640004 - 23720774 = 67919230.\n\\]\n\\[\nR^2 = \\frac{67919230}{91640004}=0.74.\n\\]\nApproximately 74.1152631% of the total variability in sale price is explained by the model using square feet and waterfront status as the explanatory variables.\n\n\n\n\n\n\n\n\n\nWe calculate \\(R^2\\) directly in R.\n\nsummary(M_House_price_sqft_wf)$r.squared\n\n[1] 0.7411526\n\n\nIncluding both square feet and waterfront status allows us to explain more variability in sale price than models that include one but not both of these variables.\n\n\n2.2.7 Adding Age\nWe’ve seen that a multiple regression model that accounts for both square feet and whether or not the house is on the waterfront explains more variability in price than a model with only one of those variables. We can consider adding additional variables to the model to try to even better explain variability with price.\nLet’s consider adding the age of the house to the model.\nThe model equation is\n\\[\n\\widehat{\\text{Price}} = b_0 + b_1\\times\\text{Sq. Ft} + b_2\\times\\text{Waterfront} + b_3\\times\\text{Age}\n\\]\nSince we now have two quantitative and one categorical explanatory variable, in addition to our response variable, it is difficult to produce a graph that would visualize this model. We can, however, fit the model in R and calculate predicted values as well as model sums of squares, just as we’ve done before.\nEstimated model coefficients are shown below.\n\nM_House_price_sqft_wf_age &lt;- lm(data=Houses, price ~ sqft_living + waterfront + age)\nM_House_price_sqft_wf_age\n\n\nCall:\nlm(formula = price ~ sqft_living + waterfront + age, data = Houses)\n\nCoefficients:\n  (Intercept)    sqft_living  waterfrontYes            age  \n    -353.1583         0.3942       733.2522         1.4212  \n\n\nThe age coefficient estimate \\(b_3\\) tells us that for each additional year in age, the expected price is expected to increase by 1.4 thousand dollars, assuming square feet and waterfront status are held constant. This is not a very large change.\nWe can calculate predicted prices, residuals, SSR, SSM, and \\(R^2\\) the same way we did for the previous models, but let’s rely on R for these calculations.\nSSR:\n\nSSR_M_House_price_sqft_wf_age &lt;- sum((M_House_price_sqft_wf_age$residuals)^2) |&gt;round(1)\nSSR_M_House_price_sqft_wf_age\n\n[1] 23436092\n\n\nSSM:\n\nSSM_M_House_price_sqft_wf_age &lt;- SST_M_House_price - SSR_M_House_price_sqft_wf_age |&gt;round(1)\nSSM_M_House_price_sqft_wf_age\n\n[1] 68203912\n\n\n\\(R^2\\)\n\nsummary(M_House_price_sqft_wf_age)$r.squared\n\n[1] 0.7442592\n\n\nRecall that the model with square feet and waterfront, but not age, as explanatory variables had an \\(R^2\\) value of 0.7411526. Adding age to the model only resulted in a tiny increase in \\(R^2\\), meaning that age only explains a very small percentage of the variability in price that was not explained by size and waterfront status. Thus, it may not be beneficial to add age to a model the model.\nWhen a new variable is added SSR will never increase and \\(R^2\\) will never decrease. Thus, adding even unimportant variables typically lead to small decreases in \\(R^2\\). This does not necessarily mean we should add these variables. For one, thing this makes the model harder to interpret. It’s also true that making a model more complex by adding additional variables makes it more prone to a phenomenon called “overfitting.” This means that the model starts modeling random noise, rather than true relationships in the data it was built on (in this case, the 200 houses), causing it to perform worse when predicting new houses than a simpler model with fewer variables would. We’ll look more at overfitting later in the course.\nSometimes, it can be good to include a variable in a model even if it only leads to a small increase in \\(R^2\\). If we were interested in drawing and reporting conclusions on the relationship between the age and expected price of a house, it would be worth including age in the model, so we can report that it does not appear to make much difference. This result might be surprising and interesting in and of itself. There is no single correct model for a given set of data. We should choose models based on relationships we see in the data, as well as the research question(s) we are interested in answering.\n\n\n2.2.8 Summary: SST, SSR, SSM, \\(R^2\\)\n\nthe total variability in the response variable is the sum of the squared differences between the observed values and the overall average.\n\n\\[\\text{Total Variability in Response Var.}= \\text{SST} =\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y})^2\\]\n\nthe variability remaining unexplained even after accounting for explanatory variable(s) in a model is given by the sum of squared residuals. We abbreviate this SSR, for sum of squared residuals.\n\n\\[\n\\text{SSR} = \\text{Variability Remaining}=\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n\\]\n\nthe variability explained by the model, abbreviated SSM, is given by\n\n\\[ \\text{SSM} = \\text{SST} - \\text{SSR} \\]\n\nThe coefficient of determination (abbreviated \\(R^2\\)) is defined as\n\n\\[R^2=\\frac{\\text{Variability Explained by Model}}{\\text{Total Variability}}=\\frac{\\text{SSM}}{\\text{SST}} =\\frac{\\displaystyle\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2}{\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y})^2}\\]\nNote that some texts use different abbreviations than the ones used here. When working with resources outside this class, be sure to carefully check the notation being used.\nFor the model with a single quantitative explanatory variable.\n\n\n\n\n\n\n\n\n\nModel with a single categorical explanatory variable with 3 categories:\n\n\n\n\n\n\n\n\n\n\nBlue Area = Total Variability (SST)\nRed Area = Variability Remaining Unexplained by Model (SSR)\nBlue Area - Red Area = Variability Explained by Model (SSM)\n\\(R^2 = \\frac{\\text{Area of Blue Squares} - \\text{Area of Red Squares}}{\\text{Area of Blue Squares}} = \\frac{\\text{SST}-\\text{SSR}}{\\text{SST}}= \\frac{\\text{SSM}}{\\text{SST}}\\)\n\n\n\n2.2.9 Model Comparison Summary\n\n\n\n\n\n\n\n\n\n\nModel\nVariables\nUnexplained Variability\nVariability Explained\n\\(R^2\\)\n\n\n\n\n0\nNone\n91640004\n0\n0\n\n\n1\nSq. Ft.\n34576162\n57063842\n0.623\n\n\n2\nWaterfront\n56895095\n34744909\n0.379\n\n\n3\nSq. Ft. and Waterfront\n23720774\n67919230\n0.741\n\n\n4\nSq. Ft., Waterfront, and Age\n23436092\n68203912\n0.744\n\n\n\nComments on \\(R^2\\):\n\n\\(R^2\\) will never decrease when a new variable is added to a model.\n\nThis does not mean that adding more variables to a model always improves its ability to make predictions on new data.\n\n\\(R^2\\) measures how well a model fits the data on which it was built.\n\nIt is possible for a model with high \\(R^2\\) to “overfit” the data it was built from, and thus perform poorly on new data. We will discuss this idea extensively later in the course.\nOn some datasets, there is a lot of “natural” variability in the response variable, and no model will achieve a high \\(R^2\\). That’s okay. Even a model with \\(R^2 = 0.10\\) or less can provide useful information.\nThe goal is not to achieve a model that makes perfect predictions, but rather to be able to quantify the amount of uncertainty associated with the predictions we make.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch2.html#analysis-of-variance",
    "href": "Ch2.html#analysis-of-variance",
    "title": "2  Introduction to Statistical Models",
    "section": "2.3 ANalysis Of VAriance",
    "text": "2.3 ANalysis Of VAriance\n\n2.3.1 Sub-Models\nIn the preceding sections, we’ve seen 5 different models for predicting house price using some combination of square feet and waterfront status.\nA model is defined to be a submodel of a larger model, if the larger model contains all of the model’s variables plus more.\n\n\n\n\n\n\n\n\n\n\nModel\nVariables\nUnexplained Variability\nVariability Explained\n\\(R^2\\)\n\n\n\n\n0\nNone\n91640004\n0\n0\n\n\n1\nSq. Ft.\n34576162\n57063842\n0.623\n\n\n2\nWaterfront\n56895095\n34744909\n0.379\n\n\n3\nSq. Ft. and Waterfront\n23720774\n67919230\n0.741\n\n\n4\nSq. Ft., Waterfront, and Age\n23436092\n68203912\n0.744\n\n\n\n\nModel 1 is a submodel of Model 3, since all variables used in Model 1 are also used in Model 3.\nModel 2 is also a submodel of Model 3.\nModels 1, 2, and 3 are all submodels of Model 4.\nModel 0 is a submodel of Models 1, 2, 3, and 4.\nModels 1 and 2 are not submodels of each other, since Model 1 contains a variable used in Model 2 and Model 2 contains a variable not used in Model 1.\n\n\n\n2.3.2 F-Statistics\nWhen one model is a submodel of another, we can compare the amount of variability explained by the models, using a technique known as ANalysis Of VAriance (ANOVA).\nReduced Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_qx_{iq}\\)\nFull Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_qx_{iq} + b_{q+1}x_{i{q+1}} \\ldots + b_px_{ip}\\)\np = # terms in Full Model, not including the intercept\nq = # terms in Reduced Model, not including the intercept\nn = number of observations\nWe calculate a statistic called F that measures the amount of variability explained by adding additional variable(s) to the model, relative to the total amount of unexplained variability.\n\\[\n\\begin{aligned}\nF  \n&= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}}\n\\end{aligned}\n\\]\n\nLarge values of F indicate that adding the additional explanatory variables is helpful in explaining variability in the response variable\n\nSmall values of F indicate that adding new explanatory variables variables does not make much of a difference in explaining variability in the response variable\n\nWhat counts as “large” is depends on \\(n, p,\\) and \\(q\\). We will revisit this later in the course.\n\nExample 1\nLet’s Calculate an ANOVA F-Statistic to compare Models 1 and 3. This will tell us whether adding information about whether or not a house is on the waterfront helps us predict the price, when compared to a model using only square feet of living space.\nReduced Model:\n\\[\n\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft\\_living}\n\\]\nFull Model:\n\\[\n\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft\\_living}+ b_2\\times\\text{Waterfront}\n\\]\n\\[\n\\begin{aligned}\nF &= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\\n&=\\frac{\\frac{23720773.8648291 - 34576162.0524397}{2-1}}{\\frac{23720773.8648291}{200-(2+1)}} \\\\  \n& = 90.15\n\\end{aligned}\n\\]\nWe can calculate the statistic directly in R, using the anova command.\n\nanova(M_House_price_sqft_wf, M_House_price_sqft)$F[2] |&gt;round(2)\n\n[1] 90.15\n\n\nAn F-statistic of 90.15 is quite large and provides strong evidence that adding waterfront status to a model already including square feet helps better explain variability in sale price.\nWe previously saw that the model including both square feet and waterfront status had a \\(R^2\\) value considerably higher than the one including only square feet. This large F-statistic is further evidence of the benefit of using both variables in our model.\nExample 2\nWe’ll calculate an F-statistic to compare Models 3 and 4. This can help us determine whether it is worthwhile to include age in a model that already includes sqaure feet and waterfront status.\nReduced Model: \\[\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft\\_living} + b_2\\times\\text{Waterfront}\\]\nFull Model: \\[\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft\\_living}+ b_2\\times\\text{Waterfront} + b_3\\times\\text{Age}\\]\n\\[\n\\begin{aligned}\nF &= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\\n&=\\frac{\\frac{23436092 - 23720773.8648291}{3-2}}{\\frac{23720773.8648291}{200-(3+1)}} \\\\  \n& = 2.38\n\\end{aligned}\n\\]\nWe can calculate the statistic directly in R, using the anova command.\n\nanova(M_House_price_sqft_wf_age, M_House_price_sqft_wf)$F[2] |&gt;round(2)\n\n[1] 2.38\n\n\nNow, the F-statistic is much smaller than we’ve seen before, indicating that adding age to the model does not help very much in explaining variability in price.\nYou might be wondering how large an F-statistic needs to be in order to merit using the larger model. This is a complicated question, which we’ll explore more in future chapters, but as a general rule, F-statistics larger than 4 typically provide evidence in favor of choosing the larger (full) model over the reduced one.\n\n\n2.3.3 F-Statistic for Comparing Groups\nWhile F-statistics can be used for models with quantitative or categorical variables, they are often used to compare groups for a categorical explanatory variable.\nAn F-statistic compares the amount of variability between groups to the amount of variability within groups. Larger F-statistics indicate more variability between groups, rather than within groups, indicating stronger evidence of differences between groups.\nIn scenario 1, we notice considerable differences between the groups, relative to the amount of variability within groups. In this scenario, knowing the group an observation is in will help us predict the response for that group, so we should include account for the groups in our model. We would obtain a large F-statistic when comparing a model that includes group to one that contains only an intercept term.\nIn scenario 2, there is little difference between the overall averages in each group, and more variability between individual observations within each group. In a scenario like this, knowing the group an observation lies in does little to help us predict the response. In this scenario, predictions from a model that includes group as an explanatory variable would not be much better than those from a model that does not. Hence, we would obtain a small F-statistic.\n\n\n\n\n\n\n\n\n\n\nM_df1_F  &lt;- summary(lm(data=df1, y~Group))$fstatistic[1]\nM_df2_F  &lt;- summary(lm(data=df2, y~Group))$fstatistic[1]\n\n\n\n\n\nScenario 1\nScenario 2\n\n\n\n\nvariation between groups\nHigh\nLow\n\n\nvariation within groups\nLow\nHigh\n\n\nF Statistic\nLarge ($F=\\(46.5970149) | Small (\\)F=$M_df2_F)\n\n\n\nResult\nEvidence of Group Differences\nNo evidence of differences\n\n\n\nWe’ll use F-statistics to compare the prices of waterfront houses to non-waterfront houses, and also to compare the three different conditions of houses.\nExample 1: Price by Waterfront Status\nThe boxplot shows the distribution of houses in each category, and the table below it provides a numerical summary.\n\nggplot(data=Houses, aes(x=waterfront, y=price)) + \n  geom_boxplot(outliers=FALSE) +\n  geom_jitter() + \n  coord_flip() +  \n  stat_summary(fun.y=\"mean\", geom=\"point\", shape=20, color=\"red\", fill=\"red\") + \n  theme_bw() + ggtitle(\"Price by Waterfront Status\")\n\n\n\n\n\n\n\n\n\nCond_Tab &lt;- Houses %&gt;% group_by(waterfront) %&gt;% summarize(Mean_Price = mean(price), \n                                             SD_Price= sd (price), \n                                             N= n())\nkable(Cond_Tab)\n\n\n\n\nwaterfront\nMean_Price\nSD_Price\nN\n\n\n\n\nNo\n552.6609\n409.7904\n175\n\n\nYes\n1812.9540\n1073.8480\n25\n\n\n\n\n\nWhen measuring differences between groups for a categorical variable, we compare a model with the categorical variable to one with only an intercept term. In this case, the models are:\nReduced Model: \\[\\widehat{\\text{Price}}= b_0\\]\nFull Model: \\[\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{waterfront}\\]\nWe’ll fit the model in R. The coefficient estimates for \\(b_0\\), \\(b_1\\) and \\(b_2\\) are shown below.\nWe previously calculated \\(SSR_full\\) to be\n\nSSR_M_House_price_wf |&gt; round(1)\n\n[1] 56895095\n\n\nFor a model containing only an intercept term, like the reduced model, SSR is equal to SST.\n\nSST_M_House_price\n\n[1] 91640004\n\n\nThe F-statistic is\n\\[\n\\begin{aligned}\nF &= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\\n&=\\frac{\\frac{91640003.8131156 - 56895095.2689966}{1-0}}{\\frac{56895095.2689966}{200-(1+1)}} \\\\  \n& = 120.92\n\\end{aligned}\n\\]\nConfirming the calculation in R, we obtain\n\nanova(M_House_price_wf, M_House_price_0)$F[2]\n\n[1] 120.9154\n\n\nThe F-statistic is very large, indicating strong efidence that waterfront status is associated with the price of a house, which is consistent with what we’ve observed in our plots as well as \\(R^2\\) calculation.\nExample 2: Price by Condition\nOne variable in the houses dataset, which we haven’t looked at yet, is the condition of the house at the time of sale. The table shows the number of houses in either 1) very good, 2) good, or 3) average or below condition.\nThe boxplot shows the distribution of houses in each category, and the table below it provides a numerical summary.\n\nggplot(data=Houses, aes(x=condition, y=price)) + \n  geom_boxplot(outliers=FALSE) +\n  geom_jitter() + \n  coord_flip() +  \n  stat_summary(fun.y=\"mean\", geom=\"point\", shape=20, color=\"red\", fill=\"red\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCond_Tab &lt;- Houses %&gt;% group_by(condition) %&gt;% summarize(Mean_Price = mean(price), \n                                             SD_Price= sd (price), \n                                             N= n())\nkable(Cond_Tab)\n\n\n\n\ncondition\nMean_Price\nSD_Price\nN\n\n\n\n\naverage or below\n690.1892\n683.0578\n142\n\n\ngood\n729.0987\n671.5967\n47\n\n\nvery_good\n887.7273\n684.5189\n11\n\n\n\n\n\nNotice that while houses in very good condition are the most expensive, differences between the three conditions are not that large and there is a lot of variability within each condition group.\nWe’ll calculate an F-statistic for a model that includes condition, compared to a model with only an intercept term.\nReduced Model: \\[\\widehat{\\text{Price}}= b_0\\]\nFull Model: \\[\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{good condition}+ b_2\\times\\text{very good condition}\\]\nNotice that the equation includes separate variables for the “good” and “very” good conditions. These variables take on value 0 if the house is not in that condition, and 1 if the house is in that condition. Here, the “average or below” condition is considered the “baseline” category.\nWe’ll fit the model in R. The coefficient estimates for \\(b_0\\), \\(b_1\\) and \\(b_2\\) are shown below.\n\nM_House_price_cond &lt;- lm(data=Houses, price~condition)\nM_House_price_cond\n\n\nCall:\nlm(formula = price ~ condition, data = Houses)\n\nCoefficients:\n       (Intercept)       conditiongood  conditionvery_good  \n            690.19               38.91              197.54  \n\n\nInterpretations\n\nOn average, houses in average or below condition cost 690.19 thousand dollars.\n\nOn average, houses in good condition cost 38.91 thousand dollars more than those in average or below condition.\n\nOn average, houses in very good condition cost 197.54 thousand dollars more than those in average or below condition.\n\nWe calculate SSR for the full model.\n\nSSR_M_House_price_cond &lt;- sum(M_House_price_cond$residuals^2)\nSSR_M_House_price_cond  \n\n[1] 91219681\n\n\nNote that in this case, \\(p=2\\), since the full model includes terms for both good and very good conditions. For a categorical variable with \\(g\\) groups, there will always be \\(g-1\\) terms in the model equation, one for each category other than the baseline.\nThe F-statistic is\n\\[\n\\begin{aligned}\nF &= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\\n&=\\frac{\\frac{91640003.8131156 - 91219680.6197368}{2-0}}{\\frac{91219680.6197368}{200-(2+1)}} \\\\  \n& = 0.45\n\\end{aligned}\n\\]\nWe perform the calculation directly in R.\n\nanova(M_House_price_cond, M_House_price_0)$F[2]\n\n[1] 0.4538695\n\n\nNotice that the F-statistic of 0.45 is considerably smaller than the F-statistics we’ve seen previously.\nThis indicates that adding condition to a model with no other explanatory variables doesn’t seem to help improve the model’s ability to account for variation in price. Put another way, there doesn’t appear to be much evidence of difference in price between houses in the different conditions. This is again consistent with our conclusion based on the very small change in \\(R^2\\) that we saw when adding condition to the model in the previous section.\n\n\n2.3.4 Relationship Between F-statistic and \\(R^2\\)\nWe’ve seen that both \\(R^2\\) and the F-statistic give us ways of comparing models and determining whether explanatory variables help explain variability in the response.\nYou may be wondering why we need to calculate F-statistics when we already have \\(R^2\\) to answer these questions. Here we’ll look at an example to illustrate the relationship between the two, showing how they’re alike, as well as how they differ.\nShown below are are two different scatterplots displaying relationships between variables \\(x\\) and \\(y\\).\nWhich plot do you think displays a stronger relationship between \\(x\\) and \\(y\\)?\n\n\n\n\n\n\n\n\n\nWe calculate \\(R^2\\) values for linear models fit to both sets of data.\n\nM_dfA &lt;- lm(data=dfA, y1~x1)\nsummary(M_dfA)$r.squared \n\n[1] 0.6278261\n\n\n\nM_dfB &lt;- lm(data=dfB, y~x)\nsummary(M_dfB)$r.squared\n\n[1] 0.6441707\n\n\nWe see that the \\(R^2\\) values are approximately the same.\nYou might have thought, however, that Scenario B shows a stronger relationship, and this would seem reasonable. Scenario B has 91 observations, while Scenario A only has 4, hence Scenario B has more information justifying a relationship between the variables.\nThe \\(R^2\\) value captures only the strength of the relationship in the observed data, without accounting for the number of observations. In this section, we’ll look at a different statistic that can be used to assess the strength of relationships between variables in a model that, unlike \\(R^2\\), accounts for the sample size.\nFor each a model with explanatory variable \\(x\\) to one with only an intercept term.\nReduced Model: \\[\\widehat{y}= b_0\\]\nFull Model: \\[\\widehat{y}= b_0+ b_1 x\\]\nSince, since \\(p=1\\) and \\(\\text{SSR}_{\\text{Reduced}}=\\text{SST}\\), the F-statistic becomes:\n\\[\nF= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} = \\frac{\\frac{\\text{SST}-\\text{SSR}_{\\text{Full}}}{1-0}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(1+1)}}=\n\\frac{\\text{SSM}_{\\text{Full}}}{\\text{SSR}_{\\text{Full}}}\\times(n-2) =\\frac{R^2}{1-R^2}\\times(n-2)\n\\]\nThus, we see that the F-statistic is closely related to \\(R^2\\), but it’s calculation involves one thing that the \\(R^2\\) calculation does not - the sample size!\nSo, for dataset A in the illustration,\n\\[F= \\frac{0.6278}{`1 - r summary(M_dfA)$r.squared|&gt; round(4)`} \\times (4-2)` = 3.3738318\\],\nwhile for dataset B,\n\\[F= \\frac{0.6442}{`1 - r summary(M_dfB)$r.squared|&gt; round(4)`} \\times (4-2)` = 161.1199341\\].\nWhile the scenarios had similar \\(R^2\\) values, Scenario B had a much greater F-statistic, due to the larger sample, size which provides more evidence of a relationship.\nBoth the F-statistic and \\(R^2\\) are useful measures of a model fit. Neither is necessarily better than the other. The F-statistic accounts for sample size, while \\(R^2\\) simply tells us how strongly related the variables are in the data we have. It is good to consider both when building and assessing models.\n\n\n2.3.5 Comparing 3 or More Categories\nF-statistics are commonly used when making comparisons involving categorical variables with 3 or more categories.\nOne variable in the houses dataset, which we haven’t looked at yet, is the condition of the house at the time of sale. The table shows the number of houses in each condition listed.\n\nsummary(Houses$condition)\n\naverage or below             good        very_good \n             142               47               11 \n\n\nWe notice that there is only one house in poor condition and one house in fair condition. These sample sizes are too small to analyze. We’ll combine these two houses with those in the “average” category, creating a new category called “average or below).\n\n\n2.3.6 Alternate F-Stat Formula\nWe saw that F-statistics are useful for comparing categorical variables with multiple groups. The “variation between” over “variation within” interpretation of an F-statistic lends itself to another (mathematically equivalent) formula for the F-statistic. We calculate the ratio of variability between different groups, relative to the amount of variability within each group\nFor a categorical variable with \\(g\\) groups,\n\nlet \\(\\bar{y}_{1\\cdot}, \\ldots, \\bar{y}_{g\\cdot}\\) represent the mean response for each group.\nlet \\(n_1, \\ldots, n_g\\) represent the sample size for each group\nThen \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}\\) gives a measure of how much the group means differ, and\n\\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}\\) gives a measure of how much individual observations differ within groups\nAn alternative formula for this F-statistic is:\n\n\\[\nF= \\frac{\\text{Variability between groups}}{\\text{Variability within groups}}= \\frac{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}}{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}}\n\\]\nIt can be shown that this statistic is equivalent to the one we saw previously.\nFor models with only one categorical explanatory variable, “variability within vs variability between” interpretation of an F-statistic is frequently in the natural and social sciences. Such studies are often referred to as One-Way ANOVA’s. In fact, these are just a special case of the “full vs reduced” model interpretation of the F-statistic, which can be applied to any two models, as long as one is a submodel of the other. In this class, we’ll use the more general “full vs reduced” model formula, but you should be aware that this other formula is just a special case of the one we’ll use in this class.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch2.html#least-squares-estimation-lse",
    "href": "Ch2.html#least-squares-estimation-lse",
    "title": "2  Introduction to Statistical Models",
    "section": "2.4 Least Squares Estimation (LSE)",
    "text": "2.4 Least Squares Estimation (LSE)\n\n2.4.1 Estimating Regression Coefficients\nWe’ve already used R to determine the estimates of \\(b_0\\), \\(b_1\\), \\(b_2\\), and \\(b_3\\) in various kinds of linear models. At this point, it is natural to wonder where these estimates are come from.\nRegression coefficients \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that minimizes the sum of the squared differences between the observed and predicted values. That is, we minimize\n\\[\n\\text{SSR} = \\displaystyle\\sum_{i=1}^{n} (y_i-\\hat{y}_i)^2\n\\]\nBecause \\(\\hat{y}_i\\) is a function of \\(b_0, b_1, \\ldots, b_p\\), we can choose the values of \\(b_0, b_1, \\ldots, b_p\\) in a way that minimizes SSR.\n\\[\\text{SSR} = \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  = \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 \\]\nThe process of estimating regression coefficients \\(b_0, b_1, \\ldots, b_p\\) in a way that minimizes SSR is called least-squares estimation.\nExample: Model with one quantitative variable\nWe start with an example of estimating the regression coefficients for a model with a single explanatory variable. This is easy to illustrate, since we can draw a scatter plot displaying our explanatory and response variable.\nThe figure below illustrates four possible trend lines that could be fit to a set of 10 points in a scatter plot. The first line is the line of best fit, in that it makes the sum of the squared residuals the smallest of all possible lines that could be drawn. The second through fourth plots all show examples of other trend lines that are not the line of best fit. The sum of squared residuals for each of these models is bigger than for the first one.\nIn the illustration, SSR is represented by the total area of the squares. The line of best fit is the one that make the intercept the smallest.\n\n\n\n\n\n\n\n\n\n\nThis Rossman-Chance applet provides an illustration of the line of best fit.\n\nReturning to the model for predicting price of a house, using only size in square feet as an explanatory variable, the scatter plot, along with the slope and intercept of the regression line are shown below.\n\nggplot(data=Houses, aes(x=sqft_living, y=price)) + geom_point() + \n  stat_smooth(method=\"lm\", se=FALSE) + theme_bw()\n\n\n\n\n\n\n\n\n\nM_House_price_sqft\n\n\nCall:\nlm(formula = price ~ sqft_living, data = Houses)\n\nCoefficients:\n(Intercept)  sqft_living  \n  -364.8497       0.4641  \n\n\nThe line \\(\\text{Price} = -485 + 0.5328 \\times \\text{Square Feet}\\) is the “line of best fit” in the sense that it minimizes the sum of the squared residuals (SSR). Any other choices for the slope or intercept of the regression line would result in larger SSR than this line.\n\n\n2.4.2 Mathematics of LSE for SLR\n\nConsider a simple linear regression(SLR) model, which is one with a singe quantitative explanatory variable \\(x\\).\n\\(\\hat{y}_i = b_0+b_1x_i\\)\nwe need to choose the values of \\(b_0\\) and \\(b_1\\) that minimize:\n\n\\[\n\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n(y_i-(b_0+b_1x_i))^2\n\\]\nWe setup the equation by substituting in the values of \\(y_i\\) and \\(x_i\\) seen in the data.\nRecall the first 3 houses in the dataset:\n\nkable(First3Houses)\n\n\n\n\nID\nprice\nwaterfront\nsqft_living\n\n\n\n\n90\n335.0\nNo\n1030\n\n\n126\n1450.0\nNo\n2750\n\n\n300\n267.5\nNo\n1590\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\displaystyle\\sum_{i=1}^{100}(y_i-\\hat{y}_i)^2 & =\\displaystyle\\sum_{i=1}^n(y_i-(b_0+b_1x_i))^2 \\\\\n& = (1225-(b_0+b_1(5420)))^2 + (885-(b_0+b_1(2830)))^2 + (385-(b_0+b_1(1620)))^2 + \\ldots\n\\end{aligned}\n\\]\nWe need to find the values of \\(b_0\\) and \\(b_1\\) that minimize this expression. This is a 2-dimensional optimization problem that can be solved using multivariable calculus or numerical or graphical methods.\nUsing calculus, it can be shown that this quantity is minimized when\n\n\\(b_1=\\frac{\\displaystyle\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\displaystyle\\sum_{i=1}^{n}(x_i-\\bar{x})^2}=\\frac{\\displaystyle\\sum_{i=1}^{n} x_i y_i-\\frac{\\displaystyle\\sum_{i=1}^{n} x_i \\displaystyle\\sum_{i=1}^{n} y_i }{n}}{\\left(\\displaystyle\\sum_{i=1}^{n} x_i^2 -\\frac{\\left(\\displaystyle\\sum_{i=1}^{n} x_i\\right)^2}{n}\\right)}\\)\n\\(b_0=\\bar{y}-b_1\\bar{x}\\) (where \\(\\bar{y}=\\frac{\\displaystyle\\sum_{i=1}^{n}{y_i}}{n}\\), and \\(\\bar{x}=\\frac{\\displaystyle\\sum_{i=1}^{n}{x_i}}{n}\\)).\n\n\n\n2.4.3 LSE for Categorical Variable\n\nConsider a model with a single categorical variable (such as waterfront), with G+1 categories, numbered \\(g=0,2, \\ldots, G\\)\nThen \\(\\hat{y}_i = b_0 + b_1x_{i1} + \\ldots +b_{G}x_{iG}\\).\nwe need to minimize\n\n\\[\n\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n(y_i-(b_0 + b_1x_{i1} + \\ldots +b_{G}x_{iG}))^2.   \n\\]\n\nIt can be shown that this is achieved when\n\n\\(b_0 = \\bar{y_0}\\) (i.e. the average response in the “baseline group”), and\n\n\\(b_j = \\bar{y_j} - \\bar{y}_0\\)\n\n\n\n\n2.4.4 LSE More Generally\n\nFor multiple regression models, including those involving interaction, the logic is the same. We need to choose \\(b_0, b_1, \\ldots, b_p\\) in order to minimize\n\n\\[ \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  = \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 \\]\n\nThe mathematics, however, are more complicated and require inverting a matrix. This goes beyond the scope of this class, so we will let R do the estimation and use the results.\nMore on least squares estimation in multiple regression can be found here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch2.html#practice-questions",
    "href": "Ch2.html#practice-questions",
    "title": "2  Introduction to Statistical Models",
    "section": "2.5 Practice Questions",
    "text": "2.5 Practice Questions\n\n1)\nWe’ll work with the data set on 146 roller coasters from around the world. We’ll attempt to predict the speed of a coaster (in mph).\nSummary information on the speeds of coasters is shown below.\n\nsummary(Coasters$Speed)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   50.00   57.00   59.11   66.00  120.00 \n\n\n\na)\nConsider a model of the form:\nModel 0:\n\\[\n\\widehat{\\text{Speed}} = b_0\n\\]\nWhat is the value of \\(b_0\\)?\n\n\nb)\nSpeeds are broken down by type of coaster (steel or wooden).\n\nggplot(data=Coasters, aes(x=Type, y=Speed)) + \n  geom_boxplot() +\n  coord_flip() +  \n  stat_summary(fun.y=\"mean\", geom=\"point\", shape=20, color=\"red\", fill=\"red\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCoasters |&gt; group_by(Type) |&gt; summarize(MeanSpeed = round(mean(Speed, na.rm=TRUE),1), Number=n())\n\n# A tibble: 2 × 3\n  Type   MeanSpeed Number\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;int&gt;\n1 Steel       61.5     83\n2 Wooden      53.7     36\n\n\nModel 1:\n\\[\n\\widehat{\\text{Speed}} = b_0 + b_1\\times{\\text{TypeWood}}\n\\]\nWhat are the values of \\(b_0\\) and \\(b_1\\)?\n\n\n\n2)\nWe’ll fit a model using both type and duration (in seconds) as explanatory variables.\n\\[\n\\widehat{\\text{Speed}} = b_0  + b_1\\times{\\text{TypeWooden}} + b_2\\times\\text{Duration}\n\\]\nA scatterplot corresponding to this model is shown again below.\n\nggplot(data=Coasters, aes(x=Duration, y=Speed, color=Type))+\n          geom_point() + \n          xlab(\"Duration in Seconds\") + \n          ylab(\"Top Speed in MPH\") +\n          geom_parallel_slopes(se=FALSE) +\n          ylim(c(0,125)) +\n          xlim(c(0,250)) + \n          theme_bw()\n\n\n\n\n\n\n\n\nApproximately what are the values of \\(b_0\\), \\(b_1\\), and \\(b_2\\)? (Hint: you may need to extend the lines to answer one or more of these.)\n\n\n3)\nContinuing with the roller coaster dataset, suppose we want to predict speed of the coaster (in mph), using either design or duration (in seconds) as explanatory variables.\n\na)\nLet’s look at the number of coasters with each design type.\n\nsummary(Coasters$Design)\n\n4th Dimension        Flying      Inverted      Pipeline      Sit Down \n            0             2            15             1            95 \n     Stand Up     Suspended          Wing \n            5             1             0 \n\n\nWhy might using design in this form in a statistical model be problematic?\n\n\nb)\nContinuing with the roller coasters data, we’ll modify the design variable to categorize each coaster as either “Sit Down” or “Other”.\n\nCoasters &lt;- Coasters |&gt;\n              mutate(Design = case_when(\n                              Design %in% c(\"Sit Down\") ~ \"Sit Down\",\n                              TRUE ~ \"Other\")) # Default for remaining categories\n\nCoasters |&gt; select(Coaster, Park, Design, Duration, Speed) |&gt; head()\n\n        Coaster                       Park   Design Duration Speed\n1 Zippin Pippin                Libertyland Sit Down       90    40\n2   Jack Rabbit             Kennywood Park Sit Down       96    45\n3   Thunderhawk                Dorney Park Sit Down       78    45\n4  Giant Dipper Santa Cruz Beach Boardwalk Sit Down      112    55\n5   Thunderbolt             Kennywood Park Sit Down      101    55\n6       Wildcat             Lake Compounce Sit Down       75    48\n\n\nShown below are scatterplots displaying the relationship between duration and speed, colored by design type.\nNotice that in Plot A, there are 2 coasters with very fast speeds, of 100 mph or more. These appear to be outliers. In plot B, the outliers are removed from the dataset. Parallel regression lines for each type of coaster are fit in each plot.\n\nCoasters_filtered &lt;- Coasters |&gt; filter(Speed &lt; 100)\nCoasters_filtered |&gt; select(Coaster, Park, Max_Height, Design, Speed) |&gt; head()\n\n        Coaster                       Park Max_Height   Design Speed\n1 Zippin Pippin                Libertyland         70 Sit Down    40\n2   Jack Rabbit             Kennywood Park         40 Sit Down    45\n3   Thunderhawk                Dorney Park         80 Sit Down    45\n4  Giant Dipper Santa Cruz Beach Boardwalk         70 Sit Down    55\n5   Thunderbolt             Kennywood Park         70 Sit Down    55\n6       Wildcat             Lake Compounce         85 Sit Down    48\n\n\n\nPlotA &lt;- ggplot(data=Coasters, aes(x=Duration, y=Speed, color=Design))+\n          geom_point() + \n          xlab(\"Duration in Seconds\") + \n          ylab(\"Top Speed in MPH\") + \n          geom_parallel_slopes(se=FALSE) +\n          ggtitle(\"Plot(A) - Including Outliers\") +\n          ylim(c(0,125)) + \n          xlim(c(0,250)) + \n          theme_bw()\n\n\nPlotB &lt;- ggplot(data=Coasters_filtered, aes(x=Duration, y=Speed, color=Design))+\n          geom_point() + \n          xlab(\"Duration in Seconds\") + \n          ylab(\"Top Speed in MPH\") +\n          geom_parallel_slopes(se=FALSE) +\n          ggtitle(\"Plot(B) - Removing Outliers\") + \n          ylim(c(0,125)) +\n          xlim(c(0,250)) + \n          theme_bw()\n\n\ngrid.arrange(PlotA, PlotB, ncol=2)\n\n\n\n\n\n\n\n\nHow does removing the outliers affect the slope and intercepts of the regression lines relating speed and duration for each type of coaster? Which plot do you think more accurately captures the relationship between speed and duration? Explain your answer.\n\n\n\n4)\nWe’ll continue with the coasters_filtered dataset that excludes the two outliers, and continue to classify each coaster design as either “sit-down” or “other”. In each part, you are asked to interpret model estimates. State the numerical value of each estimate specifically in your interpretation, as is done in Section 2.1 of the class notes.\n\na)\nConsider a model of the form\nModel 0:\n\\[\n\\widehat{\\text{Speed}} = b_0\n\\] The coefficient estimate \\(b_0\\) for this model is shown below.\n\nM_Coasters_speed_0 &lt;- lm(data=Coasters_filtered, Speed ~ 1)\nM_Coasters_speed_0\n\n\nCall:\nlm(formula = Speed ~ 1, data = Coasters_filtered)\n\nCoefficients:\n(Intercept)  \n      58.24  \n\n\nWrite a sentence interpreting the estimate \\(b_0\\) in context.\n\n\nb)\nNow consider a model of the form\nModel 1:\n\\[\n\\widehat{\\text{Speed}} = b_0 + b_1\\times\\text{Duration}\n\\] Estimates of \\(b_0\\) and \\(b_1\\) are shown below.\n\nM_Coasters_speed_dur &lt;- lm(data=Coasters_filtered, Speed ~ Duration)\nM_Coasters_speed_dur\n\n\nCall:\nlm(formula = Speed ~ Duration, data = Coasters_filtered)\n\nCoefficients:\n(Intercept)     Duration  \n    43.8001       0.1129  \n\n\nWrite sentences interpreting the estimates \\(b_0\\) and \\(b_1\\) in context.\n\n\nc)\nNow consider a model of the form\nModel 2:\n\\[\n\\widehat{\\text{Speed}} = b_0 + b_1\\times\\text{DesignSitDown}\n\\]\nEstimates of \\(b_0\\) and \\(b_1\\) are shown below.\n\nM_Coasters_speed_des &lt;- lm(data=Coasters_filtered, Speed ~ Design)\nM_Coasters_speed_des\n\n\nCall:\nlm(formula = Speed ~ Design, data = Coasters_filtered)\n\nCoefficients:\n   (Intercept)  DesignSit Down  \n         55.96            2.87  \n\n\nWrite sentences interpreting the estimates \\(b_0\\) and \\(b_1\\) in context.\n\n\nd)\nFinally, consider a model of the form\nModel 3:\n\\[\n\\widehat{\\text{Speed}} = b_0 + b_1\\times\\text{Duration} +  b_2\\times\\text{DesignSitDown}\n\\]\nEstimates of \\(b_0\\), \\(b_1\\), and \\(b_2\\) are shown below.\n\nM_Coasters_speed_dur_des &lt;- lm(data=Coasters_filtered, Speed ~ Duration + Design)\nM_Coasters_speed_dur_des\n\n\nCall:\nlm(formula = Speed ~ Duration + Design, data = Coasters_filtered)\n\nCoefficients:\n   (Intercept)        Duration  DesignSit Down  \n       40.4307          0.1159          3.7580  \n\n\nWrite sentences interpreting the estimates \\(b_0\\), \\(b_1\\), and \\(b_2\\) in context.\n\n\n\n5)\nContinuing with the model in 4(d), we’ll predict the speed of a roller coaster using duration and whether it has a sit down design as explanatory variables. The model is\n\\[\n\\widehat{\\text{Speed}} = b_0 + b_1\\times\\text{Duration} +  b_2\\times\\text{DesignSitDown}\n\\] A scatterplot displaying the relationship is shown below.\n\nPlotB + ggtitle(\"Speed by Duration and Design\")\n\n\n\n\n\n\n\n\nEstimates of \\(b_0\\), \\(b_1\\), and \\(b_2\\) are.\n\nM_Coasters_speed_dur_des &lt;- lm(data=Coasters_filtered, Speed ~ Duration + Design)\nM_Coasters_speed_dur_des\n\n\nCall:\nlm(formula = Speed ~ Duration + Design, data = Coasters_filtered)\n\nCoefficients:\n   (Intercept)        Duration  DesignSit Down  \n       40.4307          0.1159          3.7580  \n\n\n\na)\nUse the model to calculate the predicted speed of a sit down coaster with duration 150 seconds.\n\n\nb)\nExplain how you could make the prediction in (a) from the scatterplot. Is your calculated speed consistent with the plot?\n\n\nc)\nGiven that not all coasters of the same type and duration have the same speed, we cannot be sure that a sit down coaster with duration 150 seconds will have exactly the predicted speed. Using the scatterplot, give a range of speeds that you can be confident will contain the speed of an individual sit down coaster with duration 150 seconds.\n\n\nd)\nSuppose that Coasters A and B are both sit down coasters and that Coaster A has a duration 10 seconds longer than B. Which coaster has the faster predicted speed, and by how much?\n\n\ne)\nSuppose that Coasters C and D have the same duration, and that Coaster C is a sit down coaster, and D isn’t. Which coaster has the faster predicted speed, and by how much?\n\n\n\n6)\nContinuing with the model in Question 5, excluding outliers, the \\(R^2\\) value is shown below.\n\nsummary(M_Coasters_speed_dur_des)$r.squared\n\n[1] 0.1410659\n\n\n\na)\nWrite a sentence interpreting this value in context.\n\n\nb)\nWe add year built (since 1900) as an explanatory variable. The \\(R^2\\) value is shown below.\n\nM_Coasters_speed_dur_des_yr &lt;- lm(data=Coasters_filtered, Speed ~  Duration + Design + Year_Since1900)\n\nsummary(M_Coasters_speed_dur_des_yr)$r.squared\n\n[1] 0.2480657\n\n\nDoes adding year built appear to help predict the speed of a coaster? Explain your answer.\n\n\nc)\nWe now drop design from the model and use only duration as an explanatory variable. The \\(R^2\\) value is shown below.\n\nM_Coasters_speed_dur &lt;- lm(data=Coasters_filtered, Speed ~ Duration)\nsummary(M_Coasters_speed_dur)$r.squared\n\n[1] 0.1266004\n\n\nDoes dropping design from the model hurt our ability to predict speed? Explain your answer.\n\n\n\n7)\nThe scatterplot shows the relationship between top speed of a coaster (in mph) and its maximum height and design type. We again exclude the two outlier coasters.\n\nggplot(data=Coasters_filtered, aes(y=Speed, x=Max_Height, color=Design)) + \n  geom_point() + \n  xlab(\"Top Speed in mph\") + \n  ylab(\"Max Height in Feet\") + \n  geom_parallel_slopes(se=FALSE)+\n  ggtitle(\"Speed by Height and Type\") + \n  ylim(c(0,125)) +\n  xlim(c(0,315)) + \n  theme_bw()\n\n\n\n\n\n\n\n\nWe’ll use the model\n\\[\n\\widehat{\\text{Speed}} = b_0 + b_1\\times\\text{Max Height} + b_2\\times{\\text{DesignSitDown}}\n\\]\nThe model estimates are shown below.\n\nM_Coasters_speed_height_des &lt;- lm(data=Coasters_filtered, Speed ~  Max_Height + Design)\nM_Coasters_speed_height_des\n\n\nCall:\nlm(formula = Speed ~ Max_Height + Design, data = Coasters_filtered)\n\nCoefficients:\n   (Intercept)      Max_Height  DesignSit Down  \n       29.5120          0.1979          4.1504  \n\n\nShould we expect the model that predicts speed using height as an explanatory variable to have a higher or lower \\(R^2\\) value than a model that uses duration as an explanatory variable (hint: compare this scatterplot to the one in Question 5)?\n\n\n8)\nContinuing with the roller coasters data, now consider a model of the form\n\\[\n\\widehat{\\text{Speed}} = b_0 + b_1\\times\\text{Duration}\n\\]\nR commands and output related to this model are shown below.\n\nMean_Speed &lt;- mean(Coasters_filtered$Speed)\nMean_Speed\n\n[1] 58.23932\n\n\n\nsum((Coasters$Speed - Mean_Speed)^2)\n\n[1] 24069.64\n\n\n\nM_Coasters_speed_dur &lt;- lm(data=Coasters_filtered, Speed ~ Duration)\nsum(M_Coasters_speed_dur$residuals^2)\n\n[1] 16167.76\n\n\nWhat are the values of SST, SSR, SSM, and \\(R^2\\)?\n\n\n9)\nThe scatterplot shows a response variable (y), plotted against a quantitative explanatory variable (x), with a categorical explanatory variable, (either A or B) indicated by color and shape.\n\n\n\n\n\n\n\n\n\nConsider two models. Model 1 accounts for only the categorical variable, while Model 2 accounts for both the quantitative variable \\(x\\) and the categorical variable.\nModel 1:\n\\[\n\\hat{y} = b_0 + b_1x\n\\]\nModel 2:\n\\[\n\\hat{y} = b_0 + b_1\\text{CategoryB}\n\\]\nModel 3:\n\\[\n\\hat{y} = b_0 + b_1\\text{CategoryB} + b_2x\n\\]\n\na)\nWithout calculating \\(R^2\\), state approximately what \\(R^2\\) would be for each of Models 1, 2, and 3. Choose from:\n\nBetween 0 and 0.25\n\nBetween 0.25 and 0.50\n\nBetween 0.50 and 0.75\n\nBetween 0.75 and 1\n\nJustify your answers.\n\n\nb)\nBetween Models 2 and 3, which will have a greater estimate of \\(b_1\\)? Justify your answer.\n\n\nc)\nBetween Models 1 and 3, in which model will the coefficient on the variable \\(x\\) be more extreme? (In Model 1, the coefficient on \\(x\\) is \\(b_1\\) and in Model 3, it is \\(b_2\\).) Justify your answer.\n\n\n\n10)\nThe illustrations below show three different scenarios, each with a categorical explanatory variable with three levels. The red dots indicate the mean for each group.\nSuppose we calculate F-statistics comparing a model of the form:\n\\[\\hat{y} = b_0\\]\nto one of the form:\n\\[\\hat{y} = b_0 + b_1\\text{GroupB}+ b_2\\text{GroupC}\\]\n\n\n\n\n\n\n\n\n\nWhich scenario will have the largest F-statistic? Which will have the smallest? Explain your answer.\n\n\n11)\nThe four models for roller coaster speeds in Question 3 are shown again below. SSR’s for each model are also show. There are 117 roller coasters in the dataset.\nModel 0:\n\\[\n\\widehat{\\text{Speed}} = b_0\n\\]\n\nM_Coasters_speed_0 &lt;- lm(data=Coasters_filtered, Speed ~ 1)\nsum(M_Coasters_speed_0$residuals^2)\n\n[1] 18511.3\n\n\nModel 1:\n\\[\n\\widehat{\\text{Speed}} = b_0 + b_1\\times\\text{Duration}\n\\]\n\nM_Coasters_speed_dur &lt;- lm(data=Coasters_filtered, Speed ~ Duration)\nsum(M_Coasters_speed_dur$residuals^2)\n\n[1] 16167.76\n\n\nModel 2:\n\\[\n\\widehat{\\text{Speed}} = b_0 + b_1\\times\\text{DesignSitDown}\n\\]\n\nM_Coasters_speed_des &lt;- lm(data=Coasters_filtered, Speed ~ Design)\nsum(M_Coasters_speed_des$residuals^2)\n\n[1] 18354.21\n\n\nModel 3:\n\\[\n\\widehat{\\text{Speed}} = b_0 + b_1\\times\\text{Duration} +  b_2\\times\\text{DesignSitDown}\n\\]\n\nM_Coasters_speed_dur_des &lt;- lm(data=Coasters_filtered, Speed ~ Duration + Design)\nsum(M_Coasters_speed_dur_des$residuals^2)\n\n[1] 15899.99\n\n\nCalculate F-statistics for comparing each of the following pairs of models, or explain why it would be inappropriate to do so.\n\na)\nModel 0 and Model 1\n\n\nb)\nModel 0 and Model 2\n\n\nc)\nModel 1 and Model 2\n\n\nd)\nModel 1 and Model 3\n\n\ne)\nModel 2 and Model 3\n\n\n\n12)\nContinue with the models in Question 11.\n\na)\nThe F-statistic for comparing Model 2 to Model 3 is shown below. Based on the size of this F-statistic, what can we conclude about the variable(s) in these models?\n\nanova(M_Coasters_speed_dur, M_Coasters_speed_0)$F[2] |&gt; round(1)\n\n[1] 16.7\n\n\n\n\nb)\nThe F-statistic for comparing Model 2 to Model 3 is shown below. Based on the size of this F-statistic, what can we conclude about the variable(s) in these models?\n\nanova(M_Coasters_speed_dur_des, M_Coasters_speed_dur)$F[2] |&gt; round(1)\n\n[1] 1.9\n\n\n\n\n\n13)\nThe plots show three different lines fit to the same sets of points.\n\n\n\n\n\n\n\n\n\nWhich represents the least squares regression line? Show calculations to justify your answer.\n\n\n14)\nWe saw in Question 5 that for a model of the form\n\\[\n\\widehat{\\text{Speed}} = b_0 + b_1\\times\\text{Duration} +  b_2\\times\\text{DesignSitDown}\n\\]\nthe estimates of \\(b_0\\), \\(b_1\\), and \\(b_2\\) produced by R are\n\nM_Coasters_speed_dur_des\n\n\nCall:\nlm(formula = Speed ~ Duration + Design, data = Coasters_filtered)\n\nCoefficients:\n   (Intercept)        Duration  DesignSit Down  \n       40.4307          0.1159          3.7580  \n\n\nSuppose that instead of using these, we used the model used the model with the estimates of \\(b_0\\), \\(b_1\\), and \\(b_2\\) given below.\n\\[\n\\widehat{\\text{Speed}} = 25 + 0.25\\times\\text{MaxHeight} + 3\\times{\\text{DesignSitDown}}\n\\]\nCan we say how SST, SSR, and \\(R^2\\) for this new model would compare to those in the model using the estimates produced by R? Would they increase, decrease, or stay the same? Why?\n\n\n15)\nA small dataset with 6 observations is shown below. There is a categorical explanatory variable with two groups (A and B), and a quantitative response variable.\n\n\n\nObs.\n1\n2\n3\n4\n5\n6\n\n\n\n\n\nGroup\nA\nA\nA\nB\nB\nB\n\n\n\nResponse (y)\n3\n4\n8\n4\n6\n11\n\n\n\n\nConsider a model of the form:\n\\[\\hat{y} = b_0 + b_1\\text{GroupB}\\]\n\na)\nGive the values of the estimates \\(b_0\\) and \\(b_1\\) that would be produced by the lm command in R.\n\n\nb)\nUsing the estimates in (a), calculate SSR, SSM, SST, and \\(R^2\\).\n\n\nc)\nSuppose instead that we let \\(b_0 = 4\\) and \\(b_1 = 2\\) (so that the predicted value for each group corresponds to the median response value for that group). How would you expect SSR for this model to compare to the SSR you calculated in (b)?\n\n\nd)\nCalculate SSR for the values of \\(b_0\\) and \\(b_1\\) in (c). Does your answer match what you expected?\n\n\n\n16)\nThe table displays a set of fictional data, showing scores that seven students received on an exam. Also shown are whether or not the student attended a study session, and the number of hours the student spent studying on their own.\n\n\n\nStudent\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nHours\n3\n4\n2\n5\n1\n7\n6\n\n\nStudySession\nNo\nNo\nNo\nYes\nYes\nYes\nYes\n\n\nExam Score\n86\n77\n80\n89\n83\n88\n92\n\n\n\nWe’ll attempt to predict Score, using the other variables as explanatory variables.\n\na)\nWe use the model:\n\\(\\widehat{\\text{Score}} = b_0 + b_1\\times\\text{StudySessionYes}\\)\nUse the data and the plots below to perform the required calculations.\n\n\n\n\n\n\n\n\n\nCalculate:\n\nthe values of \\(b_0\\) and \\(b_1\\)\nthe predicted scores for each of the seven students in the dataset\nthe total sum of squares (SST)\nthe sum of squared residuals (SSR)\nthe sum of squares explained by the model (SSM)\nthe proportion of variation in exam score explained by the model using study session as the explanatory variable (\\(R^2\\))\n\n\n\nb)\nNow consider the model\n\\(\\widehat{\\text{Score}} = b_0 + b_1\\times\\text{Hours}\\)\nRelevant plots are shown below.\n\n\n\n\n\n\n\n\n\n\nM_Hrs&lt;- lm(data=TestScores, Score~Hours)\nM_Hrs\n\n\nCall:\nlm(formula = Score ~ Hours, data = TestScores)\n\nCoefficients:\n(Intercept)        Hours  \n       79.0          1.5  \n\n\nCalculate:\n\nthe predicted scores for each of the seven students in the dataset\nthe total sum of squares (SST)\nthe sum of squared residuals (SSR)\nthe sum of squares explained by the model (SSM)\nthe proportion of variation in exam score explained by the model using hours studying as the explanatory variable (\\(R^2\\))\n\n\n\n\n17)\n\na)\nFor the fictional test score example in 14(b), write the expression that would need to be minimized to obtain the values of regression coefficients \\(b_0\\) and \\(b_1\\). Substitute values from the data into your expression. (Hint: The only variables in the expression should be \\(b_0\\) and \\(b_1\\)).\n\n\nb)\nThere are three different ways to do this part of the question. You may complete any of the three ways. To get the most from the question, it is recommended that you complete the part that is consistent with the amount of mathematics you have taken previously.\nOption A - suggested for students who have taken multivariable calculus (MATH 155)\nUse calculus to show that the quantity in (c) is minimized when \\(b_0 = 79\\), and \\(b_1=1.5\\). Note: this question is not asking you to plug into the formulas for \\(b_0\\) and \\(b_1\\) in 2.4.2 of the notes. Rather, you are to perform the derivation to show that these values of \\(b_0\\) and \\(b_1\\) minimize the desired quantity in this context.\nOption B - suggested for students who have taken calculus (MATH 140 or equivalent), but not multivariable calculus\nSubstitute \\(b_0 = 79\\) into the expression from part C, so that \\(b_1\\) is the only variable in the expression. Then use calculus to show that this expression is minimized when \\(b_1 = 1.5\\).\nOption C - suggested for students who have not taken calculus\nSubstitute \\(b_0 = 79\\) into the expression from part C, so that \\(b_1\\) is the only variable in the expression. Use an online graphing calculator (such as https://www.desmos.com/calculator) to graph SSR as a function of \\(b_1\\) (hint, make \\(b_1\\) the x-variable). Show that SSR is minimized when \\(b_1=1.5\\). Include a screen shot of your graph.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch3.html",
    "href": "Ch3.html",
    "title": "3  Simulation-Based Inference",
    "section": "",
    "text": "Learning Outcomes\nConceptual Learning Outcomes\n7. Interpret and distinguish between sample standard deviation and standard error of a sampling distribution for a statistic.\n8. Explain how to use bootstrapping to calculate confidence intervals.\n9. Interpret confidence intervals in context.\n10. Describe the steps of a permutation-based hypothesis test in context (including writing hypotheses, identifying a test statistic, and explaining how to use permutation to obtain a p-value.)  \n11. Interpret p-values and draw conclusions from hypothesis tests in context. \n12. Compare and contrast the conclusions we can draw from confidence intervals and hypothesis tests.\nComputational Learning Outcomes\nF. Perform bootstrapping in R.\nG. Perform permutation-based hypothesis tests in R.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch3.html#sampling-variability",
    "href": "Ch3.html#sampling-variability",
    "title": "3  Simulation-Based Inference",
    "section": "3.1 Sampling Variability",
    "text": "3.1 Sampling Variability\n\n3.1.1 Population and Sample\nIn statistics, we often do not have the time, money, or means to collect data on all individuals or units on which we want to draw conclusions. Instead, we might collect data on only a subset of the individuals, and then make inferences about all individuals we are interested in, using the information we collected.\nVocabulary:\n\nA population is the entire set of individuals that we want to draw conclusions about.\nA sample is a subset of a population.\n\nA parameter is a numerical quantity pertaining to an entire population or process.\n\nA statistic is a numerical quantity calculated from a sample.\n\nThe purpose of this section is to explore how statistics calculated from a sample compare to the entire population.\nWe’ll work with data from the American National Health and Nutrition Examination survey (NHANES). This is a survey administered to civilian residents of the United States, which aims to collect data on several health and socioeconomic factors.\nThe dataset contains information on 12,280 adult U.S. residents. While this is really just a sample of all U.S. residents, let’s pretend for the purposes of this section that this really was our entire population. Perhaps, we can pretend that these are all the adults living in a certain town.\nThe first 6 rows of the data are shown below.\n\nhead(Population)\n\n# A tibble: 6 × 9\n  Gender   Age Race1   Education HHIncomeMid HomeOwn Height Weight SleepHrsNight\n  &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt; &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;int&gt;\n1 male      34 White   High Sch…       30000 Own       165.   87.4             4\n2 female    60 Black   High Sch…       12500 Rent      166   117.              4\n3 male      26 Mexican 9 - 11th…       30000 Rent      173    97.6             4\n4 female    49 White   Some Col…       40000 Rent      168.   86.7             8\n5 male      80 White   Some Col…       17500 Own       174.   79.1             6\n6 male      80 White   9 - 11th…       17500 Own       180.   89.6             9\n\n\nOne of the variables SleepHrsNight is a self-reported estimate of the number of hours of sleep the participant gets at night or on the weekends.\nThe histogram shows the distribution of sleep hours among the 12,280 residents in the population.\n\nSleep_Plot_Pop &lt;- ggplot(data=Population, aes(x=SleepHrsNight)) + \n  geom_histogram(fill=\"blue\", boundary=0, binwidth=1, color=\"white\") + \n  ggtitle(\"Distribution of Hours of Sleep\") + theme_bw()\nSleep_Plot_Pop\n\n\n\n\n\n\n\n\nWe see that the distribution of sleep hours is roughly symmetric, with most people getting between 5 and 8 hours per night.\nWe’ll calculate the mean and standard deviation in hours of sleep among the population.\n\nPopulation |&gt; summarize(Mean_Sleep = mean(SleepHrsNight),\n                        SD_Sleep = sd(SleepHrsNight), \n                        N= n())\n\n# A tibble: 1 × 3\n  Mean_Sleep SD_Sleep     N\n       &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1       6.86     1.44 12280\n\n\nRecall that if we fit a statistical model with no explanatory variables, the intercept estimate will be the overall population mean.\nIn this case, we’ll denote the intercept \\(\\beta_0\\), rather than \\(b_0\\), as we’ve done before. It is customary to use Greek letters to denote population parameters, and English letters to denote sample statistics. Since we are fitting the model to all 12,280 adult residents of our fictional town, we’ll call the intercept \\(\\beta_0\\).\n\nM_Pop_sleep &lt;- lm(data=Population, SleepHrsNight ~ 1)\nM_Pop_sleep\n\n\nCall:\nlm(formula = SleepHrsNight ~ 1, data = Population)\n\nCoefficients:\n(Intercept)  \n      6.856  \n\n\nThe mean number of hours of sleep among all adult residents of the city is \\(\\beta_0 =\\) 6.86 hours.\n\n\n3.1.2 Sampling Distribution and Standard Error\nWe typically won’t have data on the full population and won’t know the values of parameters like \\(\\beta_0\\). Instead, we’ll have data on just a sample taken from the population.\nTo illustrate, we’ll take samples of 75 residents and see how the mean hours of sleep in the samples compare to the population.\nFirst Sample\nWe use the sample command to take a random sample of 75 residents. The first 6 residents in the sample are shown below.\n\n# take sample of 75 residents\nSample1 &lt;- sample(Population, size = 75)\nhead(Sample1)\n\n# A tibble: 6 × 10\n  Gender   Age Race1   Education HHIncomeMid HomeOwn Height Weight SleepHrsNight\n  &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt; &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;int&gt;\n1 male      67 Hispan… 8th Grade       12500 Own       165.   67.6             6\n2 female    49 White   College …      100000 Own       162.   77.8             5\n3 male      57 Black   College …      100000 Own        NA    NA               6\n4 female    51 White   High Sch…       40000 Rent      157.   90.8             6\n5 female    23 White   College …       50000 Rent      158.  116.              8\n6 male      32 White   College …      100000 Own       184.   82.3             6\n# ℹ 1 more variable: orig.id &lt;chr&gt;\n\n\nThe distribution of sleep hours among the 75 people in the sample is shown below.\n\nSleep_Plot_Sample1 &lt;- ggplot(data=Sample1, aes(x=SleepHrsNight)) + \n  geom_histogram(fill=\"blue\", boundary=0, binwidth=1, color=\"white\") + \n  ggtitle(\"Distribution of Hours of Sleep in Sample\") + theme_bw()\nSleep_Plot_Sample1\n\n\n\n\n\n\n\n\nNotice that while the sample is not the same as the population, it has a roughly similar shape, center, and amount of spread.\nThe mean and standard deviation for the sample are shown below.\n\nSample1 |&gt; summarize(Mean_Sleep = mean(SleepHrsNight),\n                        SD_Sleep = sd(SleepHrsNight), \n                        N= n())\n\n# A tibble: 1 × 3\n  Mean_Sleep SD_Sleep     N\n       &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1       6.92     1.11    75\n\n\nNotice that the mean hours of sleep differs from that of the population. This is not surprising, as we wouldn’t expect a statistic calculated from a sample of 75 to exactly match the parameter calculated from the entire population.\nWe’ll fit a model to the sample data and record the intercept.\n\nM_Sample1_sleep &lt;- lm(data=Sample1, SleepHrsNight ~ 1)\nM_Sample1_sleep\n\n\nCall:\nlm(formula = SleepHrsNight ~ 1, data = Sample1)\n\nCoefficients:\n(Intercept)  \n       6.92  \n\n\nIn this case, we’ll denote the intercept \\(b_0\\), since it is a statistic calculated from our sample. \\(b_0\\) = 6.92 is an estimate of our population mean \\(\\beta_0\\), which we happen to know is 6.86\nIn a real situation, we would typically not know \\(\\beta_0\\), and would need to estimate it using \\(b_0\\). Since \\(b_0\\) shouldn’t be expected to match \\(\\beta_0\\) exactly, we cannot say that \\(\\beta_0\\) is exactly \\(b_0\\), but we can say that \\(\\beta_0\\) is likely to be “close: to \\(b_0\\). In order to know how”close”, we need to know how much the sample statistic \\(b_0\\) could plausibly differ from the population parameter \\(\\beta_0\\).\nTo investigate that, let’s take some additional samples of size 75, fit a model to estimate the mean hours of sleep \\(b_0\\) and see how much \\(b_0\\) varies between samples.\nSecond Sample\n\nSample2 &lt;- sample(Population, size = 75)\nM_Sample2_sleep &lt;- lm(data=Sample2, SleepHrsNight ~ 1)\nM_Sample2_sleep\n\n\nCall:\nlm(formula = SleepHrsNight ~ 1, data = Sample2)\n\nCoefficients:\n(Intercept)  \n      7.133  \n\n\nThird Sample\n\nSample3 &lt;- sample(Population, size = 75)\nM_Sample3_sleep &lt;- lm(data=Sample3, SleepHrsNight ~ 1)\nM_Sample3_sleep\n\n\nCall:\nlm(formula = SleepHrsNight ~ 1, data = Sample3)\n\nCoefficients:\n(Intercept)  \n      6.747  \n\n\nFourth Sample\n\nSample4 &lt;- sample(Population, size = 75)\nM_Sample4_sleep &lt;- lm(data=Sample4, SleepHrsNight ~ 1)\nM_Sample4_sleep\n\n\nCall:\nlm(formula = SleepHrsNight ~ 1, data = Sample4)\n\nCoefficients:\n(Intercept)  \n      6.547  \n\n\nFifth Sample\n\nSample5 &lt;- sample(Population, size = 75)\nM_Sample5_sleep &lt;- lm(data=Sample5, SleepHrsNight ~ 1)\nM_Sample5_sleep\n\n\nCall:\nlm(formula = SleepHrsNight ~ 1, data = Sample5)\n\nCoefficients:\n(Intercept)  \n      6.827  \n\n\nLet’s now take 10,000 more random samples of 75 adult residents and record \\(b_0\\), the estimated mean hours of sleep for each sample.\n\nSamples_sleep &lt;- do(10000)*lm(SleepHrsNight ~ 1, data = sample(Population, size=75))\n\nThe histogram below shows the distribution of \\(b_0\\), the mean hours of sleep in the 10,000 different samples.\n\nSamp_Dist_Mean_sleep &lt;- ggplot(data=Samples_sleep, aes(x=Intercept)) +\n  geom_histogram(color=\"white\", fill=\"lightblue\", binwidth=0.01) + \n  ggtitle(\"Sampling Distribution for b0 - Mean Hours of Sleep\") + \n  xlab(\"Mean Hours in Sample\") + theme_bw()\nSamp_Dist_Mean_sleep \n\n\n\n\n\n\n\n\nWe notice that most of our 10,000 samples yielded means between 6.5 and 7.25 hours and that the distribution is roughly symmetric and bell-shaped.\nThe distribution shown in this histogram is called the sampling distribution for \\(b_0\\). The sampling distribution for a statistic shows the distribution of the statistic over many samples.\nWe’ll calculate the mean and standard deviation of the sampling distribution. How do these compare to the mean and standard deviation of the population that we saw previously?\n\nSamples_sleep |&gt; summarize(Mean_b0 = mean(Intercept),\n                              SE_b0 = sd(Intercept), \n                              N= n())\n\n   Mean_b0    SE_b0     N\n1 6.855329 0.166038 10000\n\n\nNotice that the mean of the sampling distribution for \\(b_0\\) is about the same as the population mean \\(\\beta_0\\) = 6.86\nThe standard deviation of the sampling distribution for the mean \\(b_0\\), however, is much less than the population standard deviation of 1.435. This makes sense, since there is much less variability in the sampling distribution for the mean hours of sleep than there was in the distribution of sleep hours for individuals. We saw that most individuals reported getting between 5 and 8 hours per night, while most samples of size 75 resulted in a mean between 6.5 and 7.25 hours, a much narrower range.\nThe standard deviation of the sampling distribution for a statistic is also called the standard error of the statistic. In this case, it represents the amount of variability in mean hours of sleep between different samples of 75 residents. This is different than the population standard deviation, which represents the amount of variability in hours of sleep between different individual residents.\nVocabulary:\n\nThe sampling distribution of a statistic is the distribution of values the statistic takes on across many different samples of a given size.\n\nThe standard error of a statistic is the standard deviation of that statistic’s sampling distribution. It measures how much the statistic varies between different samples of a given size.\n\n\nEffect of Sample Size\nQuestion:\nSuppose the sample consisted of 10, or 30, or 200 residents, instead of 75? Would you expect the standard deviation of individual sleep times in the samples to increase, decrease or stay about the same? What about the standard error of the mean sleep time?\nThe histogram shows the distribution of sample standard deviations in random samples of each size. These represent the variability in sleep hours between individuals in the sample.\n\n\n\n\n\n\n\n\n\nNotice that the variability in sleep hours between individual people stays roughly the same, regardless of sample size.\nThe table shows the standard deviation of individual sleep hours in each of the samples.\n\n\n\n\n\nSample_Size\nSD\n\n\n\n\n10\n1.567021\n\n\n30\n1.383399\n\n\n75\n1.245966\n\n\n200\n1.419746\n\n\n\n\n\nSample size does not impact the amount of variability between individual people. Standard deviation in sleep hours does not systematically increase or decrease based one sample size (of course it varies a little based on the people randomly chosen in the sample).\nNow, we’ll examine what happens to the standard error of the mean \\(b_0\\) as the sample size changes.\nDistributions of Mean Between Different Samples\n\n\n\n\n\n\n\n\n\nNotice that as the sample size increases, the sampling distribution of the mean becomes more symmetric and bell-shaped, and also more concentrated around the population mean \\(\\mu\\).\nThe table shows the standard error of the mean for samples of different size:\n\n\n\n\n\nSample_Size\nSE\n\n\n\n\n10\n0.4547698\n\n\n30\n0.2624521\n\n\n75\n0.1637800\n\n\n200\n0.0995903\n\n\n\n\n\nAs sample size increases, variability between means of different samples decreases. Standard error of the mean decreases. This is also true of standard errors for other statistics (i.e. difference in means, regression slopes, etc.)\n\n\n\n\n3.1.3 Confidence Intervals\nWe saw that while statistics calculated from individual samples deviate from population parameters, over many samples, they approximately average to the population parameter (assuming the samples are chosen randomly).\nThus, when we have only a single sample, we can use the sample statistic as an estimate of the population parameter, provided we allow for a certain margin of error. The question is how much margin of error do we need?\nThe sampling distribution for the mean hours of sleep is shown again below. The population mean (\\(\\beta_0=6.856\\)) is marked by the purple dotted line. The gold bar at the bottom of the histogram represents the range of sample proportions that lie within \\(\\pm 2\\) standard errors of the true population mean:\n6.86 - 2(0.17) = 6.53 to \n6.86 + 2(0.17) = 7.19.\n\nSamp_Dist_Mean_sleep + geom_vline(xintercept=M_Pop_sleep_b0, color=\"purple\", linetype=\"dotted\", linewidth=2) +\n  geom_segment(aes(x= M_Pop_sleep_b0 - 2*SE_b0, xend= M_Pop_sleep_b0 + 2*SE_b0, y=20, yend=20), color=\"gold\", size=20, alpha=0.01) \n\n\n\n\n\n\n\n\nWe calculate the proportion of samples whose mean hours of sleep lies within \\(\\pm 2\\) standard errors of the true proportion.\n\nLower &lt;- M_Pop_sleep_b0 - 2*SE_b0 \nUpper &lt;- M_Pop_sleep_b0 + 2*SE_b0 \nsum((Samples_sleep$Intercept &gt;=Lower) & (Samples_sleep$Intercept &lt;= Upper))\n\n[1] 9563\n\n\nApproximately 95% of the 10,000 samples produced proportions within \\(\\pm 2\\) standard errors of the true population mean.\nIn a real situation, we won’t have access to the entire population, but only a single sample. For example, recall our original sample of 75 people, in which we observed a proportion of on-time arrivals of \\(b_0=\\) 6.92.\nSince we now know that 95% of all samples produce proportions that lie within two standard errors of the population mean, we can obtain an estimate of the population mean \\(\\beta_0\\) by adding and subtracting \\(2\\times \\text{SE}(b_0)\\) from our observed sample estimate \\(b_0\\).\nUsing probability theory, it can be shown generally that if the sampling distribution of a statistic is symmetric and bell shaped, then approximately 95% of all samples will produce sample statistics that lie within two standard errors of the corresponding population parameter. Such an interval is called an approximate 95% confidence interval for the population parameter.\nApproximate 95% confidence interval: If the sampling distribution of a statistic is symmetric and bell-shaped, a 95% confidence interval for the population parameter is:\n\\[\n\\text{Statistic} \\pm 2\\times \\text{Standard Error},\n\\]\nMore generally, if we want to use a level of confidence that is different than 95%, we can adjust the value we multiply the standard error by. In general, a standard error confidence interval has the form:\n\\[\n\\text{Statistic} \\pm m\\times \\text{Standard Error},\n\\]\nwhere the value of \\(m\\) depends on the desired level of confidence.\nConfidence intervals that are calculated by adding and subtracting a certain number of standard errors from the sample statistic are called standard error confidence intervals. This approach works as long as the sampling distribution is symmetric and bell-shaped. Probability theory tells us that in a symmetric and bell-shaped distribution, approximately 95% of the area lies within two standard errors of the center of the distribution, given by the true parameter value. We will, however, see that this approach will not work in all cases. Not all statistics produce sampling distributions that are symmetric and bell-shaped, and we will need an alternative way to calculate confidence intervals in these situations.\n\n\n\n\n\nImage from https://openintro-ims.netlify.app/foundations-mathematical\n\n\n\n\n\nExample: 95% Confidence Interval for \\(\\beta_0\\)\nWe’ll calculate a 95% confidence interval for the mean hours of sleep in our fictional city, using our original sample where \\(b_0\\) = 6.92. The 95% confidence interval is:\n\\[\n\\begin{aligned}\n& b_0 \\pm 2\\times \\text{SE}(b_0) \\\\\n& = 6.92 \\pm 2(0.17)\n\\end{aligned}\n\\]\nThe confidence interval is:\n\nc(b0 - 2*SE_b0, b0 + 2*SE_b0) |&gt; as.numeric() |&gt; round(2)\n\n[1] 6.59 7.25\n\n\nBased on our sample of 75 residents, we can be 95% confident that the mean number of hours of sleep among all residents in the city is between 6.59 and 7.25 hours.\nKnowing what we do about the true value of the population parameter \\(\\beta_0\\), we can see that our interval does indeed contain the true population value of \\(\\beta_0=\\) 6.86.\n\n\nWhat does 95% Confidence Mean?\nSuppose, instead of \\(\\beta_0=\\) 6.8560261, our sample of 75 had resulted in a mean of \\(\\beta_0=\\) 6.3. We can see from the sampling distribution that some samples did produce means that low.\nThen, using the same formula we would obtain an approximate 95% confidence interval of\n\\[\n\\begin{aligned}\n& 6.3 \\pm 2\\times \\text{SE}(b_0) \\\\\n& = 6.3 \\pm 2(0.17)\n\\end{aligned}\n\\]\nwhich is 5.97 to 6.63.\nNotice that this interval does not contain the true value of \\(\\beta_0=6.85\\). Does this mean we did something wrong when we calculated the confidence interval?\nThe answer is “no”. Notice we claimed to be only “95%” confident that our interval contains the true value of the population parameter \\(\\beta_0\\). This means that we should expect 5% of samples taken randomly to yield a sample mean \\(b_0\\) so different from the population mean \\(\\beta_0\\), that the resulting confidence interval would not contain the true value of \\(\\beta_0\\). This does not mean we did anything wrong, just that we obtained an unusual sample just by chance. Since our procedure, namely adding and subtracting two standard errors, is designed to work 95% of the time, we can expect such samples to be rare.\nIn a real situation, we won’t know the true value of the population parameter, so we won’t know for sure whether or not our confidence interval contains this true parameter value.\nTo further understand the meaning of “95% confidence”, let’s explore what happens when we calculate confidence intervals based on estimates \\(b_0\\) obtained from many different samples. For each of our 10,000 different samples taken from our population, we’ll add and subtract two standard errors from the sample mean \\(b_0\\) corresponding to that sample.\nThe table below displays the value of \\(b_0\\), for the first 20 samples we took, along with the lower and upper bounds of the confidence interval, and whether or not the confidence interval contains the true parameter value \\(\\beta_0\\) (either 1=TRUE or 0=FALSE).\n\nSamples_df_b0 &lt;- Samples_sleep %&gt;% mutate(Sample = row_number(),\n                                                b_0 = Intercept,\n                                                Lower = b_0 - 2*SE_b0, \n                                                Upper = b_0 + 2*SE_b0,\n                                                Contains_beta0 = beta0 &gt;= Lower & beta0 &lt;= Upper) |&gt;\n                                                    dplyr::select(Sample, b_0, Lower, Upper, Contains_beta0)  \nkable(head(Samples_df_b0 |&gt; round(2), 20))\n\n\n\n\nSample\nb_0\nLower\nUpper\nContains_beta0\n\n\n\n\n1\n6.73\n6.40\n7.07\n1\n\n\n2\n6.84\n6.51\n7.17\n1\n\n\n3\n6.91\n6.57\n7.24\n1\n\n\n4\n6.97\n6.64\n7.31\n1\n\n\n5\n6.80\n6.47\n7.13\n1\n\n\n6\n7.12\n6.79\n7.45\n1\n\n\n7\n6.72\n6.39\n7.05\n1\n\n\n8\n6.65\n6.32\n6.99\n1\n\n\n9\n6.83\n6.49\n7.16\n1\n\n\n10\n6.87\n6.53\n7.20\n1\n\n\n11\n6.73\n6.40\n7.07\n1\n\n\n12\n6.93\n6.60\n7.27\n1\n\n\n13\n7.04\n6.71\n7.37\n1\n\n\n14\n6.76\n6.43\n7.09\n1\n\n\n15\n6.73\n6.40\n7.07\n1\n\n\n16\n7.03\n6.69\n7.36\n1\n\n\n17\n6.60\n6.27\n6.93\n1\n\n\n18\n6.65\n6.32\n6.99\n1\n\n\n19\n6.92\n6.59\n7.25\n1\n\n\n20\n6.97\n6.64\n7.31\n1\n\n\n\n\n\nThe graphic below visualizes the confidence intervals produced using the estimates from the first 100 samples. The purple dotted line indicates the true value of \\(\\beta_0\\). The black dots indicate the value of \\(b_0\\) for each sample. Intervals that do in fact contain the true value of \\(p\\) are shown in blue, and intervals that do not contain the true value of \\(p\\) are shown in green.\n\nggplot(data=Samples_df_b0[1:100,], aes(y=Sample, x=b_0)) +    \n  geom_point() +\n  geom_errorbar(aes(xmin = Lower, xmax = Upper, color=Contains_beta0))  + \n  xlab(\"Confidence Interval\") + \n  ylab(\"Sample\") + \n  geom_vline(xintercept = beta0, color=\"purple\", linetype=\"dotted\", size=2) + \n  ggtitle(\"100 Different Confidence Intervals for beta_0\") + \n  theme_bw() \n\n\n\n\n\n\n\n\nOut of these 100 samples, 98 contain the true value of the population parameter \\(p\\). This is close to the desired 95% confidence level.\nThe picture shows confidence intervals produced by the first 100 samples, but we actually took 10,000 different samples of 75 residents. Let’s calculate how many of these samples produced confidence intervals that contain the true value of \\(p\\).\n\nprop(Samples_df_b0$Contains_beta0 == TRUE)\n\nprop_TRUE \n   0.9563 \n\n\nAgain, notice that close to 95% of the samples produced confidence intervals contain the true population parameter \\(\\beta_0\\). Note that for the red intervals that do not contain \\(p\\) nothing was done incorrectly. The sample was taken at random, and the confidence interval was calculated using the correct formula. It just happened that by chance, we obtained a sample mean \\(b_0\\) that was unusually high or low, leading to an interval that did not capture the true population parameter. This, of course, happens rarely, and approximately 95% of the samples do, in fact, result in intervals that contain the true value of \\(b_0\\).\nThis brings us back to the question “what does 95% confidence mean?”. An approximate 95% confidence interval means that if we take a large number of samples and calculate confidence intervals from each of them, then approximately 95% of the samples will produce intervals containing the true population parameter. In reality, we’ll only have on sample, and won’t know whether or not our interval contains the true parameter value. Assuming we have taken the sample and calculated the interval correctly, we can rest assured in the knowledge that that 95% of all intervals taken would contain the true parameter value, and hope that ours is among that 95%.\nIt might be tempting to say that “there is approximately a 95% chance” that the population parameter lies within the confidence interval, but this is incorrect. In the statistical framework used here (known as classical, or frequentist statistics), the population parameter is assumed to be a fixed, but (typically) unknown number. It either is within the interval, or it isn’t. We just (typically) don’t know which. There’s nothing random about whether or not the parameter value is in our interval, so it doesn’t make sense to speak of it in terms of chance or randomness. Randomness comes into play due to the fact that we selected a random sample, which will produce a statistic likely to differ from the population parameter due to sampling variability. A different statistical framework, known as Bayesian statistics approaches this differently, and would allow us to use randomness and chance to describe our beliefs about any uncertain quantity, including a population proportion. In this class, however, we’ll stick to the classical frequentist interpretation.\nOf course, you might ask why we needed to calculate a confidence interval for the mean hours of sleep among all residents in the first place, since we actually have data on all 12,280 residents in our fictional town and already know the mean hours of sleep. The answer is that we don’t. But, in most real situations, we will only have data from a single sample, not the entire population, and we won’t know the true population parameter. We’ll be able to build on the ideas of sampling distributions and standard error that we learned about in this section to calculate confidence intervals in those scenarios.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch3.html#bootstrap-confidence-intervals",
    "href": "Ch3.html#bootstrap-confidence-intervals",
    "title": "3  Simulation-Based Inference",
    "section": "3.2 Bootstrap Confidence Intervals",
    "text": "3.2 Bootstrap Confidence Intervals\n\n3.2.1 Mercury Concentration in Florida Lakes\nA 2004 study by Lange, T., Royals, H. and Connor, L. examined Mercury accumulation in large-mouth bass, taken from a sample of 53 Florida Lakes. If Mercury accumulation exceeds 0.5 ppm, then there are environmental concerns. In fact, the legal safety limit in Canada is 0.5 ppm, although it is 1 ppm in the United States.\nIn our sample, we have data on 53 lakes, out of more than 30,000 lakes in the the state of Florida. We’ll attempt to draw conclusions about the entire population, consisting of all lakes in Florida, using data from our sample of 53. It is not clear how the lakes in this sample of 53 were selected, or how representative they are of all lakes in the state of Florida. Let’s assume for our purposes that the lakes in the sample can be reasonably thought of as being representative of all lakes in Florida.\n\n\n\n\n\nhttps://www.maine.gov/ifw/fish-wildlife/fisheries/species-information/largemouth-bass.html\n\n\n\n\n\nFloridaLakes &lt;- read_csv(\"FloridaLakes.csv\")\nFloridaLakes &lt;- FloridaLakes |&gt; rename(Mercury = AvgMercury) |&gt; \n                                select(ID, Lake, pH, Mercury, Chlorophyll, Location, Depth)\n\n\nglimpse(FloridaLakes)\n\nRows: 53\nColumns: 7\n$ ID          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Lake        &lt;chr&gt; \"Alligator\", \"Annie\", \"Apopka\", \"Blue Cypress\", \"Brick\", \"…\n$ pH          &lt;dbl&gt; 6.1, 5.1, 9.1, 6.9, 4.6, 7.3, 5.4, 8.1, 5.8, 6.4, 5.4, 7.2…\n$ Mercury     &lt;dbl&gt; 1.23, 1.33, 0.04, 0.44, 1.20, 0.27, 0.48, 0.19, 0.83, 0.81…\n$ Chlorophyll &lt;dbl&gt; 0.7, 3.2, 128.3, 3.5, 1.8, 44.1, 3.4, 33.7, 1.6, 22.5, 14.…\n$ Location    &lt;chr&gt; \"S\", \"S\", \"N\", \"S\", \"S\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"S\"…\n$ Depth       &lt;chr&gt; \"Medium\", \"Deep\", \"Medium\", \"Shallow\", \"Medium\", \"Shallow\"…\n\n\nWe’ll divide the state along route 50, which runs East-West, passing through Northern Orlando.\n\n\n\n\n\nfrom Google Maps\n\n\n\n\nOur data come from a sample of 53 lakes, out of more then 30,000 in the entire state of Florida. The mercury levels of the 53 lakes in the sample are shown in the table below.\n\nprint.data.frame(FloridaLakes, row.names = FALSE)\n\n ID              Lake  pH Mercury Chlorophyll Location   Depth\n  1         Alligator 6.1    1.23         0.7        S  Medium\n  2             Annie 5.1    1.33         3.2        S    Deep\n  3            Apopka 9.1    0.04       128.3        N  Medium\n  4      Blue Cypress 6.9    0.44         3.5        S Shallow\n  5             Brick 4.6    1.20         1.8        S  Medium\n  6            Bryant 7.3    0.27        44.1        N Shallow\n  7            Cherry 5.4    0.48         3.4        N Shallow\n  8          Crescent 8.1    0.19        33.7        N Shallow\n  9        Deer Point 5.8    0.83         1.6        N  Medium\n 10              Dias 6.4    0.81        22.5        N    Deep\n 11              Dorr 5.4    0.71        14.9        N  Medium\n 12              Down 7.2    0.50         4.0        S    Deep\n 13             Eaton 7.2    0.49        11.6        N Shallow\n 14 East Tohopekaliga 5.8    1.16         5.8        S  Medium\n 15           Farm-13 7.6    0.05        71.1        N Shallow\n 16            George 8.2    0.15        78.6        N Shallow\n 17           Griffin 8.7    0.19        80.1        N  Medium\n 18            Harney 7.8    0.77        13.9        N Shallow\n 19              Hart 5.8    1.08         4.6        S  Medium\n 20        Hatchineha 6.7    0.98        17.0        S Shallow\n 21           Iamonia 4.4    0.63         9.6        N    Deep\n 22         Istokpoga 6.7    0.56         9.5        S Shallow\n 23           Jackson 6.1    0.41        21.0        N    Deep\n 24         Josephine 6.9    0.73        32.1        S Shallow\n 25          Kingsley 5.5    0.34         1.6        N    Deep\n 26         Kissimmee 6.9    0.59        21.5        S Shallow\n 27         Lochloosa 7.3    0.34        24.7        N Shallow\n 28            Louisa 4.5    0.84         7.0        S  Medium\n 29        Miccasukee 4.8    0.50        14.8        N  Medium\n 30          Minneola 5.8    0.34         0.7        N  Medium\n 31            Monroe 7.8    0.28        43.8        N  Medium\n 32           Newmans 7.4    0.34        32.7        N Shallow\n 33        Ocean Pond 3.6    0.87         3.2        N    Deep\n 34      Ocheese Pond 4.4    0.56         3.2        N    Deep\n 35        Okeechobee 7.9    0.17        16.1        S Shallow\n 36            Orange 7.1    0.18        45.2        N Shallow\n 37       Panasoffkee 6.8    0.19        16.5        N Shallow\n 38            Parker 8.4    0.04       152.4        S  Medium\n 39            Placid 7.0    0.49        12.8        S    Deep\n 40            Puzzle 7.5    1.10        20.1        N Shallow\n 41            Rodman 7.0    0.16         6.4        N    Deep\n 42          Rousseau 6.8    0.10         6.2        N  Medium\n 43           Sampson 5.9    0.48         1.6        N    Deep\n 44             Shipp 8.3    0.21        68.2        S Shallow\n 45           Talquin 6.7    0.86        24.1        N    Deep\n 46            Tarpon 6.2    0.52         9.6        S Shallow\n 47          Trafford 8.9    0.27         9.6        S Shallow\n 48             Trout 4.3    0.94         6.4        S    Deep\n 49      Tsala Apopka 7.0    0.40         4.6        N Shallow\n 50              Weir 6.9    0.43        16.5        N    Deep\n 51      Tohopekaliga 6.2    0.65        27.7        S  Medium\n 52           Wildcat 5.2    0.25         2.6        N    Deep\n 53              Yale 7.9    0.27         8.8        N  Medium\n\n\nThe histogram shows the distribution of mercury levels in the 53 lakes in the sample.\n\nLakes_Hist &lt;- ggplot(data=FloridaLakes, aes(x=Mercury)) + \n  geom_histogram(fill= \"blue\", color=\"white\", binwidth = 0.1) + \n  ggtitle(\"Mercury Levels in Sample of 53 Florida Lakes\") + \n  xlab(\"Mercury Level\") + ylab(\"Frequency\") + theme_bw()\nLakes_Hist\n\n\n\n\n\n\n\n\nThe table below gives the mean and standard deviation in mercury level among the 53 lakes in our sample.\n\nFloridaLakes |&gt; summarize(Mean_Hg = mean(Mercury), \n                          SD_Hg = sd(Mercury),\n                          N = n())\n\n# A tibble: 1 × 3\n  Mean_Hg SD_Hg     N\n    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1   0.527 0.341    53\n\n\nThinking in terms of models, we can estimate the mean mercury level by fitting a model with no explanatory variables. The model equation is\n\\[\n\\begin{aligned}\n\\widehat{\\text{Mercury}} & = b_0\n\\end{aligned}\n\\]\n\nM_Lakes_merc_0 &lt;- lm(data=FloridaLakes, Mercury~1)\nM_Lakes_merc_0\n\n\nCall:\nlm(formula = Mercury ~ 1, data = FloridaLakes)\n\nCoefficients:\n(Intercept)  \n     0.5272  \n\n\nWe see that in our sample of 53 lakes, the mean mercury level is 0.527 ppm., which is just above Canada’s legal limit, though below the United States’ limit.\nSuppose we want to estimate the mean mercury level among all Florida lakes. As we saw in the previous section, we should not expect the population proportion to exactly match the sample, due to random variability between samples. We can, however, use the sample statistic as an estimates and construct confidence intervals for the unknown population proportions.\nIn order to construct the confidence interval, we need to know how much each statistic could vary between different samples of size 53. That is, we need to know the standard error. In the previous section, we calculated the standard error by taking 10,000 different samples of the same size as ours from the population, calculating the proportion for each sample, and then calculating the standard deviation of the proportions obtained from these 10,000 different samples. This procedure will not work here, however, because unlike the previous example where we really did have data on the entire population of our city’s residents, we do not have data on all 30,000+ lakes in Florida. We cannot take a lot of different samples of size 53 from the population of all lakes, and thus, cannot obtain the sampling distributions for our statistics or calculate their standard errors. Instead, we’ll use a process that closely mimics what we did in Section 3.1, using only the information from the 53 lakes contained in our sample.\n\n\n3.2.2 Bootstrap Sampling\nAll we have is a single sample of 53 lakes. We need to figure out how much the mean mercury level would vary between different samples of size 53, using only the information contained in our one sample.\nTo do this, we’ll implement a popular simulation-based strategy, known as bootstrapping.\nLet’s assume our sample is representative of all Florida lakes. Then, we’ll duplicate the sample many times to create a large set that will look like the population of all Florida Lakes. We can then draw samples of 53 from that large population, and record the mean mercury level for each sample of 53.\nAn illustration of the bootstrapping procedure is shown below, using a sample of 12 colored dots, instead of the 53 lakes.\n\n\n\n\n\n\n\n\n\nIn fact, duplicating the sample many times and selecting new samples of size \\(n\\) has the same effect as drawing samples of size \\(n\\) from the original sample, by putting the item drawn back in each time, a procedure called sampling with replacement. Thus, we can skip the step of copying/pasting the sample many times, and instead draw our samples with replacement.\nThis means that in each new sample, some lakes will be drawn multiple times and others not at all. It also ensures that each sample is different, allowing us to estimate variability in the sample mean between the different samples of size 53.\nAn illustration of the concept of bootstrapping, using sampling with replacement is shown below.\n\n\n\n\n\n\n\n\n\nThe variability in a statistic (in this case mean mercury concentration) in our newly drawn samples is used to approximate the variability we would see in our statistic between different samples of 53 lakes if we could draw different samples from the population of all Florida Lakes.\nThe point of bootstrapping is to observe how much a statistic varies between bootstrap samples. This can act as an estimate of how much that statistic would vary between different samples of size \\(n\\), drawn from the population.\nThe steps of bootstrap sampling can be summarized in the following algorithm.\nBootstrap Algorithm\nFor an original sample of size \\(n\\):\n\nTake a sample size \\(n\\) by randomly sampling from the original, with replacement. Thus, some observations will show up multiple times, and others not at all. This sample is called a bootstrap sample.\nCalculate the statistic of interest in the bootstrap sample. This statistic could be a single mean, a difference in means, a regression slope, or other statistics like median, standard deviation, or a proportion.\nRepeat steps 1 and 2 many (say 10,000) times, keeping track of the statistic of interest that is calculated in each bootstrap sample.\nLook at the distribution of the statistic across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the statistic of interest.\n\n\nBootstrap Samples of Lakes\nBootstrap Sample 1\nThe resample() function samples the specified number rows from a data frame, with replacement.\nThe lakes in the first sample are shown below. Notice that some lakes occur multiple times, and others not at all.\n\nBootstrap1 &lt;- resample(FloridaLakes, replace = TRUE) |&gt; arrange(ID)\nBootstrap1\n\n# A tibble: 53 × 8\n      ID Lake            pH Mercury Chlorophyll Location Depth   orig.id\n   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1     2 Annie          5.1    1.33         3.2 S        Deep    2      \n 2     2 Annie          5.1    1.33         3.2 S        Deep    2      \n 3     3 Apopka         9.1    0.04       128.  N        Medium  3      \n 4     4 Blue Cypress   6.9    0.44         3.5 S        Shallow 4      \n 5     5 Brick          4.6    1.2          1.8 S        Medium  5      \n 6     6 Bryant         7.3    0.27        44.1 N        Shallow 6      \n 7     8 Crescent       8.1    0.19        33.7 N        Shallow 8      \n 8     8 Crescent       8.1    0.19        33.7 N        Shallow 8      \n 9     8 Crescent       8.1    0.19        33.7 N        Shallow 8      \n10     9 Deer Point     5.8    0.83         1.6 N        Medium  9      \n# ℹ 43 more rows\n\n\nA histogram of the mercury levels in the bootstrap sample is shown below.\n\nggplot(data=Bootstrap1, aes(x=Mercury)) + \n  geom_histogram(fill= \"blue\",  color=\"white\", binwidth = 0.1) + \n  ggtitle(\"Mercury Levels in Bootstrap Sample\") + \n  xlab(\"Mercury Level\") + ylab(\"Frequency\") + theme_bw()\n\n\n\n\n\n\n\n\nThe statistics of interest for the bootstrap sample are shown below.\n\nBootstrap1 |&gt; summarize(Mean_Hg = mean(Mercury), \n                          SD_Hg = sd(Mercury), \n                          N = n())\n\n# A tibble: 1 × 3\n  Mean_Hg SD_Hg     N\n    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1   0.539 0.327    53\n\n\nWe can also obtain the sample mean using a model.\n\nM_Lakes_merc_0_Boot1 &lt;- lm(data=Bootstrap1, Mercury~1)\nM_Lakes_merc_0_Boot1\n\n\nCall:\nlm(formula = Mercury ~ 1, data = Bootstrap1)\n\nCoefficients:\n(Intercept)  \n     0.5394  \n\n\nNotice that the sample mean of the bootstrap sample differs from that of the original sample. This happens because some lakes in the original sample show up multiple times and others not at all. By taking many bootstrap samples and estimating the variability in \\(b_0\\), we can estimate the standard error needed to calculate a confidence interval for the mean mercury level among all Florida Lakes.\nBootstrap Sample #2\nSince bootstrap samples are taken with replacement, every sample will be different, allowing us to measure the amount of variability in our statistics between samples.\nWe take a second bootstrap sample and display the statistics below. Notice that the lakes chosen and omitted differ from the first sample.\n\nBootstrap2 &lt;- resample(FloridaLakes, replace = TRUE) |&gt; arrange(ID)\nBootstrap2\n\n# A tibble: 53 × 8\n      ID Lake          pH Mercury Chlorophyll Location Depth   orig.id\n   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1     1 Alligator    6.1    1.23         0.7 S        Medium  1      \n 2     2 Annie        5.1    1.33         3.2 S        Deep    2      \n 3     6 Bryant       7.3    0.27        44.1 N        Shallow 6      \n 4     6 Bryant       7.3    0.27        44.1 N        Shallow 6      \n 5     7 Cherry       5.4    0.48         3.4 N        Shallow 7      \n 6     8 Crescent     8.1    0.19        33.7 N        Shallow 8      \n 7     9 Deer Point   5.8    0.83         1.6 N        Medium  9      \n 8     9 Deer Point   5.8    0.83         1.6 N        Medium  9      \n 9    11 Dorr         5.4    0.71        14.9 N        Medium  11     \n10    12 Down         7.2    0.5          4   S        Deep    12     \n# ℹ 43 more rows\n\n\nA histogram of the mercury levels in the bootstrap sample is shown below.\n\nggplot(data=Bootstrap2, aes(x=Mercury)) + \n  geom_histogram(fill = \"blue\", color=\"white\", binwidth = 0.1) + \n  ggtitle(\"Mercury Levels in Bootstrap Sample\") + \n  xlab(\"Mercury Level\") + ylab(\"Frequency\") + theme_bw()\n\n\n\n\n\n\n\n\nWe again use a model to estimate the mean mercury level in the bootstrap sample. Notice that these differ from the original sample and from the first bootstrap sample.\n\nM_Lakes_merc_0_Boot2 &lt;- lm(data=Bootstrap2, Mercury~1)\nM_Lakes_merc_0_Boot2\n\n\nCall:\nlm(formula = Mercury ~ 1, data = Bootstrap2)\n\nCoefficients:\n(Intercept)  \n     0.5392  \n\n\nBootstrap Sample #3\nWe’ll take another bootstrap sample. Notice how the lakes selected differ between bootstrap samples, ensuring our estimates of \\(b_0\\) will differ and we’ll be able to estimate variability.\n\nBootstrap3 &lt;- resample(FloridaLakes, replace = TRUE) |&gt; arrange(ID)\nBootstrap3\n\n# A tibble: 53 × 8\n      ID Lake            pH Mercury Chlorophyll Location Depth   orig.id\n   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1     2 Annie          5.1    1.33         3.2 S        Deep    2      \n 2     3 Apopka         9.1    0.04       128.  N        Medium  3      \n 3     3 Apopka         9.1    0.04       128.  N        Medium  3      \n 4     3 Apopka         9.1    0.04       128.  N        Medium  3      \n 5     4 Blue Cypress   6.9    0.44         3.5 S        Shallow 4      \n 6     4 Blue Cypress   6.9    0.44         3.5 S        Shallow 4      \n 7     5 Brick          4.6    1.2          1.8 S        Medium  5      \n 8     5 Brick          4.6    1.2          1.8 S        Medium  5      \n 9     6 Bryant         7.3    0.27        44.1 N        Shallow 6      \n10     8 Crescent       8.1    0.19        33.7 N        Shallow 8      \n# ℹ 43 more rows\n\n\n\nggplot(data=Bootstrap3, aes(x=Mercury)) + \n  geom_histogram(fill = \"blue\", color=\"white\", binwidth = 0.1) + \n  ggtitle(\"Mercury Levels in Bootstrap Sample\") + \n  xlab(\"Mercury Level\") + ylab(\"Frequency\") + theme_bw()\n\n\n\n\n\n\n\n\n\nM_Lakes_merc_0_Boot2 &lt;- lm(data=Bootstrap3, Mercury~1)\nM_Lakes_merc_0_Boot2\n\n\nCall:\nlm(formula = Mercury ~ 1, data = Bootstrap3)\n\nCoefficients:\n(Intercept)  \n     0.4981  \n\n\n\n\n\n3.2.3 Bootstrap Distribution for Mean\nNow that we’ve seen how bootstrap sampling works, we’ll take many different bootstrap samples and calculate our statistics for each of the samples. This will tell how how much our statistics can vary between sample. We’ll take 10,000 different bootstrap samples.\nThe do(10000) command tells R to do the command following it (in this case, take the bootstrap sample and calculate the statistic of interest) 10,000 times. We’ll store the results in a vector called Lakes_Bootstrap_Mean.\nWe’ll take 10,000 bootstrap samples, fit a linear model with only an intercept term, \\(b_0\\), and record the value of \\(b_0\\) in each bootstrap sample.\nBootstrap Distribution for Mean\n\nLakes_Bootstrap_Mean &lt;- do(10000) * lm( Mercury ~ 1, data = resample(FloridaLakes, replace=TRUE))\n\nThe means of the first 5 bootstrap samples are:\n\nhead(Lakes_Bootstrap_Mean$Intercept)\n\n[1] 0.6050943 0.4232075 0.4845283 0.6169811 0.5615094 0.5079245\n\n\nThe distribution of means observed in the 10,000 different bootstrap samples is shown below. This distribution is called the bootstrap distribution.\n\nLakes_Bootstrap_Mean_Plot &lt;- ggplot(data=Lakes_Bootstrap_Mean, aes(x=Intercept)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\", binwidth=0.02) +\n  xlab(\"b0: Mean Mercury in Sample\") + ylab(\"Frequency\") +\n  ggtitle(\"Bootstrap Distribution for b0: Mean Mercury\") + \n  theme(legend.position = \"none\") + theme_bw()\nLakes_Bootstrap_Mean_Plot\n\n\n\n\n\n\n\n\nThe bootstrap distribution is meant to approximate the sampling distribution of the statistic of interest (in this case the mean mercury level). Because it is based on the sample, the bootstrap distribution will be centered at the sample statistic (\\(b_0 = 0.527\\) in this case) while the sampling distribution would have been centered at the population parameter (\\(\\beta_0\\)), the mean mercury level among all Florida lakes, which is unknown. The important things, however, is that the variability in the bootstrap distribution gives a good approximation of the amount of variability in the sampling distribution, so we can use the standard deviation of the bootstrap distribution (called bootstrap standard error) in our confidence interval calculation.\nBootstrap Standard Error\nWe calculate the standard deviation of this bootstrap distribution, which is an estimate of the standard error of the sample mean, \\(\\bar{x}\\). It measures how much the mean mercury level varies between samples of size 53.\n\nSE_mean &lt;- sd(Lakes_Bootstrap_Mean$Intercept)\nSE_mean\n\n[1] 0.045823\n\n\nNotice that the standard error of the mean is much less than the sample standard deviation of 0.341.\nInterpretations of sample standard deviation and standard error of the mean\n\nThe sample standard deviation (0.341) measures the amount of variability in mercury levels between the 53 individual lakes in our sample.\nThe standard error of the mean (0.045823) measures the amount of variability in sample mean mercury levels between different samples of size 53.\n\nThere is more variability between mercury levels in individual lakes than there is between average mercury levels in different samples of size 53.\n95% Confidence Interval:\nWe calculate an approximate 95% confidence interval using the formula:\n\\[\n\\text{Statistic} \\pm 2\\times\\text{Standard Error}\n\\]\nIn this case, the statistic of interest is the sample mean \\(b_0=0.527\\). The approximate confidence interval is\n\\[\n\\begin{aligned}\n& \\bar{x} \\pm 2\\times\\text{SE}(\\bar{x}) \\\\\n& = 0.527 \\pm 2\\times\\text{0.045823}\n\\end{aligned}\n\\]\n\n\n       name     lower    upper level method  estimate margin.of.error\n1 Intercept 0.4375052 0.617128  0.95 stderr 0.5271698      0.08981142\n\n\nWe are 95% confident that the average mercury level among all Florida lakes is between 0.44 and 0.62 parts per million.\nThe 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below.\n\nLakes_Bootstrap_Mean_Plot + \n  geom_segment(aes(x = CI$lower, xend = CI$upper, y=50, yend=50), \n               color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nIt is important to note that we are not saying that we are 95% confident that an individual lake lie in this range, or that 95% of all individual lakes lie in this range. We are only saying that we are confident that the average mercury level among all lakes lies in this range. A confidence interval is a statement about a population parameter (in this case the average mercury level), rather than about individual lakes in the population. Since there is more variability about individual lakes than overall averages, we’ll need to make a wider interval when talking about the mercury level for an individual lake.\nPercentile Confidence Intervals\nThe confidence interval we obtained by taking \\(\\text{Estimate} \\pm 2\\times\\text{Standard Error}\\) is called a standard error confidence interval. This method is appropriate when the sampling distribution for our statistic (which we approximate using the bootstrap distribution) is symmetric and bell-shaped. An alternate and more general method for obtaining a confidence interval via bootstrapping is called the bootstrap percentile confidence interval. In this approach, we take the middle 95% of the values in the bootstrap distribution.\nThe percentile bootstrap confidence interval is implemented by setting method=percentile.\n\nCI_SE &lt;- confint(Lakes_Bootstrap_Mean, level = 0.95, parm=c(\"Intercept\"), method = \"percentile\")\nCI_SE\n\n       name    lower     upper level     method  estimate\n1 Intercept 0.440566 0.6190566  0.95 percentile 0.5271698\n\n\nNotice that the standard error confidence interval closely matches the percentile interval. This will be true when the bootstrap distribution is symmetric and bell-shaped. When the bootstrap distribution is not symmetric and bell-shaped, then the percentile interval will be more reliable than the standard error interval.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch3.html#bootstrapping-model-coefficients",
    "href": "Ch3.html#bootstrapping-model-coefficients",
    "title": "3  Simulation-Based Inference",
    "section": "3.3 Bootstrapping Model Coefficients",
    "text": "3.3 Bootstrapping Model Coefficients\nWe’ve seen how to use bootstrapping to obtain confidence intervals for the mean mercury level among all Florida lakes. We can use the same process to obtain confidence intervals for other statistics, such as coefficients \\(b_0\\), \\(b_1\\), representing differences in means, or slopes, like we saw in Chapter 2.\n\n3.3.1 CI for Difference in Means\nSuppose we want to compare mercury levels for lakes in Northern Florida to lakes in Southern Florida.\nThe boxplot shows and table below describe the distribution of mercury levels for lakes in Northern Florida, compared to Southern Florida.\n\nLakesBP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=Mercury, fill=Location)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + coord_flip() + theme_bw()\nLakesBP\n\n\n\n\n\n\n\n\n\nLakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(Mercury), \n                                                  StDevHg=sd(Mercury), \n                                                  N=n())\nLakesTable\n\n# A tibble: 2 × 4\n  Location MeanHg StDevHg     N\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1 N         0.425   0.270    33\n2 S         0.696   0.384    20\n\n\nIn our sample of 33 Northern Lakes and 20 Southern Lakes, we saw a difference of 0.27 ppm. We’ll calculate a confidence interval to estimate how big or small this difference could be among all Florida lakes.\nWe’ll use a statistical model to calculate the average mercury levels in Northern and Southern Florida.\n\\[\n\\widehat{\\text{Mercury}} = b_0 +b_1\\times{\\text{South}}\n\\]\n\n\\(b_0\\) represents the mean mercury level for lakes in North Florida, and\n\n\\(b_1\\) represents the mean difference in mercury level for lakes in South Florida, compared to North Florida\n\nThe estimates for corresponding to the original sample are shown below.\n\nM_Lakes_merc_loc &lt;- lm(data=FloridaLakes, Mercury~Location)\nM_Lakes_merc_loc\n\n\nCall:\nlm(formula = Mercury ~ Location, data = FloridaLakes)\n\nCoefficients:\n(Intercept)    LocationS  \n     0.4245       0.2720  \n\n\nA confidence interval for \\(b_0\\) will tell us a range of plausible values for the mean mercury level among all lakes in Northern Florida.\nA confidence interval for \\(b_1\\) will tell us a range of plausible values for the mean mercury level among all lakes in Southern Florida.\nWe can obtain these confidence intervals by fitting a regression model to each of our bootstrap samples and recording the values of the statistics \\(b_0\\) and \\(b_1\\) in the bootstrap samples.\n\n#First Bootstrap Sample \nBootstrap1 &lt;- resample(FloridaLakes, replace = TRUE)\nBootstrap1 |&gt; arrange(ID)\n\n# A tibble: 53 × 8\n      ID Lake            pH Mercury Chlorophyll Location Depth   orig.id\n   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1     1 Alligator      6.1    1.23         0.7 S        Medium  1      \n 2     1 Alligator      6.1    1.23         0.7 S        Medium  1      \n 3     4 Blue Cypress   6.9    0.44         3.5 S        Shallow 4      \n 4     4 Blue Cypress   6.9    0.44         3.5 S        Shallow 4      \n 5     5 Brick          4.6    1.2          1.8 S        Medium  5      \n 6     6 Bryant         7.3    0.27        44.1 N        Shallow 6      \n 7     6 Bryant         7.3    0.27        44.1 N        Shallow 6      \n 8     7 Cherry         5.4    0.48         3.4 N        Shallow 7      \n 9    11 Dorr           5.4    0.71        14.9 N        Medium  11     \n10    11 Dorr           5.4    0.71        14.9 N        Medium  11     \n# ℹ 43 more rows\n\n\nBootstrap Sample 1\n\nBootstrap1_BP &lt;- ggplot(data=Bootstrap1, aes(x=Location, y=Mercury, fill=Location)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + coord_flip()\nBootstrap1_BP\n\n\n\n\n\n\n\n\n\nBootstrap1_BP &lt;- Bootstrap1 %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(Mercury), \n                                                  StDevHg=sd(Mercury), \n                                                  N=n())\nBootstrap1_BP\n\n# A tibble: 2 × 4\n  Location MeanHg StDevHg     N\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1 N         0.488   0.231    32\n2 S         0.7     0.310    21\n\n\nWe fit a regression model to the bootstrap sample and calculate the regression coefficients. We’re interested in the second coefficient, \\(b_1\\), which represents the mean difference between lakes in Southern and Northern Florida.\n\n# Fit a linear model\nM_Lakes_merc_loc_Boot1 &lt;- lm(Mercury ~ Location, data = Bootstrap1)\nM_Lakes_merc_loc_Boot1\n\n\nCall:\nlm(formula = Mercury ~ Location, data = Bootstrap1)\n\nCoefficients:\n(Intercept)    LocationS  \n     0.4878       0.2122  \n\n\nThe bootstrap estimates are \\(b_0\\) = 0.4878125 and \\(b_1\\) = 0.2121875.\nBootstrap Sample 2\n\n# Second Bootstrap Sample\n\nBootstrap2 &lt;- resample(FloridaLakes, replace = TRUE)\nBootstrap2 |&gt; arrange(ID)\n\n# A tibble: 53 × 8\n      ID Lake        pH Mercury Chlorophyll Location Depth   orig.id\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1     2 Annie      5.1    1.33         3.2 S        Deep    2      \n 2     5 Brick      4.6    1.2          1.8 S        Medium  5      \n 3     6 Bryant     7.3    0.27        44.1 N        Shallow 6      \n 4     6 Bryant     7.3    0.27        44.1 N        Shallow 6      \n 5     7 Cherry     5.4    0.48         3.4 N        Shallow 7      \n 6     7 Cherry     5.4    0.48         3.4 N        Shallow 7      \n 7     8 Crescent   8.1    0.19        33.7 N        Shallow 8      \n 8    10 Dias       6.4    0.81        22.5 N        Deep    10     \n 9    10 Dias       6.4    0.81        22.5 N        Deep    10     \n10    11 Dorr       5.4    0.71        14.9 N        Medium  11     \n# ℹ 43 more rows\n\n\n\nBootstrap2_BP &lt;- ggplot(data=Bootstrap2, aes(x=Location, y=Mercury, fill=Location)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + coord_flip()\nBootstrap2_BP\n\n\n\n\n\n\n\n\n\nBootstrap2_BP &lt;- Bootstrap2 %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(Mercury), \n                                                  StDevHg=sd(Mercury), \n                                                  N=n())\nBootstrap2_BP\n\n# A tibble: 2 × 4\n  Location MeanHg StDevHg     N\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1 N         0.455   0.276    42\n2 S         0.807   0.363    11\n\n\nWe fit a regression model to the bootstrap sample and calculate the regression coefficients. We’re interested in the second coefficient, \\(b_1\\), which represents the mean difference between lakes in Southern and Northern Florida.\n\n# Fit a linear model\nM_Lakes_merc_loc_Boot2 &lt;- lm(Mercury ~ Location, data = Bootstrap2)\nM_Lakes_merc_loc_Boot2\n\n\nCall:\nlm(formula = Mercury ~ Location, data = Bootstrap2)\n\nCoefficients:\n(Intercept)    LocationS  \n     0.4545       0.3527  \n\n\nThe bootstrap estimates are \\(b_1\\) = 0.4545238 and \\(b_1\\) = 0.3527489.\nWe’ll now take 10,000 different bootstrap samples and look at the bootstrap distribution for \\(b_1\\), the difference in mean mercury levels between lakes in Southern and Northern Florida\n\nM_Lakes_merc_loc_Bootstrap &lt;- do(10000) * lm(Mercury ~ Location, data = resample(FloridaLakes))\n\nThe bootstrap distribution for the difference in means, \\(b_0\\), and \\(b_1\\) are shown below.\n\n# plot for b0\nM_Lakes_merc_loc_Bootstrap_Plot_b0 &lt;- ggplot(data=M_Lakes_merc_loc_Bootstrap, aes(x=Intercept)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\", binwidth=0.02) +\n  xlab(\"b0: Mean mercury in Northern Fla.\") + ylab(\"Frequency\") +\n  ggtitle(\"Bootstrap b0\") + \n  theme(legend.position = \"none\") + theme_bw()\n\n# plot for b1\nM_Lakes_merc_loc_Bootstrap_Plot_b1 &lt;- ggplot(data=M_Lakes_merc_loc_Bootstrap, aes(x=LocationS)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\", binwidth=0.02) +\n  xlab(\"b1: Mean difference between southern and northern Fla.\") + ylab(\"Frequency\") +\n  ggtitle(\"Bootstrap b1\") + \n  theme(legend.position = \"none\") + theme_bw()\n\ngrid.arrange(M_Lakes_merc_loc_Bootstrap_Plot_b0, M_Lakes_merc_loc_Bootstrap_Plot_b1)\n\n\n\n\n\n\n\n\nSince both distributions are symmetric and bell-shaped, we can calculate standard error confidence intervals. The confint command gives us standard errors and confidence intervals.\nThe standard error for \\(b_0\\) is\n\nSE_b0 &lt;- sd(M_Lakes_merc_loc_Bootstrap$Intercept)\nSE_b0\n\n[1] 0.04653398\n\n\nThis tells us how much the mean mercury level among lakes in Northern Florida would vary between different samples of 20 Northern lakes.\nA 95% confidence interval for the mean mercury level among all lakes in Northern Florida is given by\n\\[\n\\begin{aligned}\n& b_0 \\pm 2\\times\\text{SE}(b_0) \\\\\n& = 0.425 \\pm 2\\times{0.046}\n\\end{aligned}\n\\]\nThe standard error for \\(b_1\\) is\n\nSE_b0 &lt;- sd(M_Lakes_merc_loc_Bootstrap$Intercept)\nSE_b0\n\n[1] 0.04653398\n\n\nThis tells us how much the difference in mean mercury level between lakes in Northern and Southern Florida would vary between different samples of 53 lakes.\n\nSE_b1 &lt;- sd(M_Lakes_merc_loc_Bootstrap$LocationS)\nSE_b1\n\n[1] 0.09593809\n\n\n\\[\n\\begin{aligned}\n& b_1 \\pm 2\\times\\text{SE}(b_1) \\\\\n& = 0.271 \\pm 2\\times{0.097}\n\\end{aligned}\n\\]\nWe can calculate these intervals automatically in R using the confint command. When calculating confidence intervals for a regression model we add parm = c(\"Intercept\", \"LocationS\") to tell it to return intervals for the interval and the estimate associated with LocationS. If we leave out the parm argument, R will return additional confidence intervals that we aren’t interested in here.\n\nCI &lt;- confint(M_Lakes_merc_loc_Bootstrap, parm = c(\"Intercept\", \"LocationS\"),\n              level = 0.95, method = \"se\")\nCI\n\n       name      lower     upper level method  estimate margin.of.error\n1 Intercept 0.33333149 0.5157413  0.95 stderr 0.4245455      0.09120492\n2 LocationS 0.08256301 0.4586334  0.95 stderr 0.2719545      0.18803521\n\n\nWe add the confidence intervals to the plots of the bootstrap distributions below.\n\n# CI plot for intercept b1\nCI_Plot_b0 &lt;- M_Lakes_merc_loc_Bootstrap_Plot_b0 + \n                geom_segment(aes(x=CI$lower[1], xend=CI$upper[1] , y=50, yend=50), \n                color=\"gold\", size=10, alpha=0.01) \n\n# CI plot for b1\nCI_Plot_b1 &lt;- M_Lakes_merc_loc_Bootstrap_Plot_b1 + \n                geom_segment(aes(x=CI$lower[2], xend=CI$upper[2] , y=50, yend=50), \n                color=\"gold\", size=10, alpha=0.01)\n\ngrid.arrange(CI_Plot_b0, CI_Plot_b1)\n\n\n\n\n\n\n\n\nWe are 95% confident that the mean mercury level among all lakes in Northern Florida is between 0.33 and 0.52 ppm.\nWe are 95% confident that the mean mercury level among all lakes in Southern Florida is between 0.46 and 0.46 higher than the mean mercury level among all lakes in Northern Florida.\n\n\n3.3.2 CI for Regression Slope\nNow, suppose we also want to investigate whether there is a relationship between a lake’s mercury level and pH. We’ll add pH to the model.\nThe model equation is:\n\\[\n\\widehat{\\text{Mercury}} = b_0 + b_1\\times{\\text{South}} + b_2\\times{\\text{pH}}\n\\]\nIn this model:\n- \\(b_0\\) represents the mean mercury level among all lakes in Northern Florida with pH level 0. Since lakes will not have pH level 0 in reality, the intercept no longer has a meaningful interpretation.\n- \\(b_1\\) represents the difference in mean mercury level among lakes in Southern and Northern Florida, assuming they have the same pH.\n- \\(b_2\\) represents the expected change in mercury level for each 1-unit increase in pH, assuming location is the same.\nThe scatterplot displays the relationship between mercury level and pH, colored by location, with parallel regression lines fit to the data.\n\nggplot(data=FloridaLakes, aes(y=Mercury, x=pH, color=Location)) + \n  geom_point() + \n  geom_parallel_slopes(se=FALSE) +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe see that mercury tends to decrease as pH increases.\nWe fit the model in R and display the estimates \\(b_0\\), \\(b_1\\), and \\(b_2\\).\n\nM_Lakes_merc_loc_pH &lt;- lm(data=FloridaLakes, Mercury ~ Location + pH)\nM_Lakes_merc_loc_pH\n\n\nCall:\nlm(formula = Mercury ~ Location + pH, data = FloridaLakes)\n\nCoefficients:\n(Intercept)    LocationS           pH  \n     1.4071       0.2552      -0.1481  \n\n\n\nOn average, lakes with pH level 0 are expected to have a mercury level of 1.4 ppm. Since no lake has a pH of 0, this is not a meaningful interpretation.  \nOn average, lakes in Southern Florida are expected to have a mercury level 0.255 ppm higher than lakes in Northern Florida with the same pH.\n\nFor each one-unit increase in pH, mercury level is expected to decrease by 0.15 ppm, assuming location is the same.\n\nAs before, \\(b_0\\), \\(b_1\\), and \\(b_2\\) are estimates calculated from a single sample of 53 lakes. We can use them as estimates of the coefficients we would see if we were able to fit a regression model to all Florida lakes.\nWe’ll use bootstrapping to determine how much these estimates could vary between samples, and to obtain confidence intervals for the quantities of interest.\nBootstrap Sample 1\nWe select a bootstrap sample and plot the relationship between Mercury, pH, and location.\n\n#First Bootstrap Sample \nBootstrap1 &lt;- resample(FloridaLakes, replace = TRUE)\n\n#Plot Bootstrap Sample\nggplot(data=Bootstrap1, aes(y=Mercury, x=pH, color=Location)) + \n  geom_point() + \n  geom_parallel_slopes(se=FALSE) +\n  theme_bw()\n\n\n\n\n\n\n\n\nModel estimates for the bootstrap sample are shown below.\n\n# Fit a linear model\nM_Lakes_merc_pH_Boot1 &lt;- lm(Mercury ~ Location + pH, data = Bootstrap1)\nM_Lakes_merc_pH_Boot1\n\n\nCall:\nlm(formula = Mercury ~ Location + pH, data = Bootstrap1)\n\nCoefficients:\n(Intercept)    LocationS           pH  \n     1.4862       0.1830      -0.1605  \n\n\nBootstrap Sample 2\nWe select a bootstrap sample and plot the relationship between Mercury, pH, and location.\n\n#First Bootstrap Sample \nBootstrap2 &lt;- resample(FloridaLakes, replace = TRUE)\n\n#Plot Bootstrap Sample\nggplot(data=Bootstrap2, aes(y=Mercury, x=pH, color=Location)) + \n  geom_point() + \n  geom_parallel_slopes(se=FALSE) +\n  theme_bw()\n\n\n\n\n\n\n\n\nModel estimates for the bootstrap sample are shown below.\n\n# Fit a linear model\nM_Lakes_merc_pH_Boot2 &lt;- lm(Mercury ~ Location + pH, data = Bootstrap2)\nM_Lakes_merc_pH_Boot2\n\n\nCall:\nlm(formula = Mercury ~ Location + pH, data = Bootstrap2)\n\nCoefficients:\n(Intercept)    LocationS           pH  \n     1.5792       0.2737      -0.1743  \n\n\nWe’ll now take 10,000 different bootstrap samples and look at the bootstrap distribution for \\(b_1\\), the difference in mean mercury levels between lakes in Southern and Northern Florida\n\nM_Lakes_merc_loc_pH_Bootstrap &lt;- do(10000) * lm(Mercury ~ Location + pH, data = resample(FloridaLakes))\n\nWe’ll focus on the bootstrap distribution for \\(b_2\\), pertaining to the effect of pH.\n\n# plot for b0\nM_Lakes_merc_loc_pH_Bootstrap_Plot_b2 &lt;- ggplot(data=M_Lakes_merc_loc_pH_Bootstrap, aes(x=pH)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\", binwidth=0.01) +\n  xlab(\"b2: Effect of pH\") + ylab(\"Frequency\") +\n  ggtitle(\"Bootstrap Estimate b2\") + \n  theme(legend.position = \"none\") + theme_bw()\n\nM_Lakes_merc_loc_pH_Bootstrap_Plot_b2\n\n\n\n\n\n\n\n\nAgain, notice that the bootstrap distribution is symmetric and bell-shaped, so we can calculate a confidence interval using the standard error method.\nThe standard error for \\(b_2\\) is\n\nSE_b2 &lt;- sd(M_Lakes_merc_loc_pH_Bootstrap$pH)\nSE_b2\n\n[1] 0.02389449\n\n\nThis tells us how much the slope associated with pH would vary between different samples of 53 lakes.\nA 95% confidence interval is\n\\[\n\\begin{aligned}\n& b_2 \\pm 2\\times\\text{SE}(b_2) \\\\\n& = -0.15 \\pm 2\\times{0.024}\n\\end{aligned}\n\\]\nThe confint() command displays this confidence interval, as well as those associated with \\(b_0\\) and \\(b_1\\).\n\nCI &lt;- confint(M_Lakes_merc_loc_pH_Bootstrap, parm = c(\"Intercept\", \"LocationS\", \"pH\"),\n              level = 0.95, method = \"se\")\nCI\n\n       name      lower      upper level method   estimate margin.of.error\n1 Intercept  1.0913270  1.7178923  0.95 stderr  1.4071357      0.31328265\n2 LocationS  0.1133657  0.3984815  0.95 stderr  0.2551666      0.14255789\n3        pH -0.1944582 -0.1007935  0.95 stderr -0.1481292      0.04683234\n\n\nThe confidence interval associated with \\(b_2\\), the effect of pH is shown below.\n\n# CI plot for intercept b2\nCI_Plot_b2 &lt;- M_Lakes_merc_loc_pH_Bootstrap_Plot_b2 + \n                geom_segment(aes(x=CI$lower[3], xend=CI$upper[3] , y=50, yend=50), \n                color=\"gold\", size=10, alpha=0.01) \nCI_Plot_b2\n\n\n\n\n\n\n\n\nWe are 95% confident that for each one-unit increase in pH, mercury level is expected to decrease between about 0.1 and 0.19 ppm.\n\n\n3.3.3 CI for Regression Response\nIn addition to calculating a confidence interval for the quantities associated with our estimates \\(b_0\\), \\(b_1\\), and \\(b_2\\), we can also calculate a confidence interval for the average mercury level among all lakes with a given location and pH.\nWe’ll calculate a confidence interval for the average mercury level among all lakes in Southern Florida with a neutral pH level of 7.\nThe regression model equation is\n\\[\n\\begin{aligned}\n\\widehat{\\text{Mercury}} & = b_0 + b_1\\times\\text{LocationS} + b_2\\times\\text{pH}\n\\end{aligned}\n\\]\nso the expected mercury level among all lakes in Southern Florida with \\(\\text{pH} = 7\\) is\n\\[\nb_0 + b_1 + 7b_2\n\\]\nPlugging in the model estimates of \\(b_0\\), \\(b_1\\), and \\(b_2\\),\n\\[\n\\begin{aligned}\n\\widehat{\\text{Mercury}} & = b_0 + b_1\\times\\text{LocationS} + b_2\\times\\text{pH}  \\\\\n\\ & = 1.407 + 0.255(1) - 0.148(7) \\\\\n& \\approx 0.626\n\\end{aligned}\n\\]\nWe can also calculate this using the predict function.\n\npredict(M_Lakes_merc_loc_pH, newdata = data.frame(Location = \"S\", pH=7))\n\n       1 \n0.625398 \n\n\nWe estimate that the average mercury level among lakes in Southern Florida with pH of 7 is 0.626 ppm.\nThis estimate is, however, a sample statistic like all the others we’ve seen. It was calculated from just our sample of 53 lakes, and if we had a different sample, we would have gotten a different estimate. Thus, we need to provide a confidence interval for mean mercury level among all lakes in South Florida with pH 7. We can do that using bootstrapping.\nBootstrap Sample 1\nWe select a bootstrap sample and plot the relationship between Mercury, pH, and location.\n\n#First Bootstrap Sample \nBootstrap1 &lt;- resample(FloridaLakes, replace = TRUE)\n\n#Fit regression model to bootstrap sample\nM_Lakes_merc_loc_pH_Boot1 &lt;- lm(Mercury ~ Location + pH, data=Bootstrap1)\n\n# predict mercury level\npredict(M_Lakes_merc_loc_pH_Boot1, newdata=data.frame(Location = \"S\", pH=7))\n\n        1 \n0.6315449 \n\n\nBased on this bootstrap sample, the mean mercury level among lakes in Southern Florida is estimated to be 0.625398 ppm.\nBootstrap Sample 2\nWe select a bootstrap sample and plot the relationship between Mercury, pH, and location.\n\nBootstrap2 &lt;- resample(FloridaLakes, replace = TRUE)\n\n#Fit regression model to bootstrap sample\nM_Lakes_merc_loc_pH_Boot2 &lt;- lm(Mercury ~ Location + pH, data=Bootstrap2)\n\n# predict mercury level\npredict(M_Lakes_merc_loc_pH_Boot2, newdata=data.frame(Location = \"S\", pH=7))\n\n        1 \n0.5106231 \n\n\nBased on the second bootstrap sample, the mean mercury level among lakes in Southern Florida is estimated to be 0.5106231 ppm.\nWe’ll now take 10,000 different bootstrap samples and look at the bootstrap distribution for \\(b_0 + b_1 + 7b_2\\), the estimated average mercury level among all lakes in Southern Florida with pH = 7.\n\nM_Lakes_merc_locS_pH7_Bootstrap &lt;- do(10000) * predict(lm(Mercury ~ Location + pH, data = resample(FloridaLakes)), newdata=data.frame(Location=\"S\", pH=7))\n\nThe bootstrap distribution for average mercury level among all lakes in Southern Florida with pH = 7 is shown below.\n\n# plot for b0 + b_1 + 7*b_2\nM_Lakes_merc_locS_pH7_Bootstrap_Plot &lt;- ggplot(data=M_Lakes_merc_locS_pH7_Bootstrap, aes(x=X1)) +  \n  geom_histogram(color=\"white\", fill=\"lightblue\", binwidth=0.01) +\n  xlab(\"Estimated Mercury\") + ylab(\"Frequency\") +\n  ggtitle(\"Bootstrap Estimate b0 + b1 + 7*b2\") + \n  theme(legend.position = \"none\") + theme_bw()\n\nM_Lakes_merc_locS_pH7_Bootstrap_Plot\n\n\n\n\n\n\n\n\nAgain, notice that the bootstrap distribution is symmetric and bell-shaped, so we can calculate a confidence interval using the standard error method.\nThe standard error for \\(b_0 + b_1 + 7b_2\\) is\n\nSE_Est &lt;- sd(M_Lakes_merc_locS_pH7_Bootstrap$X1)\nSE_Est\n\n[1] 0.05969632\n\n\nThis tells us how much the estimated mercury level among lakes in Southern Florida with pH=7 would vary between different samples of 53 lakes. Note, this is not the same things as the variability between individual lakes with pH 7.\nA 95% confidence interval for mean mercury level among lakes in Southern Florida with pH=7 is\n\\[\n\\begin{aligned}\n& b_0 + b_1 + 7b_2 \\pm 2\\times\\text{SE}(b_0 + b_1 + 7b_2) \\\\\n& = 0.625 \\pm 2\\times{0.059}\n\\end{aligned}\n\\]\nWe can also calculate the confidence interval using the confint() command below.\n\nM_Lakes_merc_locS_pH7_Bootstrap_CI &lt;- do(10000) * predict(lm(Mercury ~ Location + pH, data = resample(FloridaLakes)), newdata=data.frame(Location=\"S\", pH=7), interval = \"confidence\", level=0.95)\n\nCI &lt;- confint(M_Lakes_merc_locS_pH7_Bootstrap_CI, parm = \"fit\", method = \"stderr\")\nCI\n\n  name     lower     upper level method estimate margin.of.error\n1  fit 0.5118066 0.7411329  0.95 stderr 0.625398       0.1146632\n\n\n\nM_Lakes_merc_locS_pH7_Bootstrap_Plot  + \n                geom_segment(aes(x=CI$lower, xend=CI$upper , y=50, yend=50), \n                color=\"gold\", size=10, alpha=0.01) \n\n\n\n\n\n\n\n\nWe are 95% confident that the mean mercury level among lakes in Southern Florida with pH=7 is between 0.51 and 0.74 ppm.\nIt is important to note that we are not saying that we are 95% confident that the mercury level of an individual lake in South Florida with a pH of 7 will lie in this range. Confidence intervals are about population parameters like averages, rather than about individuals. An interval pertaining to mercury levels in individual lakes would need to be wider than this one. We’ll talk about how to obtain these kinds of intervals later in the course.\n\n\n3.3.4 Bootstrapping Cautions\nWhile bootstrapping is a popular and robust procedure for calculating confidence intervals, it does have cautions and limitations. We should be sure to use the bootstrap procedure appropriate for our context.\n\nA standard-error bootstrap interval is appropriate when the sampling distribution for our statistic is roughly symmetric and bell-shaped. When this is not true, a percentile bootstrap interval can be used as long as there are no gaps or breaks in the bootstrap distribution.\nIn situations where there are gaps and breaks in the bootstrap distribution, then the bootstrap distribution may not be a reasonable approximation of the sampling distribution we are interested in.\nWhen working with a categorical variable, it is a good idea to resample in each group, with sizes matching those in the original samples. For example, in the Florida lakes study, rather than resampling 53 lakes, we would ideally resample 33 Northern lakes and 20 Southern lakes, matching the numbers from the original study. The mosaic package, which we used to do bootstrapping does not allow this option, unfortunately. In this case, the sample sizes are similar enough that this issue will not have a big impact on our results. If sample sizes within groups were small or very different, we may want to write our own code to do this.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch3.html#permutation-based-hypothesis-testing",
    "href": "Ch3.html#permutation-based-hypothesis-testing",
    "title": "3  Simulation-Based Inference",
    "section": "3.4 Permutation-Based Hypothesis Testing",
    "text": "3.4 Permutation-Based Hypothesis Testing\n\n3.4.1 Mercury in Lakes: N vs S\nRecall the comparison of mercury levels in lakes in Northern Florida, compared to Southern Florida. The boxplot and table below, also seen in the previous section, show the distribution of mercury levels among the 33 northern and 20 southern lakes in the sample.\n\nLakes_merc_loc_BP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=Mercury, fill=Location)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + \n  theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + \n  coord_flip() + theme_bw()  \nLakes_merc_loc_BP\n\n\n\n\n\n\n\n\n\nLakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% \n                                summarize(MeanHg=mean(Mercury), \n                                          StDevHg=sd(Mercury),  \n                                          N=n())\nkable(LakesTable)\n\n\n\n\nLocation\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n0.4245455\n0.2696652\n33\n\n\nS\n0.6965000\n0.3838760\n20\n\n\n\n\n\nWe see that on average mercury levels were higher among the southern lakes than the northern ones, a difference of \\(0.697-0.445= 0.272\\) ppm.\nAs we’ve seen, we can use a statistical model to estimate a lake’s mercury level, using its location (N or S) as our explanatory variable.\n\\(\\widehat{\\text{Hg}} = b_0 +b_1\\times\\text{South}\\)\n\n\\(b_0\\) represents the mean mercury level for lakes in North Florida, and\n\n\\(b_1\\) represents the mean difference in mercury level for lakes in South Florida, compared to North Florida\n\nFitting the model in R, we obtain the estimates for \\(b_0\\) and \\(b_1\\).\n\nM_Lakes_loc &lt;- lm(data=FloridaLakes, Mercury ~ Location)\nM_Lakes_loc\n\n\nCall:\nlm(formula = Mercury ~ Location, data = FloridaLakes)\n\nCoefficients:\n(Intercept)    LocationS  \n     0.4245       0.2720  \n\n\n\\[\n\\widehat{\\text{Hg}} = 0.42+0.27\\times\\text{South}\n\\]\n\n\\(b_1 = 0.272= 0.6965 - 0.4245\\) is equal to the difference in mean mercury levels between Northern and Southern lakes. (We’ve already seen that for categorical variables, the least-squares estimate is the mean, so this makes sense.)\nWe can use \\(b_1\\) to assess the size of the difference in mean mercury concentration levels.\n\n\nHypotheses and Key Question\nSince the lakes we observed are only a sample of 53 lakes out of more than 30,000, we cannot assume the difference in mercury concentration for all Northern vs Southern Florida lakes is exactly 0.272. Instead, we need to determine whether a difference of this size in our sample is large enough to provide evidence of a difference in average mercury level between all Northern and Southern lakes in Florida.\nOne possible explanation for us getting the results we did in our sample is that there really is no difference in average mercury levels between all lakes in Northern and Southern Florida, and we just happened, by chance, to select more lakes with higher mercury concentrations in Southern Florida than in Northern Florida. A different possible explanation is that there really is a difference in average mercury level between lakes in Northern and Southern Florida.\nIn a statistical investigation, the null hypothesis is the one that says there is no difference between groups , or no relationship between variables in the larger population, and that any difference/relationship observed in our sample occurred merely by chance. The alternative hypothesis contradicts the null hypothesis, stating that there is a difference/relationship.\nStated formally, the hypotheses are:\nNull Hypothesis: There is no difference in average mercury level between all lakes in Northern Florida and all lakes in Southern Florida.\nAlternative Hypothesis: There is a difference in average mercury level between all lakes in Northern Florida and all lakes in Southern Florida.\nA statistician’s job is to determine whether the data provide strong enough evidence to rule out the null hypothesis.\nThe question we need to investigate is:\n*“How likely is it that we would have observed a difference in means (i.e. a value of* \\(b_1\\)) as extreme as 0.6965-0.4245 = 0.272 ppm, merely by chance, if there is really no relationship between location and mercury level?”\n\n\nPermutation Test Procedure\nWe can answer the key question using a procedure known as a permutation test. In a permutation test, we randomly permute (or shuffle) the values of our explanatory variable to simulate a situation where there is no relationship between our explanatory and response variable. We observe whether it is plausible to observe values of a statistic (in this case the difference in means) as extreme or more extreme than what we saw in the actual data.\nWe’ll simulate situations where there is no relationship between location and mercury level, and see how often we observe a difference in means (\\(b_1\\)) as extreme as 0.272.\nProcedure:\n\nRandomly shuffle the locations of the lakes, so that any relationship between location and mercury level is due only to chance.\nCalculate the difference in mean mercury levels (i.e. value of \\(b_1\\)) in “Northern” and “Southern” lakes, using the shuffled data. The statistic used to measure the size of the difference or relationship in the sample is called the test statistic.\nRepeat steps 1 and 2 many (say 10,000) times, recording the test statistic (difference in means, \\(b_1\\)) each time.\nAnalyze the distribution of the test statistic (mean difference), simulated under the assumption that there is no relationship between location and mercury level. Look whether the value of the test statistic we observed in the sample (0.272) is consistent with values simulated under the assumption that the null hypothesis is true.\n\nThis simulation can be performed using this Rossman-Chance App.\n\n\nThree Permutations in R\nWe’ll use R to perform permutation test.\nFirst Permutation\nRecall these groups were randomly assigned, so the only differences in averages are due to random chance.\n\nLakes_shuffle_loc1 &lt;- FloridaLakes |&gt; mutate(ShuffledLocation = shuffle(Location))  \nhead(Lakes_shuffle_loc1)                                        \n\n# A tibble: 6 × 8\n     ID Lake            pH Mercury Chlorophyll Location Depth   ShuffledLocation\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;           \n1     1 Alligator      6.1    1.23         0.7 S        Medium  N               \n2     2 Annie          5.1    1.33         3.2 S        Deep    S               \n3     3 Apopka         9.1    0.04       128.  N        Medium  S               \n4     4 Blue Cypress   6.9    0.44         3.5 S        Shallow N               \n5     5 Brick          4.6    1.2          1.8 S        Medium  N               \n6     6 Bryant         7.3    0.27        44.1 N        Shallow N               \n\n\nNotice that the locations of the lakes have now been mixed up and assigned randomly. So, any relationship between location and mercury level will have occurred merely by chance.\nWe create a boxplot and calculate the difference in mean mercury levels for the shuffled data.\n\nLakes_shuffle_loc11_BP &lt;- ggplot(data=Lakes_shuffle_loc1, aes(x=`ShuffledLocation`, \n                                         y=Mercury, fill=`ShuffledLocation`)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + \n  ylim(c(0, 1.5)) + coord_flip() + theme_bw()\nLakes_shuffle_loc11_BP\n\n\n\n\n\n\n\n\n\nLakes_shuffle_loc1_Tab &lt;- Lakes_shuffle_loc1 %&gt;% group_by(`ShuffledLocation`) |&gt;\n                                        summarize(MeanHg=mean(Mercury), \n                                                  StDevHg=sd(Mercury),  \n                                                  N=n())\nkable(Lakes_shuffle_loc1_Tab)\n\n\n\n\nShuffledLocation\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n0.4539394\n0.3205361\n33\n\n\nS\n0.6480000\n0.3472539\n20\n\n\n\n\n\nWe fit a model to the shuffled data and observe the coefficient \\(b_1\\) representing the difference in means between the lakes randomly assigned to the Northern and Southern groups.\n\nlm(data=Lakes_shuffle_loc1, Mercury ~ ShuffledLocation)\n\n\nCall:\nlm(formula = Mercury ~ ShuffledLocation, data = Lakes_shuffle_loc1)\n\nCoefficients:\n      (Intercept)  ShuffledLocationS  \n           0.4539             0.1941  \n\n\nNotice that the sample means are not identical. We observe a difference of -0.1960606 just by chance associated with the assignment of the lakes to their random location groups.\nThis difference is considerably smaller than the difference of 0.272 that we saw in the actual data, suggesting that perhaps a difference as big as 0.272 would not be likely to occur by chance. Before we can be sure of this, however, we should repeat our simulation many times to get a better sense for how big of a difference we might reasonable expect to occur just by chance.\nSecond Permutation\nWe’ll repeat the procedure, again randomly shuffling the locations of lakes.\n\nLakes_shuffle_loc2 &lt;- FloridaLakes |&gt; mutate(ShuffledLocation = shuffle(Location))  \nhead(Lakes_shuffle_loc2)                                        \n\n# A tibble: 6 × 8\n     ID Lake            pH Mercury Chlorophyll Location Depth   ShuffledLocation\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;           \n1     1 Alligator      6.1    1.23         0.7 S        Medium  S               \n2     2 Annie          5.1    1.33         3.2 S        Deep    N               \n3     3 Apopka         9.1    0.04       128.  N        Medium  N               \n4     4 Blue Cypress   6.9    0.44         3.5 S        Shallow N               \n5     5 Brick          4.6    1.2          1.8 S        Medium  S               \n6     6 Bryant         7.3    0.27        44.1 N        Shallow N               \n\n\n\nLakes_shuffle_loc2_BP &lt;- ggplot(data=Lakes_shuffle_loc2, aes(x=`ShuffledLocation`, \n                                         y=Mercury, fill=`ShuffledLocation`)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + \n  ylim(c(0, 1.5)) + coord_flip() + theme_bw()\nLakes_shuffle_loc2_BP\n\n\n\n\n\n\n\n\n\nLakes_shuffle_loc2_Tab &lt;- Lakes_shuffle_loc2 %&gt;% group_by(`ShuffledLocation`) |&gt;\n                                        summarize(MeanHg=mean(Mercury), \n                                                  StDevHg=sd(Mercury),  \n                                                  N=n())\nkable(Lakes_shuffle_loc2_Tab)\n\n\n\n\nShuffledLocation\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n0.5130303\n0.3457735\n33\n\n\nS\n0.5505000\n0.3406260\n20\n\n\n\n\n\n\nlm(data=Lakes_shuffle_loc2, Mercury ~ ShuffledLocation)\n\n\nCall:\nlm(formula = Mercury ~ ShuffledLocation, data = Lakes_shuffle_loc2)\n\nCoefficients:\n      (Intercept)  ShuffledLocationS  \n          0.51303            0.03747  \n\n\nObserved Difference in means: -0.0369697.\nThird Permutation\nWe’ll repeat the procedure one more time, randomly shuffling the locations of lakes.\n\nLakes_shuffle_loc3 &lt;- FloridaLakes |&gt; mutate(ShuffledLocation = shuffle(Location))  \nhead(Lakes_shuffle_loc3)                                        \n\n# A tibble: 6 × 8\n     ID Lake            pH Mercury Chlorophyll Location Depth   ShuffledLocation\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;           \n1     1 Alligator      6.1    1.23         0.7 S        Medium  S               \n2     2 Annie          5.1    1.33         3.2 S        Deep    S               \n3     3 Apopka         9.1    0.04       128.  N        Medium  S               \n4     4 Blue Cypress   6.9    0.44         3.5 S        Shallow N               \n5     5 Brick          4.6    1.2          1.8 S        Medium  N               \n6     6 Bryant         7.3    0.27        44.1 N        Shallow S               \n\n\n\nLakes_shuffle_loc3_BP &lt;- ggplot(data=Lakes_shuffle_loc3, aes(x=`ShuffledLocation`, \n                                         y=Mercury, fill=`ShuffledLocation`)) + \n  geom_boxplot() +   geom_jitter() + ggtitle(\"Mercury Levels in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Mercury Level\") + theme(axis.text.x = element_text(angle = 90)) + \n  ylim(c(0, 1.5)) + coord_flip() + theme_bw()\nLakes_shuffle_loc3_BP\n\n\n\n\n\n\n\n\n\nLakes_shuffle_loc3_Tab &lt;- Lakes_shuffle_loc3 %&gt;% group_by(`ShuffledLocation`) |&gt;\n                                          summarize(MeanHg=mean(Mercury), \n                                                    StDevHg=sd(Mercury),  \n                                                    N=n())\nkable(Lakes_shuffle_loc3_Tab)\n\n\n\n\nShuffledLocation\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n0.4957576\n0.3290804\n33\n\n\nS\n0.5790000\n0.3624609\n20\n\n\n\n\n\n\nlm(data=Lakes_shuffle_loc3, Mercury ~ ShuffledLocation)\n\n\nCall:\nlm(formula = Mercury ~ ShuffledLocation, data = Lakes_shuffle_loc3)\n\nCoefficients:\n      (Intercept)  ShuffledLocationS  \n          0.49576            0.08324  \n\n\nObserved Difference in means: -0.0842424.\nSo far, we haven’t seen a difference anywhere near as big as the 0.272 we saw in the actual sample. But, to be sure, we’ll repeat the shuffling 10,000 times and see how often we get a difference as extreme as the one we saw in the sample.\n\n\nMany Permutations\nNow, we’ll perform 10,000 permutations and record the value of \\(b_1\\) (the difference in sample means) for each simulation.\nThe values of \\(b_1\\), the estimate pertaining to LocationS obtained for the first 10 permutations are shown below.\n\nLakes_merc_loc_perm_test &lt;- do(10000) * lm(Mercury ~ shuffle(Location), data = FloridaLakes)\n\n\nhead(Lakes_merc_loc_perm_test$LocationS, 10)\n\n [1] -0.197015152 -0.001878788  0.013378788 -0.074151515  0.055136364\n [6] -0.006696970 -0.053272727  0.130621212 -0.065318182 -0.034000000\n\n\nWe again see that none of these differences are as extreme as the 0.272 we saw in the actual data, suggesting that such a difference is unlikely to occur just by chance.\nThe histogram shows the distribution of differences in the group means observed in our simulation. The red lines indicate the difference we actually observed in the data (0.272), as well as an equally large difference in the opposite direction (-0.272).\n\nEstimate &lt;- 0.272\nExtreme &lt;- abs(Lakes_merc_loc_perm_test$LocationS) &gt; 0.272\n  \n  \nLakes_merc_loc_perm_test_plot &lt;- ggplot(data=Lakes_merc_loc_perm_test, aes(x=LocationS)) + \n  geom_histogram(aes(fill = Extreme), color=\"white\") + \n  scale_fill_manual(values = c(\"FALSE\" = \"blue\", \"TRUE\" = \"red\")) +\n  geom_vline(xintercept=c(Estimate, -1*Estimate), color=\"red\") + \n  xlab(\"Lakes: Simulated Value of b1\") + ylab(\"Frequency\") + \n  ggtitle(\"Distribution of b1 under assumption of no relationship\") +\n  theme(legend.position = \"none\") + theme_bw()\n\nLakes_merc_loc_perm_test_plot\n\n\n\n\n\n\n\n\nThe red lines are quite extreme, relative to the simulated values shown in the histogram. Based on the simulation, it is rare to obtain a difference as extreme as the 0.272 value we saw in the actual data, by chance when there is actually no difference in average mercury levels between Northern and Southern Florida lakes.\nWe calculate the precise number of simulations (out of 10,000) resulting in difference in means more extreme than 0.272.\n\nsum(~ (abs(LocationS) &gt; 0.272), data = Lakes_merc_loc_perm_test)\n\n[1] 30\n\n\nThe proportion of simulations resulting in difference in means more extreme than 0.272 is\n\nprop(~ (abs(LocationS) &gt; 0.272), data = Lakes_merc_loc_perm_test)\n\nprop_TRUE \n    0.003 \n\n\nWe only observed a difference between the groups as extreme or more extreme than the 0.272 difference we saw in the sample in a proportion of 30 of our simulations (less than 1%).\nThe probability of getting a difference in means as extreme or more extreme than 0.272 ppm by chance, when there is no relationship between location and mercury level is about 0.003.\nIt is very unlikely that we would have observed a result like we did by chance alone. Thus, we have strong evidence that there is a difference in average mercury level between lakes in Northern and Southern Florida. In this case, there is strong evidence that mercury level is higher in Southern Florida lakes than Northern Florida lakes.\nRecall that in the previous section, we found that we could be 95% confident that the mean mercury level among all lakes in Southern Florida is between 0.08 and 0.46 higher than the mean mercury level among all lakes in Northern Florida.\n\n\nTest-statistic and p-value\nThe statistic we use to measure the size of the difference or effect is called the test statistic. In this case, the test statistic is the difference in means.\nThe p-value represents the probability of getting a test statistic as extreme or more extreme than we did in our sample when the null hypothesis is true.\nIn this situation, the p-value represents the probability of observing a difference in sample means as extreme or more extreme than 0.272 if there is actually no difference in average mercury level among all lakes in Northern Florida, compared to Southern Florida.\nIn our study, the p-value was 0.003, which is very low. This provides strong evidence against the null hypothesis that there is no difference in average mercury levels between all Northern and Southern Florida lakes.\nA low p-value tells us that the difference in average Mercury levels that we saw in our sample is unlikely to have occurred by chance, providing evidence that there is indeed a difference in average Mercury levels between Northern and Southern lakes. When a p-value is small enough to suggest that the result was unlikely to have occurred by chance, the result is said to be statistically discernible.\nNote that it is fairly common for people to use the phrase statistically significant instead of statistically discernible. We will avoid this phrase because the word significant can be easily confused with important. Not all statistically discernible results are actually important in a practical sense. The p-value does not tell us anything about the size of the difference! Sometimes, we might get a small p-value even when difference is so small that it is not practically meaningful. This is especially true when we have a large sample size. In addition to a p-value, we should consider whether a difference is big enough to be meaningful in a practical way, before making any policy decisions.\nFor now, we can use the difference in sample means of 0.272 ppm as an estimate of the size of the difference. Based on our limited knowledge of mercury levels, this does seem big enough to merit further investigation, and possible action.\nAt this point, a reasonable question is “how small must a p-value be in order to provide evidence against the null hypothesis?” While it is sometimes common to establish strict cutoffs for what counts as a small p-value (such as \\(&lt;0.05\\)), the American Statistical Association does not recommend this. In reality, a p-value of 0.04 is practically no different than a p-value of 0.06. Rather than using strict cutoffs for what counts as small, it is better to interpreting p-values on a sliding scale, as illustrated in the diagram below. A p-value of 0.10 or less provides at least some evidence against a null hypothesis, and the smaller the p-value is, the stronger the evidence gets.\n\nknitr::include_graphics(\"pvals.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 Chlorophyll in Lakes: N vs S\nChlorophyll, which comes from plants, is also present in water. Unlike mercury, higher levels of chlorophyll are not dangerous, but the amount of chlorophyll can be an indicator of plant life in the lake. We’ll test whether we have evidence of a difference in mean amount of chlorophyll between lakes in Northern and Southern Florida.\nThe boxplot and table below show the distribution of chlorophyll levels among the 33 northern and 20 southern lakes in the sample.\n\nLakes_chlor_loc_BP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=Chlorophyll, fill=Location)) + \n  geom_boxplot(outliers=FALSE) +   geom_jitter() + ggtitle(\"Chlorophyll in Florida Lakes\") + \n  xlab(\"Location\") + ylab(\"Chlorophyll Amount in mg/m^3 \") + theme(axis.text.x = element_text(angle = 90)) + coord_flip()  + theme_bw()\nLakes_chlor_loc_BP\n\n\n\n\n\n\n\n\n\nLakes_chlor_loc_tab &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% \n                              summarize(MeanHg=mean(Chlorophyll), \n                                        StDevHg=sd(Chlorophyll),  \n                                        N=n())\nkable(Lakes_chlor_loc_tab)\n\n\n\n\nLocation\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n24.59697\n28.73836\n33\n\n\nS\n20.67500\n34.61171\n20\n\n\n\n\n\nWe see that on average mercury levels were higher among the lakes in northern Florida by about 4 mg/\\(m^3\\). We’ll use a permutation test to investigate whether such a difference could occur by random selection of lakes if there is really no relationship between chlorophyll and location.\nThe hypotheses are:\nNull Hypothesis: There is no difference in average chlorophyll amount between all lakes in Northern Florida and all lakes in Southern Florida.\nAlternative Hypothesis: There is a difference in average chlorophyll amount between all lakes in Northern Florida and all lakes in Southern Florida.\nThe model equation is\n\\(\\widehat{\\text{Chlorophyll}} = b_0 +b_1\\times\\text{South}\\)\n\n\\(b_0\\) represents the mean chlorophyll amount for lakes in North Florida, and\n\n\\(b_1\\) represents the mean difference in chlorophyll amount for lakes in South Florida, compared to North Florida\n\nFitting the model in R, we obtain the estimates for \\(b_0\\) and \\(b_1\\).\n\nM_Lakes_chlor_loc &lt;- lm(data=FloridaLakes, Chlorophyll ~ Location)\nM_Lakes_chlor_loc\n\n\nCall:\nlm(formula = Chlorophyll ~ Location, data = FloridaLakes)\n\nCoefficients:\n(Intercept)    LocationS  \n     24.597       -3.922  \n\n\n\\(b_1\\) gives the mean difference in chlorophyll between the Northern and Southern lakes in our sample. The question we need to investigate is:\n*“How likely is it that we would have observed a difference in means (i.e. a value of* \\(b_1\\)) as extreme as -3.9 mg/\\(m^3\\), merely by chance, if there is really no relationship between location and chlorophyll amount?”\nWe’ll perform 10,000 permutations and record the value of \\(b_1\\) (the difference in sample means) for each simulation, just as we did for mercury levels.\nThe values of \\(b_1\\), the estimate pertaining to LocationS obtained for the first 10 permutations are shown below.\n\nLakes_chlor_loc_perm_test &lt;- do(10000) * lm(Chlorophyll ~ shuffle(Location), data = FloridaLakes)\n\nThe histogram shows the distribution of differences in the group means observed in our simulation. The red lines indicate the difference we actually observed in the data (0.272), as well as an equally large difference in the opposite direction (-0.272).\n\nEstimate &lt;- -3.922\nExtreme &lt;- abs(Lakes_chlor_loc_perm_test$LocationS) &gt; 3.922\n  \n  \nLakes_chlor_loc_perm_test_plot &lt;- ggplot(data=Lakes_chlor_loc_perm_test, aes(x=LocationS)) + \n  geom_histogram(aes(fill = Extreme), color=\"white\") + \n  scale_fill_manual(values = c(\"FALSE\" = \"blue\", \"TRUE\" = \"red\")) +\n  geom_vline(xintercept=c(Estimate, -1*Estimate), color=\"red\") + \n  xlab(\"Lakes: Simulated Value of b1\") + ylab(\"Frequency\") + \n  ggtitle(\"Distribution of b1 under assumption of no relationship\") +\n  theme(legend.position = \"none\") + theme_bw()\n\nLakes_chlor_loc_perm_test_plot\n\n\n\n\n\n\n\n\nThis time, the red lines are not very extreme. The difference of -3.9 mg/\\(m^3\\) seen in our sample happens quite often, even in a situation where there is really no relationship between location and chlorophyll amount.\nOur p-value, represemtomg the proportion of simulations resulting in difference in means more extreme than 3.9 is\n\nprop(~ (abs(LocationS) &gt; 3.9), data = Lakes_chlor_loc_perm_test)\n\nprop_TRUE \n   0.6711 \n\n\nThis p-value represents the probability of observing a difference in means as extreme or more extreme than -3.9 \\(mg/m^3\\) if there is really no relationship between location and chlorophyll amount.\nSince the p-value is quite large (much bigger than common criteria like 0.05 or 0.10), we cannot rule out the null hypothesis. It would not be unusual to see data like we did in a situation where the null hypothesis is actually true. Thus, we do not have enough evidence to conclude that average chlorophyll amount differs between lakes in Northern and Southern Florida.\nJust because we cannot rule out, or reject, the null hypothesis does not mean that we should believe it is true. Afterall, we did observe an average difference of about 4 \\(mg/m^3\\). It’s just that given the size of our sample and the amount of variability in the data, we cannot be sure that this difference didn’t occur by chance associated with random selection of the lakes. Thus, while we do not reject the null hypothesis, we also shouldn’t accept it, or believe it to be true. We shouldn’t say that we believe that chlorophyll amount is the same in Northern and Southern Florida, only that we don’t have enough evidence to conclude that it differs. “Accepting” the null hypothesis is a common fallacy committed in hypothesis testing that has sometimes had disastrous implications in scientific research.\n\n\n3.4.3 Effect of pH on Mercury\nNext, we’ll investigate whether there is a relationship between a lake’s mercury level and it’s pH, a measure of the acidity of a lake. A pH of 7 is considered neutral, while a lower pH indicates that the water is more acidic and higher pH indicates that the water is more basic.\nThe plot below shows the relationship between a lake’s mercury level and pH.\n\nggplot(data=FloridaLakes, aes(y=Mercury, x=pH)) + \n  geom_point() + stat_smooth(method=\"lm\", se=FALSE) + \n  xlim(c(3, 10)) + ylim(c(0,1.5)) + theme_bw()\n\n\n\n\n\n\n\n\nThere seems to be a strong relationship between the two, but we should investigate whether such relationship could plausible occur in a sample of 53 lakes, in a situation where there is really no such relationship among all lakes.\nThe regression equation is\n\\[\n\\widehat{\\text{Mercury}} = b_0 + b_1\\times\\text{pH}\n\\]\nRegression estimates \\(b_0\\) and \\(b_1\\) are shown below.\n\nLakes_M_merc_pH &lt;- lm(data=FloridaLakes, Mercury~pH)\nLakes_M_merc_pH\n\n\nCall:\nlm(formula = Mercury ~ pH, data = FloridaLakes)\n\nCoefficients:\n(Intercept)           pH  \n     1.5309      -0.1523  \n\n\nFor quantitative explanatory variables, we can use the slope of the regression line \\(b_1\\) to measure the strength relationship between Mercury and pH. Based on our sample, for each one-unit increase in pH, mercury level is expected to decrease by 0.15 ppm.\nIf there was really no relationship, then the slope among all lakes would be 0. But, of course, we would not expect the slope in our sample to exactly match the slope for all lakes. Our question of interest is whether it is plausible that we could have randomly selected a sample resulting in a slope as extreme as 0.15 by chance, when there is actually no relationship between mercury and pH levels, among all lakes. In other words, could we plausible have drawn the sample of 53 lakes shown in blue from a population like the one in red, shown below?\n\n\n\n\n\n\n\n\n\nKey Question:\n\nHow likely is it that we would have observed a slope (i.e. a value of \\(b_1\\)) as extreme as 0.15 by chance, if there is really no relationship between mercury level and pH?\n\nNull Hypothesis: Among all Florida lakes, there is no relationship between mercury level and pH.\nAlternative Hypothesis: Among all Florida lakes, there is a relationship between mercury level and pH.\nProcedure:\n\nRandomly shuffle the pH values, so that any relationship between acceleration mercury and pH is due only to chance.\nFit a regression line to the shuffled data and record the slope of the regression line.\nRepeat steps 1 and 2 many (say 10,000) times, recording the slope (i.e. value of \\(b_1\\)) each time.\nAnalyze the distribution of slopes, simulated under the assumption that there is no relationship between mercury and pH. Look whether the actual slope we observed is consistent with the simulation results.\n\nWe’ll illustrate the first three permutations.\nFirst Permutation\nRecall these groups were randomly assigned, so the only differences in averages are due to random chance.\n\nLakes_shuffle_pH1 &lt;- FloridaLakes |&gt; mutate(ShuffledpH = shuffle(pH))  \nhead(Lakes_shuffle_pH1)                                 \n\n# A tibble: 6 × 8\n     ID Lake            pH Mercury Chlorophyll Location Depth   ShuffledpH\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;\n1     1 Alligator      6.1    1.23         0.7 S        Medium         7.8\n2     2 Annie          5.1    1.33         3.2 S        Deep           6.2\n3     3 Apopka         9.1    0.04       128.  N        Medium         7.8\n4     4 Blue Cypress   6.9    0.44         3.5 S        Shallow        7.2\n5     5 Brick          4.6    1.2          1.8 S        Medium         5.8\n6     6 Bryant         7.3    0.27        44.1 N        Shallow        5.8\n\n\nNotice that the pH’s of the lakes have now been mixed up and assigned randomly. So, any relationship between pH and mercury level will have occurred merely by chance.\nWe create a scatterplot and fit a regression line to the shuffled data.\nThe red line indicates the slope of the regression line fit to the shuffled data. The blue line indicates the regression line for the actual lakes in the sample, which has a slope of -0.15.\n\nggplot(data=Lakes_shuffle_pH1, aes(x=ShuffledpH, y=Mercury)) + \n  geom_point() + stat_smooth(method=\"lm\", se=FALSE, color=\"red\") + \n  xlim(c(3, 10)) + ylim(c(0,1.5)) + \n  geom_abline(slope=-0.1523, intercept=1.5309, color=\"blue\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nSlope of regression line from permuted data:\n\nM_Lakes_shuffle_pH1 &lt;- lm(data=Lakes_shuffle_pH1, Mercury~ShuffledpH)\nsummary(M_Lakes_shuffle_pH1)$coef[2]\n\n[1] -0.03920538\n\n\nWe observed a slight correlation just by chance, but nowhere near as large as the slope of -0.15 that we saw in the actual data.\nSecond Permutation\nWe’ll repeat the procedure, again randomly shuffling the pH’s of lakes.\nRecall these groups were randomly assigned, so the only differences in averages are due to random chance.\n\nLakes_shuffle_pH2 &lt;- FloridaLakes |&gt; mutate(ShuffledpH = shuffle(pH))  \nhead(Lakes_shuffle_pH2)                                   \n\n# A tibble: 6 × 8\n     ID Lake            pH Mercury Chlorophyll Location Depth   ShuffledpH\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;\n1     1 Alligator      6.1    1.23         0.7 S        Medium         7.8\n2     2 Annie          5.1    1.33         3.2 S        Deep           4.4\n3     3 Apopka         9.1    0.04       128.  N        Medium         7.4\n4     4 Blue Cypress   6.9    0.44         3.5 S        Shallow        6.1\n5     5 Brick          4.6    1.2          1.8 S        Medium         7.8\n6     6 Bryant         7.3    0.27        44.1 N        Shallow        5.5\n\n\n\nggplot(data=Lakes_shuffle_pH2, aes(x=ShuffledpH, y=Mercury)) + \n  geom_point() + stat_smooth(method=\"lm\", se=FALSE, color=\"red\") + \n  xlim(c(3, 10)) + ylim(c(0,1.5)) + \n  geom_abline(slope=-0.1523, intercept=1.5309, color=\"blue\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nSlope of regression line from permuted data:\n\nM_Lakes_shuffle_pH2 &lt;- lm(data=Lakes_shuffle_pH2, Mercury~ShuffledpH)\nsummary(M_Lakes_shuffle_pH2)$coef[2]     \n\n[1] -0.05110224\n\n\nThe slope on the randomly shuffled data is again not nearly as large as the slope of -0.15 that we saw in the actual data.\nThird Permutation\nWe’ll repeat the procedure one more time, randomly shuffling the pH’s of lakes.\nRecall these groups were randomly assigned, so the only differences in averages are due to random chance.\n\nLakes_shuffle_pH3 &lt;- FloridaLakes |&gt; mutate(ShuffledpH = shuffle(pH))  \nhead(Lakes_shuffle_pH3)                                      \n\n# A tibble: 6 × 8\n     ID Lake            pH Mercury Chlorophyll Location Depth   ShuffledpH\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;\n1     1 Alligator      6.1    1.23         0.7 S        Medium         7.5\n2     2 Annie          5.1    1.33         3.2 S        Deep           8.7\n3     3 Apopka         9.1    0.04       128.  N        Medium         8.4\n4     4 Blue Cypress   6.9    0.44         3.5 S        Shallow        6.8\n5     5 Brick          4.6    1.2          1.8 S        Medium         6.8\n6     6 Bryant         7.3    0.27        44.1 N        Shallow        7.1\n\n\n\nggplot(data=Lakes_shuffle_pH3, aes(x=ShuffledpH, y=Mercury)) + \n  geom_point() + stat_smooth(method=\"lm\", se=FALSE, color=\"red\") + \n  xlim(c(3, 10)) + ylim(c(0,1.5)) + \n  geom_abline(slope=-0.1523, intercept=1.5309, color=\"blue\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nSlope of regression line from permuted data:\n\nM_Lakes_shuffle_pH3 &lt;- lm(data=Lakes_shuffle_pH3, Mercury~ShuffledpH)\nsummary(M_Lakes_shuffle_pH3)$coef[2]     \n\n[1] -0.005704182\n\n\nNone of our three simulations resulted in a slope near as extreme as the -0.15 that we saw in the actual data. This seems to suggest that it is unlikely that we would have observed a slope as extreme as -0.15 if there is actually no relationship between mercury and pH among all lakes.\nTo be more sure, we should repeat the simulation many more times to see whether getting a slope as extreme as -0.15 is plausible in a situation where there is no relationship between mercury level and pH of a lake.\n\nLakes_merc_pH_perm_test &lt;- do(10000) * lm(Mercury ~ shuffle(pH), data = FloridaLakes)\n\n\nEstimate &lt;- -0.1523\nExtreme &lt;- abs(Lakes_merc_pH_perm_test$pH) &gt; 0.1523\n  \n  \nLakes_merc_pH_perm_test_plot &lt;- ggplot(data=Lakes_merc_pH_perm_test, aes(x=pH)) + \n  geom_histogram(aes(fill = Extreme), color=\"white\") + \n  scale_fill_manual(values = c(\"FALSE\" = \"blue\", \"TRUE\" = \"red\")) +\n  geom_vline(xintercept=c(Estimate, -1*Estimate), color=\"red\") + \n  xlab(\"Lakes pH: Simulated Value of b1\") + ylab(\"Frequency\") + \n  ggtitle(\"Distribution of b1 under assumption of no relationship\") +\n  theme(legend.position = \"none\") + theme_bw()\n\nLakes_merc_pH_perm_test_plot\n\n\n\n\n\n\n\n\np-value: Proportion of simulations resulting in value of \\(b_1\\) more extreme than -0.15\n\nprop(~ (abs(pH) &gt; 0.1523), data = Lakes_merc_pH_perm_test)\n\nprop_TRUE \n        0 \n\n\nThe p-value represents the probability of observing a slope as extreme or more extreme than -0.15 by chance when there is actually no relationship between mercury level and pH.\nIt is extremely unlikely that we would observe a value of \\(b_1\\) as extreme as -0.15 by chance, if there is really no relationship between mercury level and pH. In fact, this never happened in any of our 10,000 simulations!\nThere is very strong evidence of a relationship mercury level and pH.\nA low p-value tells us only that there is evidence of a relationship, not that it is practically meaningful. We have seen that for each one-unit increase in pH, mercury level is expected to decrease by 0.15 ppm on average, which seems like a pretty meaningful decrease, especially considering that mercury levels typically stay between 0 and 1.\nWe used the slope as our test statistic to measure the evidence of the relationship between the explanatory and response variables. In fact, we could have also used the correlation coefficient \\(r\\) as our test statistic, and we would have gotten the same p-value. Either slope or correlation may be used for a hypothesis test involving two quantitative variables, but we will use slope in this class.\n\n\n3.4.4 F-Test for Comparing Three or More Groups\nNext, we’ll investigate whether mercury levels tend to differ between lakes of different depths. The Depth variable classifies lakes as either shallow (less than 15 feet at deepest point), medium (15-30 feet deep), or deep (deeper than 30 feet).\nComparative boxplots and a table are shown below.\n\nggplot(data=FloridaLakes, aes(x=Depth, y=Mercury, fill=Depth)) + \n  geom_boxplot(outlier.shape = NA) + geom_jitter(alpha=0.5) +\n  stat_summary(fun.y=\"mean\", geom=\"point\", shape=20, color=\"red\", fill=\"red\")+ \n  coord_flip() + ggtitle(\"Mercury by Depth\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nLakes_merc_cond_Tab &lt;- FloridaLakes %&gt;% group_by(Depth) %&gt;% summarize(Mean_Hg = mean(Mercury) |&gt; round(3), \n                                                                SD_Hg= sd (Mercury) |&gt; round(3), \n                                                                N= n())\nkable(Lakes_merc_cond_Tab)\n\n\n\n\nDepth\nMean_Hg\nSD_Hg\nN\n\n\n\n\nDeep\n0.604\n0.307\n15\n\n\nMedium\n0.591\n0.429\n16\n\n\nShallow\n0.428\n0.276\n22\n\n\n\n\n\nWe see that mean mercury level is highest in the deepest lakes, and lowest in the shallowest lakes. We’ll again want to investigate whether the differences we see could be evidence of a relationship between depth and mercury level, or whether differences like these could have plausibly occurred even if there was no relationship.\nWe fit a model to the data. The deep category is used as a baseline, and the other two are included as terms in the model.\nThe model equation is:\n\\[\n\\widehat{\\text{Mercury}} = b_0 + b_1\\times\\text{DepthMedium} + b_2\\times\\text{DepthShallow}\n\\]\nThe coefficient estimates are:\n\nM_Lakes_merc_depth &lt;- lm(data=FloridaLakes, Mercury~Depth)\nM_Lakes_merc_depth\n\n\nCall:\nlm(formula = Mercury ~ Depth, data = FloridaLakes)\n\nCoefficients:\n (Intercept)   DepthMedium  DepthShallow  \n     0.60400      -0.01275      -0.17582  \n\n\nInterpretations\n\\(b_0\\) - the average mercury content in deep lakes in Florida is 0.604 ppm.\n\\(b_1\\) - the average mercury content in medium lakes in Florida is 0.013 ppm less than in deep lakes.\n\\(b_2\\) - the average mercury content in shallow lakes in Florida is 0.18 ppm less than in deep lakes.\nWe want to test whether there is a relationship between depth and mercury level. If there is, then we would expect the average mercury level to differ between at least some shallow, medium, and deep lakes.\nThe hypotheses are\nNull Hypothesis: There is no difference in average mercury levels between shallow, medium, and deep lakes.\nAlternative Hypothesis: There is no difference in average mercury levels between shallow, medium, and deep lakes.\nIf we were only comparing two groups, we could use the difference in means as a test statistic, like we did when testing for differences in location. But with three groups, we would have three different differences in means to test (shallow-medium, shallow-deep, medium-deep). We need a single statistic that can measure the size of differences between all three groups. An F-statistic can do this, so we’ll use the F-statistic as our test statistic here.\nThe F statistic for the actual data in our sample is\n\nM_Lakes_merc_depth_test &lt;-  do(1) * lm(data=FloridaLakes, Mercury ~ Depth)\nM_Lakes_merc_depth_test$F\n\n[1] 1.628254\n\n\nOur question of interest is “How likely is it to observe an F-statistic as extreme or more extreme than 1.63 if there is actually no difference in average mercury level between the three depths?”\nWe’ll use a permutation-based hypothesis test to investigate this question.\nProcedure:\n\nRandomly shuffle the depth categories of the lakes, so that any relationship between depth and mercury is due only to chance.\nUsing the shuffled data, calculate an F-statistic for a model predicting mercury, comparing a full model that uses depth as an explanatory variable, to a reduced model with no explanatory variables.\nRepeat steps 1 and 2 many (say 10,000) times, recording the F-statistic each time.\nAnalyze the distribution of F-statistics, simulated under the assumption that there is no relationship between depth and mercury level. Look whether the actual F-statistic we observed is consistent with the simulation results.\n\nWe’ll illustrate the first three permutations.\nFirst Permutation\nRecall these groups were randomly assigned, so the only differences in averages are due to random chance.\n\nLakes_shuffle_depth1 &lt;- FloridaLakes |&gt; mutate(ShuffledDepth = shuffle(Depth))  \nhead(Lakes_shuffle_depth1)                                 \n\n# A tibble: 6 × 8\n     ID Lake            pH Mercury Chlorophyll Location Depth   ShuffledDepth\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;        \n1     1 Alligator      6.1    1.23         0.7 S        Medium  Shallow      \n2     2 Annie          5.1    1.33         3.2 S        Deep    Deep         \n3     3 Apopka         9.1    0.04       128.  N        Medium  Shallow      \n4     4 Blue Cypress   6.9    0.44         3.5 S        Shallow Medium       \n5     5 Brick          4.6    1.2          1.8 S        Medium  Shallow      \n6     6 Bryant         7.3    0.27        44.1 N        Shallow Deep         \n\n\nNotice that the depths of the lakes have now been mixed up and assigned randomly. So, any relationship between depth and mercury level will have occurred merely by chance.\nWe create a boxplot and calculate the mean mercury levels for each depth class in the shuffled data.\n\nggplot(data=Lakes_shuffle_depth1, aes(x=ShuffledDepth, y=Mercury, fill=ShuffledDepth)) + \n  geom_boxplot(outlier.shape = NA) + geom_jitter(alpha=0.5) +\n  stat_summary(fun.y=\"mean\", geom=\"point\", shape=20, color=\"red\", fill=\"red\")+ \n  coord_flip() + ggtitle(\"Boxplot for Shuffled Depth Classes\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCond_Tab &lt;- Lakes_shuffle_depth1 %&gt;% group_by(ShuffledDepth) %&gt;% summarize(Mean_Hg = mean(Mercury) |&gt; round(3), \n                                                                SD_Hg= sd (Mercury) |&gt; round(3), \n                                                                N= n())\nkable(Cond_Tab)\n\n\n\n\nShuffledDepth\nMean_Hg\nSD_Hg\nN\n\n\n\n\nDeep\n0.553\n0.374\n15\n\n\nMedium\n0.517\n0.271\n16\n\n\nShallow\n0.517\n0.376\n22\n\n\n\n\n\nNotice that the differences between the groups appear as big or bigger than what we saw in the actual data, even though these groups were obtained by randomly assigning lakes to depth classes, meaning there is actually no relationship between depth and mercury level. The differences we see occur just by random assignment.\nWe fit a model to the shuffled data and record the F-statistic.\n\nLakes_merc_depth_perm1 &lt;- do(1) * lm(Mercury ~ ShuffledDepth , data = Lakes_shuffle_depth1)\nLakes_merc_depth_perm1$F\n\n[1] 0.05933947\n\n\nThe F-statistic observed in the shuffled data is Lakes_merc_depth_perm1$F.\nSecond Permutation\nWe’ll repeat the procedure, again randomly shuffling the depths of lakes.\nRecall these groups were randomly assigned, so the only differences in averages are due to random chance.\n\nLakes_shuffle_depth2 &lt;- FloridaLakes |&gt; mutate(ShuffledDepth = shuffle(Depth))  \nhead(Lakes_shuffle_depth2)                                 \n\n# A tibble: 6 × 8\n     ID Lake            pH Mercury Chlorophyll Location Depth   ShuffledDepth\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;        \n1     1 Alligator      6.1    1.23         0.7 S        Medium  Medium       \n2     2 Annie          5.1    1.33         3.2 S        Deep    Medium       \n3     3 Apopka         9.1    0.04       128.  N        Medium  Medium       \n4     4 Blue Cypress   6.9    0.44         3.5 S        Shallow Shallow      \n5     5 Brick          4.6    1.2          1.8 S        Medium  Shallow      \n6     6 Bryant         7.3    0.27        44.1 N        Shallow Shallow      \n\n\n\nggplot(data=Lakes_shuffle_depth2, aes(x=ShuffledDepth, y=Mercury, fill=ShuffledDepth)) + \n  geom_boxplot(outlier.shape = NA) + geom_jitter(alpha=0.5) +\n  stat_summary(fun.y=\"mean\", geom=\"point\", shape=20, color=\"red\", fill=\"red\")+ \n  coord_flip() + ggtitle(\"Boxplot for Shuffled Depth Classes\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCond_Tab &lt;- Lakes_shuffle_depth2 %&gt;% group_by(ShuffledDepth) %&gt;% summarize(Mean_Hg = mean(Mercury) |&gt; round(3), \n                                                                SD_Hg= sd (Mercury) |&gt; round(3), \n                                                                N= n())\nkable(Cond_Tab)\n\n\n\n\nShuffledDepth\nMean_Hg\nSD_Hg\nN\n\n\n\n\nDeep\n0.460\n0.260\n15\n\n\nMedium\n0.668\n0.414\n16\n\n\nShallow\n0.470\n0.315\n22\n\n\n\n\n\n\nLakes_merc_depth_perm2 &lt;- do(1) * lm(Mercury ~ ShuffledDepth , data = Lakes_shuffle_depth2)\nLakes_merc_depth_perm2$F\n\n[1] 2.040298\n\n\nThe F-statistic observed in the shuffled data is 2.0402981.\nThird Permutation\nWe’ll repeat the procedure, one more time, again randomly shuffling the depths of lakes.\n\nLakes_shuffle_depth3 &lt;- FloridaLakes |&gt; mutate(ShuffledDepth = shuffle(Depth))  \nhead(Lakes_shuffle_depth3)                                 \n\n# A tibble: 6 × 8\n     ID Lake            pH Mercury Chlorophyll Location Depth   ShuffledDepth\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;        \n1     1 Alligator      6.1    1.23         0.7 S        Medium  Medium       \n2     2 Annie          5.1    1.33         3.2 S        Deep    Medium       \n3     3 Apopka         9.1    0.04       128.  N        Medium  Medium       \n4     4 Blue Cypress   6.9    0.44         3.5 S        Shallow Shallow      \n5     5 Brick          4.6    1.2          1.8 S        Medium  Deep         \n6     6 Bryant         7.3    0.27        44.1 N        Shallow Deep         \n\n\n\nggplot(data=Lakes_shuffle_depth3, aes(x=ShuffledDepth, y=Mercury, fill=ShuffledDepth)) + \n  geom_boxplot(outlier.shape = NA) + geom_jitter(alpha=0.5) +\n  stat_summary(fun.y=\"mean\", geom=\"point\", shape=20, color=\"red\", fill=\"red\")+ \n  coord_flip() + ggtitle(\"Boxplot for Shuffled Depth Classes\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCond_Tab &lt;- Lakes_shuffle_depth3 %&gt;% group_by(ShuffledDepth) %&gt;% summarize(Mean_Hg = mean(Mercury) |&gt; round(3), \n                                                                SD_Hg= sd (Mercury) |&gt; round(3), \n                                                                N= n())\nkable(Cond_Tab)\n\n\n\n\nShuffledDepth\nMean_Hg\nSD_Hg\nN\n\n\n\n\nDeep\n0.555\n0.336\n15\n\n\nMedium\n0.629\n0.400\n16\n\n\nShallow\n0.434\n0.285\n22\n\n\n\n\n\n\nLakes_merc_depth_perm3 &lt;- do(1) * lm(Mercury ~ ShuffledDepth , data = Lakes_shuffle_depth3)\nLakes_merc_depth_perm3$F\n\n[1] 1.617857\n\n\nThe F-statistic observed in the shuffled data is 1.6178574.\nWe’ll repeat the simulation many more times to see whether getting an F-statistic as extreme as 1.62 is plausible in a situation where there is no relationship between depth and average mercury level. The first 6 simulated F-statistics are shown below. Recall that these are simulated under the assumption that there is no difference in average mercury between shallow, medium, and deep lakes, i.e. no relationship between mercury level and depth.\n\nLakes_merc_depth_perm_test &lt;- do(10000) * lm(Mercury ~ shuffle(Depth) , data = FloridaLakes)\nhead(Lakes_merc_depth_perm_test$F)\n\n[1] 0.265481460 0.242068358 0.007345152 1.450498889 1.238486261 0.093325750\n\n\nThe distribution of the F-statistics is shown below. As usual, the red line represents the F-statistic observed in our actual data. Since F-statistics cannot be negative, we don’t need to worry about finding an F-statistic as extreme in the opposite direction.\n\nFstat &lt;- 1.628\nExtreme &lt;- Lakes_merc_depth_perm_test$F &gt; 1.628\n  \n  \nLakes_merc_depth_perm_test_plot &lt;- ggplot(data=Lakes_merc_depth_perm_test, aes(x=F)) + \n  geom_histogram(aes(fill = Extreme), color=\"white\") + \n  scale_fill_manual(values = c(\"FALSE\" = \"blue\", \"TRUE\" = \"red\")) +\n  geom_vline(xintercept=c(Fstat), color=\"red\") + \n  xlab(\"Lakes Depth: Simulated F-statistic\") + ylab(\"Frequency\") + \n  ggtitle(\"Distribution of F-stat under assumption of no relationship\") +\n  theme(legend.position = \"none\") + \n  theme_bw()\n\nLakes_merc_depth_perm_test_plot\n\n\n\n\n\n\n\n\nNotice that unlike the permutation distributions we’ve seen before, the distribution of the F-statistic is not symmetric. Recall that F-statistics can only take on positive values, which results in this right-skewed shape. Nevertheless, we can still look at where the F-statistic we observed in our data lies relative to those obtained in a simulation that assumed the null hypothesis of no relationship was true.\nThe result we observed seems pretty consistent with the simulated values in the histogram. The red line is not very extreme.\np-value: Proportion of simulations resulting in F-statistic more extreme than 1.628:\n\nprop(~ (F &gt; 1.628), data = Lakes_merc_depth_perm_test)\n\nprop_TRUE \n   0.2089 \n\n\nThis p-value represents the probabilty of observing an F-statistic as extreme or more extreme than we 1.628 if there is really no relationship between a lake’s depth and mercury level.\nSince the p-value is large, it is plausible that we would see differences like we did in the actual data in a situation where the null hypothesis is true.\nWe cannot reject the null hypothesis. We do not have enough evidence to say that average mercury levels differ between shallow, medium, and deep lakes.\nWe are not saying that we think mercury levels are the same between the three different depths (i.e. we’re not accepting the null hypothesis), just that we don’t have enough evidence to say that they differ.\n\n\n\n3.4.5 Responsible Hypothesis Testing\nWhile hypothesis tests are a powerful tool in statistics, they are also one that has been widely misused, to the detriment of scientific research. The hard caused by these misuses caused the American Statistical Association to release a 2016 statement, intended to provide guidance and clarification to scientists who use hypothesis testing and p-values in their research.\nThe statement provides the following six principles for responsible use of hypothesis tests and p-values.\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\nThe statement provides important guidance for us to consider as we work with hypothesis testing in this class, as well as in future classes and potentially in our own research.\n\nA hypothesis test can only tell us the strength of evidence against the null hypothesis. The absence of evidence against the null hypothesis should not be interpreted as evidence for the null hypothesis.\nWe should never say that the data support/prove/confirm the null hypothesis.\nWe can only say that the data do not provide evidence against the null hypothesis.\n\nWhat to conclude from p-values and what not to:\n\nA low p-value provides evidence against the null hypothesis. It suggests the test statistic we observed is inconsistent with the null hypothesis.\nA low p-value does not tell us that the difference or relationship we observed is meaningful in a practical sense. Researchers should look at the size of the difference or strength of the relationship in the sample before deciding whether it merits being acted upon.\nA high p-value means that the data could have plausibly been obtained when the null hypothesis is true. The test statistic we observed is consistent with what we would have expected to see when the null hypothesis is true, and thus we cannot rule out the null hypothesis.\nA high p-value does not mean that the null hypothesis is true or probably true. A p-value can only tell us the strength of evidence against the null hypothesis, and should never be interpreted as support for the null hypothesis.\n\nJust because our result is consistent with the null hypothesis does not mean that we should believe that null hypothesis is true. Lack of evidence against a claim does not necessarily mean that the claim is true.\nIn this scenario, we got a small p-value, but we should also be aware of what we should conclude if the p-value is large. Remember that the p-value only measures the strength of evidence against the null hypothesis. A large p-value means we lack evidence against the null hypothesis. This does not mean, however, that we have evidence supporting null hypothesis.\nA hypothesis test can be thought of as being analogous to a courtroom trial, where the null hypothesis is that the defendant did not commit the crime. Suppose that after each side presents evidence, the jury remains unsure whether the defendant committed the crime. Since the jury does not have enough evidence to be sure, they must, under the law of the United States find the defendant “not guilty.” This does not mean that the jury thinks the defendant is innocent, only that they do not have enough evidence to be sure they are guilty. Similarly in a hypothesis test, a large p-value indicates a lack of evidence against the null hypothesis, rather than evidence supporting it. As such, we should avoid statements suggesting we “support”, “accept”, or “believe” the null hypothesis, and simply state that we lack evidence against it.\nThings to say when the p-value is small:\n\nThe data are not consistent with the null hypothesis.\n\nWe have evidence against the null hypothesis.\n\nWe detect a difference or effect.\n\nThe difference or effect is statistically discernible.\n\nThings NOT to say based on a small p-value alone:\n\nThe difference is practically meaningful or significant.\n\nWe should take action based on the result.\n\nIn some cases, a small p-value might merit taking action, but it is important to also consider the size of the difference, the method of in which the data were collected, and how representative it is, as well as the costs and benefits of a potential decision, rather than acting on a p-value alone.\nThings to say when the p-value is large:\n\nThe data are consistent with the null hypothesis.\n\nWe do not have enough evidence against the null hypothesis.\n\nWe cannot reject the null hypothesis.\n\nThe null hypothesis is plausible.\n\nThings NOT to say when the p-value is large:\n\nThe data support the null hypothesis.\n\nThe data provide evidence for the null hypothesis.\n\nWe accept the null hypothesis.\n\nWe conclude that the null hypothesis is true.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch3.html#practice-questions",
    "href": "Ch3.html#practice-questions",
    "title": "3  Simulation-Based Inference",
    "section": "3.5 Practice Questions",
    "text": "3.5 Practice Questions\n\n1)\nFor each situation identify:\n\na relevant population of interest\n\nthe sample\n\nthe population parameter\n\nthe sample statistic\n\nIf the size of the sample or population, or the numeric value of the sample/parameter are known, say what they are.\n\na)\nData were collected from a cross-sectional study to investigate the relationship between personal characteristics and dietary factors, and plasma concentrations of retinol, beta-carotene and other carotenoids. 315 study subjects were patients who had an elective surgical procedure during a three-year period to biopsy or remove a lesion of the lung, colon, breast, skin, ovary or uterus that was found to be non-cancerous. Each person’s daily cholesterol consumption (in mg/day) was recorded. Researchers are interested in estimating the average daily cholesterol consumption among all hospital patients with similar conditions.\n\n\nb)\nData are from samples of ten courses in each of four disciplines at a liberal arts college. The classes were classified as either Arts, Humanities, Natural Sciences, or Sciences. For each course the bookstore’s website lists the required texts(s) and costs. We are interested in comparing average material costs between the four different areas of classes. For this question, assume we’re comparing humanities and natural sciences. It is found that, among the books in the sample, books for natural science courses cost $50.50 more than for books for humanities courses, on average.\n\n\nc)\nA 2012 study in Great Britain examines the relationship between the number of friends an individual has on Facebook and grey matter density in the areas of the brain associated with social perception and associative memory. The study included 40 students at City University London. Researchers then used a linear regression model to study the relationship between number of Facebook friends and grey matter density in the brain. Using data from the 40 students in the sample, they calculated the slope of the regression line to be 82.45.\n\n\n\n2)\nData were collected from a cross-sectional study to investigate the relationship between personal characteristics and dietary factors, and plasma concentrations of retinol, beta-carotene and other carotenoids. 315 study subjects were patients who had an elective surgical procedure during a three-year period to biopsy or remove a lesion of the lung, colon, breast, skin, ovary or uterus that was found to be non-cancerous. Each person’s daily cholesterol consumption (in mg/day) was recorded. Researchers are interested in estimating the average daily cholesterol consumption among all hospital patients with similar conditions.\nThe distribution of amount of cholesterol consumed in a day for the 315 people in the nutrition study from 1(a) is shown below along with the sample mean and standard deviation.\n\nggplot(data=NutritionStudy, aes(x=Cholesterol)) + \n  geom_histogram(color=\"white\", fill=\"lightblue\")   + theme_bw()\n\n\n\n\n\n\n\n\n\nNutritionStudy |&gt; summarize(Mean_Chol = mean(Cholesterol), \n                            SD_Chol=sd(Cholesterol))\n\n  Mean_Chol  SD_Chol\n1  242.4606 131.9916\n\n\nWe use a model with only an intercept term to estimate the mean cholesterol among all hospital patients with similar conditions. The model is\n\\[\n\\begin{aligned}\n\\widehat{\\text{Cholesterol}} & = b_0\n\\end{aligned}\n\\] R output for the model is shown below.\n\nM_Nut_chol_0 &lt;- lm(data=NutritionStudy, Cholesterol~1)\nM_Nut_chol_0\n\n\nCall:\nlm(formula = Cholesterol ~ 1, data = NutritionStudy)\n\nCoefficients:\n(Intercept)  \n      242.5  \n\n\nThe bootstrap distribution for \\(b_0\\) is shown below along with its mean and standard deviation.\n\nNutrition_Bootstrap_Mean &lt;- do(10000) * lm( Cholesterol ~ 1, data = resample(NutritionStudy, replace=TRUE))\n\n\n\n\n\n\n\n\n\n\n\nNutrition_Bootstrap_Mean |&gt; summarize(Mean_Chol = mean(Intercept) |&gt; round(1), \n                                      SE_Mean=sd(Intercept) |&gt; round(1))\n\n  Mean_Chol SE_Mean\n1     242.6     7.4\n\n\n\na)\nExplain the difference between the standard deviation of 132 and the standard error of the mean of 7.4. State clearly what each number is measuring the variability between.\n\n\nb)\nSuppose the sample had included 500 patients instead of 315. Should we expect the standard deviation to be more, less, or about the same as 132? Should we expect the standard error of the mean to be more, less, or about the same as 7.4?\n\n\n\n3)\nFor each of the scenarios (a-c) in Question 1, list the steps of the Bootstrap algorithm in Section 3.2.2 that we would need to perform to obtain approximate 95% bootstrap confidence intervals for the parameter of interest. In your steps, be sure to explicitly identify the size of the bootstrap samples and the statistic you would calculate.\n\n\n4)\nRefer to the information in Question 2. Give an approximate 95% standard error confidence interval for the relevant population parameter. Write a sentence interpreting your confidence interval in context.\n\n\n5)\nData are from samples of ten courses in each of four disciplines at a liberal arts college. The classes were classified as either Arts, Humanities, Natural Sciences, or Sciences. For each course the bookstore’s website lists the required texts(s) and costs. We are interested in comparing average material costs between the four different areas of classes. For this question, assume we’re comparing humanities and natural sciences.\nWe fit a model of the form\n\\[\n\\begin{aligned}\n\\widehat{\\text{Cost}} & = b_0 + b_1 \\text{Humanities} + b_2 \\text{NaturalScience} + b_3 \\text{SocialScience},\n\\end{aligned}\n\\] where the Humanities, NaturalScience, and SocialScience variables all have value 1 if the course is in that field and 0 otherwise.\nModel estimates are shown below.\n\nM_Book_cost_Field &lt;- lm(data=TextbookCosts, Cost~Field) \nM_Book_cost_Field\n\n\nCall:\nlm(formula = Cost ~ Field, data = TextbookCosts)\n\nCoefficients:\n        (Intercept)      FieldHumanities  FieldNaturalScience  \n               94.6                 25.7                 76.2  \n FieldSocialScience  \n               23.7  \n\n\nWe use bootstrapping to produce confidence intervals for parameters associated with the model.\n\nBooks_Bootstrap &lt;- do(10000) * lm( Cost ~ Field, \n                                   data = resample(TextbookCosts, replace=TRUE))\n\n\n\na)\nThe bootstrap distribution for \\(b_0\\) is shown below, along with its mean and standard error.\n\n\n\n\n\n\n\n\n\n\nBooks_Bootstrap |&gt; summarize(Mean_b0 = mean(Intercept) |&gt; round(1), \n                                      SE_b0=sd(Intercept) |&gt; round(1))\n\n  Mean_b0 SE_b0\n1    94.6    14\n\n\nGive an approximate 95% standard error confidence interval for the relevant population parameter. Write a sentence interpreting your confidence interval in context.\n\n\nb)\nThe bootstrap distribution for \\(b_3\\) is shown below, along with its mean and standard error.\n\n\n\n\n\n\n\n\n\n\nBooks_Bootstrap |&gt; summarize(Mean_b3 = mean(FieldSocialScience) |&gt; round(1), \n                                      SE_b3=sd(FieldSocialScience) |&gt; round(1))\n\n  Mean_b3 SE_b3\n1    23.7  20.9\n\n\nGive an approximate 95% standard error confidence interval for the relevant population parameter. Write a sentence interpreting your confidence interval in context.\n\n\nc)\nThe bootstrap distribution for \\(b_0 + b_1\\) is shown below, along with its mean and standard error.\n\n\n\n\n\n\n\n\n\n\nBooks_b0_b1_Bootstrap |&gt; summarize(Mean = mean(X1) |&gt; round(1), \n                                      SE=sd(X1) |&gt; round(1))\n\n   Mean   SE\n1 120.3 18.1\n\n\nGive an approximate 95% standard error confidence interval for the relevant population parameter. Write a sentence interpreting your confidence interval in context.\n\n\n6)\nA 2012 study in Great Britain examines the relationship between the number of friends an individual has on Facebook and grey matter density in the areas of the brain associated with social perception and associative memory. The study included 40 students at City University London. Researchers then used a linear regression model to study the relationship between number of Facebook friends and grey matter density in the brain.\nWe fit a model of the form\n\\[\n\\begin{aligned}\n\\widehat{\\text{Cost}} & = b_0 + b_1 \\text{GMdensity}\n\\end{aligned}\n\\] where the GMdensity is a measure of grey matter density in the brain. The GM density variable was standardized, so a person who has average grey matter density will have GMdensity =0, and the GMdensity variable can be interpreted as the number of standard deviations a person is above or below the average amount of grey matter density.\nModel estimates are shown below.\n\nM_FB_friends_gm &lt;- lm(data=FacebookFriends, FBfriends ~ GMdensity) \nM_FB_friends_gm\n\n\nCall:\nlm(formula = FBfriends ~ GMdensity, data = FacebookFriends)\n\nCoefficients:\n(Intercept)    GMdensity  \n     366.64        82.45  \n\n\nWe use bootstrapping to produce confidence intervals for parameters associated with the model.\n\nFB_Bootstrap &lt;- do(10000) * lm( FBfriends ~ GMdensity, \n                                data = resample(FacebookFriends, replace=TRUE))\n\n\n\na)\nThe bootstrap distribution for \\(b_0\\) is shown below, along with its mean and standard error.\n\n\n\n\n\n\n\n\n\n\nFB_Bootstrap |&gt; summarize(Mean_b0 = mean(Intercept) |&gt; round(1), \n                                      SE_b0=sd(Intercept) |&gt; round(1))\n\n  Mean_b0 SE_b0\n1   367.5  26.4\n\n\nGive an approximate 95% standard error confidence interval for the relevant population parameter. Write a sentence interpreting your confidence interval in context.\n\n\nb)\nThe bootstrap distribution for \\(b_1\\) is shown below, along with its mean and standard error.\n\n\n\n\n\n\n\n\n\n\nFB_Bootstrap |&gt; summarize(Mean_b1 = mean(GMdensity) |&gt; round(1), \n                                      SE_b1=sd(GMdensity) |&gt; round(1))\n\n  Mean_b1 SE_b1\n1    81.6  22.8\n\n\nGive an approximate 95% standard error confidence interval for the relevant population parameter. Write a sentence interpreting your confidence interval in context.\n\n\nc)\nThe bootstrap distribution for \\(b_0 + 0.5b_1\\) is shown below, along with its mean and standard error.\n\n\n\n\n\n\n\n\n\n\nBooks_b0_b1_Bootstrap |&gt; summarize(Mean = mean(X1) |&gt; round(1), \n                                      SE=sd(X1) |&gt; round(1))\n\n   Mean   SE\n1 120.3 18.1\n\n\nGive an approximate 95% standard error confidence interval for the relevant population parameter. Write a sentence interpreting your confidence interval in context.\n\n\n7)\nState whether each of the following statements about bootstrapping are true or false. If the statement is false, explain why.\n\na)\nBootstrapping helps us measure the amount of variability we would see in a statistic between different samples taken from a population.\n\n\nb)\nBootstrapping increases the size of the sample, giving us more accurate estimates of the mean and other regression coefficients.\n\n\nc)\nBootstrap samples must be the same size as the original sample.\n\n\nd)\nIf we took bootstrap samples without using replacement, then every sample would be the same, and we would not be able to measure variability in our statistic.\n\n\ne)\nWhen the bootstrap distribution is not symmetric and bell-shaped, it is better to use a standard error bootstrap confidence interval than a percentile bootstrap confidence interval.\n\n\nf)\nThe mean and standard deviation of the bootstrap distribution for the mean should both be the same as the mean and standard deviation of the sampling distribution for the mean.\n\n\n\n8)\nFor the cholesterol study in Questions 2 and 4, a person suggests that we could reduce the standard error, and thus obtain a more precise confidence interval, by taking bootstrap samples of 500 (instead of the sample size of 315 seen in the actual study). Is this a good idea? Why or why not?\n\n\n9)\nEach day, United Airlines has a flight from Boston to San Francisco, departing at 6 am (Eastern time). The flight is scheduled to arrive in San Francisco at 9:30 am (Pacific time). This results in an expected total flight duration of 6.5 hours (390 minutes). Due to weather conditions, however, the exact duration of the flight varies from day to day. A random sample of flights from 31 different days was selected. The times of the 28 flights are shown below, listed from smallest to largest.\n\n\n [1] 334 340 344 351 352 354 362 366 367 368 368 368 368 369 371 373 377 377 381\n[20] 382 385 386 388 388 388 391 395 398\n\n\nThe mean, standard deviation, and sample size in flight times are shown below.\n\n\n\n\n\nMean\nStDev\nn\n\n\n\n\n371.1071\n16.73395\n28\n\n\n\n\n\nA 95% bootstrap standard error confidence interval for the mean flight time for a model including only an intercept \\(b_0\\) is shown below.\n\nFlights_Bootstrap &lt;- do(10000) * lm( AirTime ~ 1, \n                                data = resample(Flight433, replace=TRUE))\nconfint(Flights_Bootstrap, parm=\"Intercept\", level=0.95, method=\"se\")\n\n       name    lower    upper level method estimate margin.of.error\n1 Intercept 365.0501 377.1973  0.95 stderr 371.1071        6.073613\n\n\nA manager sees the data and confidence interval and says:\n“This is supposed to be a 95% confidence interval, but only 11 of the 28 flight times were actually in this range (between 365 and 377). A 95% confidence interval should contain about 95% of the flight times, and 11/28 is much less than 95%. Someone must have made a mistake calculating this interval.”\nIs the manager’s reasoning correct? If so, identify the mistake in the method used to obtain the confidence interval. If not, identify the mistake in the manager’s reasoning.\n\n\nInfo for Questions (10-12)\nFor Questions 10-12, refer to the following data and questions.\nWe’ll work with a dataset pertaining to a sample of 200 high school seniors. Data were collected from student responses to the High School and Beyond Survey. The data are available in the openintro R package.\nVariables of interest include:\nschtyp - school type (public or private)\nprog - type of program (general, academic, or vocational)\nwrite - writing score on a standardized test\nscience - science score on a standardized test\nThe first 6 rows of the data are shown below.\n\nlibrary(openintro)\ndata(hsb2)\nHSSeniors &lt;- hsb2\n\n\nhead(hsb2) |&gt; select(schtyp, prog, write, science)\n\n# A tibble: 6 × 4\n  schtyp prog       write science\n  &lt;fct&gt;  &lt;fct&gt;      &lt;int&gt;   &lt;int&gt;\n1 public general       52      47\n2 public vocational    59      63\n3 public general       33      58\n4 public vocational    44      53\n5 public academic      52      53\n6 public academic      52      63\n\n\nSome graphics summarizing the data are shown below.\n\nggplot(data=HSSeniors, aes(y=science, x=schtyp)) + geom_boxplot() +\n  ggtitle(\"Science Scores for Public and Private Schools\") + \n  xlab(\"Type of School\") + ylab(\"Average Score\")  + theme_bw()\n\n\n\n\nScience test scores in public and private schools\n\n\n\n\n\nggplot(data=HSSeniors, aes(y=science, x=prog)) + \n  geom_boxplot() + ggtitle(\"Science Scores for Program Types\") + \n  xlab(\"Type of Program\") + ylab(\"Average Score\")  + theme_bw()\n\n\n\n\nScience test scores for each academic program\n\n\n\n\n\nggplot(data=HSSeniors, aes(y=science, x=write)) + geom_point() +\n  ggtitle(\"Science and Writing Scores\") + \n  xlab(\"Writing Score\") + \n  ylab(\"Science Score\")  + theme_bw()\n\n\n\n\nRelationship between science and writing scores\n\n\n\n\n\nT1 &lt;- HSSeniors %&gt;% group_by(schtyp) %&gt;% summarize(Mean_Science=mean(science),\n                                                  Med_Science=median(science),\n                                                  StDev_science=sd(science),\n                                                  n=n()) \nkable(T1, caption=\"Summary of science test scores by school type\")\n\n\nSummary of science test scores by school type\n\n\nschtyp\nMean_Science\nMed_Science\nStDev_science\nn\n\n\n\n\npublic\n51.57143\n53\n10.192501\n168\n\n\nprivate\n53.31250\n55\n8.185106\n32\n\n\n\n\n\n\nT2 &lt;- HSSeniors %&gt;% group_by(prog) %&gt;% summarize(Mean_Science=mean(science),\n                                                Med_Science=median(science),\n                                                StDev_science=sd(science),\n                                                n=n())\nkable(T2, caption=\"Summary of science test scores by program type\")\n\n\nSummary of science test scores by program type\n\n\nprog\nMean_Science\nMed_Science\nStDev_science\nn\n\n\n\n\ngeneral\n52.44444\n53\n9.680241\n45\n\n\nacademic\n53.80000\n55\n9.127726\n105\n\n\nvocational\n47.22000\n47\n10.333796\n50\n\n\n\n\n\n\n\n10)\nWe want to know whether there is evidence of a difference in average science test scores between public and private schools.\nWe fit a model of the form\n\\[\n\\begin{aligned}\n\\widehat{\\text{Score}} & = b_0 + b_1 \\text{Public}\n\\end{aligned}\n\\] Model estimates are shown below.\n\nM_Seniors_science_schtyp &lt;- lm(data=HSSeniors, science ~ schtyp)\nM_Seniors_science_schtyp\n\n\nCall:\nlm(formula = science ~ schtyp, data = HSSeniors)\n\nCoefficients:\n  (Intercept)  schtypprivate  \n       51.571          1.741  \n\n\nThe following code performs a permutation-based hypothesis test.\n\nscience_schtyp_perm_test &lt;- do(10000) * lm(science ~ shuffle(schtyp), data = HSSeniors)\n\nThe histogram shows the distribution of \\(b_1\\) in our simulation.\n\nEstimate &lt;- M_Seniors_science_schtyp$coefficients[2]\nExtreme &lt;- abs(science_schtyp_perm_test$schtyp) &gt; abs(Estimate)\n  \n  \nscience_schtyp_perm_test_plot &lt;- ggplot(data=science_schtyp_perm_test, aes(x=schtypprivate)) + \n  geom_histogram(aes(fill = Extreme), color=\"white\") + \n  scale_fill_manual(values = c(\"FALSE\" = \"blue\", \"TRUE\" = \"red\")) +\n  geom_vline(xintercept=c(Estimate, -1*Estimate), color=\"red\") + \n  xlab(\"Lakes: Simulated Value of b1\") + ylab(\"Frequency\") + \n  ggtitle(\"Distribution of b1 under assumption of no relationship\") +\n  theme(legend.position = \"none\") + theme_bw()\n\nscience_schtyp_perm_test_plot\n\n\n\n\n\n\n\n\np-value:\n\nprop(~ (abs(schtypprivate) &gt; 1.741), data = science_schtyp_perm_test)\n\nprop_TRUE \n   0.3638 \n\n\n\na)\nWrite null and alternative hypotheses.\n\n\nb)\nState the appropriate test statistic words and symbols. Give the numerical value of the test statistic.\n\n\nc)\nWrite out in words the steps in the procedure to perform a permutation-based hypothesis test. (See Section 3.4.1 for an example of how to write the steps.)\n\n\nd)\nInterpret the p-value in context by completing the sentence “The p-value represents the probability of _____, assuming _____.”\n\n\ne)\nExplain, in context, what we should conclude from the p-value.\n\n\nf)\nState whether the difference or effect is statistically discernible. Also state whether you think it is practically meaningful and comment on how the result of the test is or isn’t consistent with the information in the graphs and tables.\n\n\ng)\nSay whether zero would be contained in a 95% confidence interval for the parameter of interest. Explain your answer.\n\n\n\n11)\nWe want to know whether there is evidence of differences in average science test scores between two or more of the three programs (general, academic, vocational).\nWe fit a model of the form\n\\[\n\\begin{aligned}\n\\widehat{\\text{Score}} & = b_0 + b_1 \\text{general} + b_2 \\text{vocational}\n\\end{aligned}\n\\] Model estimates are shown below.\n\nM_Seniors_science_prog &lt;- lm(data=HSSeniors, science ~ prog)\nM_Seniors_science_prog\n\n\nCall:\nlm(formula = science ~ prog, data = HSSeniors)\n\nCoefficients:\n   (Intercept)    progacademic  progvocational  \n        52.444           1.356          -5.224  \n\n\nAn F-statistic comparing this model to a model with only an intercept is shown below.\n\nsummary(M_Seniors_science_prog)$fstat[1]\n\n  value \n8.12799 \n\n\nThe following code performs a permutation-based hypothesis test.\n\nscience_prog_perm_test &lt;-  do(10000) * lm(data=HSSeniors, science ~ shuffle(prog))\n\nThe histogram shows the distribution of \\(b_1\\) in our simulation.\n\nEstimate &lt;- science_prog_perm_test$F\nExtreme &lt;- abs(science_prog_perm_test$F) &gt; Estimate\n  \n  \nscience_prog_perm_test_plot &lt;- ggplot(data=science_prog_perm_test, aes(x=F)) + \n  geom_histogram(aes(fill = Extreme), color=\"white\") + \n  scale_fill_manual(values = c(\"FALSE\" = \"blue\", \"TRUE\" = \"red\")) +\n  geom_vline(xintercept=8.128, color=\"red\") + \n  xlab(\"Simulated Value of F\") + ylab(\"Frequency\") + \n  ggtitle(\"Distribution of F under assumption of no relationship\") +\n  theme(legend.position = \"none\") + theme_bw()\n\nscience_prog_perm_test_plot\n\n\n\n\n\n\n\n\np-value:\n\nprop(~ (schtypprivate &gt; 8.128), data = science_schtyp_perm_test)\n\nprop_TRUE \n        0 \n\n\n\na)\nWrite null and alternative hypotheses. Be sure to clearly identify the parameter of interest in your hypotheses.\n\n\nb)\nState the appropriate test statistic words and symbols. Give the numerical value of the test statistic.\n\n\nc)\nWrite out in words the steps to perform a permutation-based hypothesis test.\n\n\nd)\nInterpret p-value in context by completing the sentence “The p-value represents the probability of _____, assuming _____.”\n\n\ne)\nExplain, in context, what we should conclude from the p-value.\n\n\nf)\nState whether the difference or effect is statistically discernible. Also state whether you think it is practically meaningful and comment on how the result of the test is or isn’t consistent with the information in the graphs and tables.\n\n\n\n12)\nWe want to know whether there is evidence of a relationship between students’ science scores and writing scores on standardized tests.\nWe fit a model of the form\n\\[\n\\begin{aligned}\n\\widehat{\\text{Score}} & = b_0 + b_1 \\text{Writing}\n\\end{aligned}\n\\]\nModel estimates are shown below.\n\nM_Seniors_science_write &lt;- lm(data=HSSeniors, science ~ write)\nM_Seniors_science_write\n\n\nCall:\nlm(formula = science ~ write, data = HSSeniors)\n\nCoefficients:\n(Intercept)        write  \n    20.4037       0.5959  \n\n\nThe following code performs a permutation-based hypothesis test.\n\nscience_write_perm_test &lt;- do(10000) * lm(science ~ shuffle(write), data = HSSeniors)\n\nThe histogram shows the distribution of \\(b_1\\) in our simulation.\n\nEstimate &lt;- M_Seniors_science_write$coefficients[2]\nExtreme &lt;- abs(science_write_perm_test$write) &gt; abs(Estimate)\n  \n  \nscience_write_perm_test_plot &lt;- ggplot(data=science_write_perm_test, aes(x=write)) + \n  geom_histogram(aes(fill = Extreme), color=\"white\") + \n  scale_fill_manual(values = c(\"FALSE\" = \"blue\", \"TRUE\" = \"red\")) +\n  geom_vline(xintercept=c(Estimate, -1*Estimate), color=\"red\") + \n  xlab(\"Lakes: Simulated Value of b1\") + ylab(\"Frequency\") + \n  ggtitle(\"Distribution of b1 under assumption of no relationship\") +\n  theme(legend.position = \"none\") + theme_bw()\n\nscience_write_perm_test_plot\n\n\n\n\n\n\n\n\np-value:\n\nprop(~ (abs(write) &gt; 0.5959), data = science_write_perm_test)\n\nprop_TRUE \n        0 \n\n\n\na)\nWrite null and alternative hypotheses. Be sure to clearly identify the parameter of interest in your hypotheses.\n\n\nb)\nState the appropriate test statistic words and symbols. Give the numerical value of the test statistic.\n\n\nc)\nWrite out in words the steps to perform a permutation-based hypothesis test.\n\n\nd)\nInterpret p-values in context by completing the sentence “The p-value represents the probability of _____, assuming _____.”\n\n\ne)\nExplain, in context, what we should conclude from the p-value.\n\n\nf)\nState whether the difference or effect is statistically discernible. Also state whether you think it is practically meaningful and comment on how the result of the test is or isn’t consistent with the information in the graphs and tables.\n\n\ng)\nSay whether zero would be contained in a 95% confidence interval for the parameter of interest.\n\n\n13)\nThe Bootstrap distribution for the difference in average cost of books in natural science classes, compared with humanities classes, was given in Question 6(c). If we perform a permutation-based hypothesis test for the null hypothesis that there is no difference in average books costs among natural science courses, compared to humanities courses, how would the distribution of mean differences compare to the bootstrap distribution from 6(c)? What would be different about it? Would the p-value associated with the hypothesis test be more or less than 0.05? Explain your answer.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation-Based Inference</span>"
    ]
  },
  {
    "objectID": "Ch4.html",
    "href": "Ch4.html",
    "title": "4  Inference from Models",
    "section": "",
    "text": "Learning Outcomes\nConceptual Learning Outcomes\n13. Use standard error formulas to calculate quantities in R model summary output (including estimates, standard errors, t-statistics, \\(R^2\\), and F-statistics), as well as output for associated tests and confidence intervals.\n14. State the hypotheses associated with each p-value in the lm summary table and explain what we should conclude from each test. \n15. Interpret quantities in the R lm summary output including standard errors, residual standard error, \\(R^2\\), and the F-statistic.\n16. Interpret confidence intervals for expected response and prediction intervals and explain the sources of variability that contribute to each.\n17. Explain the regression effect in context.  \n18. Think beyond the p-value and draw appropriate conclusions (i) in situations involving multiple testing; (ii) when statistically discernible results might not be practically important, or (iii) when a large p-value might still be meaningful.\nComputational Learning Outcomes\nH. Perform hypothesis tests and calculate confidence and prediction intervals based on models using R.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#ordinary-linear-regression-model",
    "href": "Ch4.html#ordinary-linear-regression-model",
    "title": "4  Inference from Models",
    "section": "4.1 Ordinary Linear Regression Model",
    "text": "4.1 Ordinary Linear Regression Model\nYou’ve probably noticed that most of the sampling distributions for statistics we’ve seen were symmetric and bell-shaped in nature. When working with statistics that have symmetric and bell-shaped distributions it is often possible to use well-known probability facts to obtain confidence intervals and perform hypothesis tests without actually performing simulation.\nIn this chapter, we’ll examine a set of assumptions that, if true, would ensure that statistics like means and differences in means, and regression coefficients follow symmetric and bell-shaped distributions. We’ll learn how to use facts from probability to calculate confidence intervals and p-values without actually performing simulation in these instances.\n\n4.1.1 Signal and Noise\n\n\n\n\n\n\n\n\n\nSuppose an ice cream machine is manufactured to dispense 2 oz. of ice cream per second, on average. If 15 people used the ice cream machine, holding the dispenser for different amounts of time, and each person got exactly 2 oz. per second, the relationship between time holding the dispenser and amount dispensed would look like this:\n\n\n\n\n\n\n\n\n\nIn reality, however, the actual amount dispensed each time it is used will vary due to unknown factors like:\n\nforce applied to dispenser\n\ntemperature\n\nbuild-up of ice cream\n\nother unknown factors\n\nThus, if 15 real people held the dispenser and recorded the amount of ice cream they got, the scatter plot we would see would look something like this:\n\n\n\n\n\n\n\n\n\nWe can think of the amount of ice cream a person receives as being a result of two separate components, often referred to as signal and noise. In the graphic the red line represents the signal, and the amount by which each point deviates from the line is the noise.\nSignal represents the average amount of ice cream a person is expected to receive based on the amount of time holding the dispenser. In this case, signal is given by the function \\(\\text{Expected Amount} = 2\\times\\text{Time}\\). Everyone who holds the dispenser for \\(t\\) seconds is expected to receive \\(2t\\) ounces of ice cream.\nNoise represents how much each person’s actual amount of ice cream deviates from their expected amount. For example, a person who holds the dispenser for 1.5 seconds and receives 3.58 oz. of ice cream will have received 0.58 ounces more than expected due to noise (i.e. factors other than the amount of time holding the dispenser).\nIn a statistical model, we assume that in the data we observe, the value of the response variable is the sum of the signal and noise. The signal is a function of the explanatory variables in the model and noise is a deviation from the signal due to factors beyond those accounted for in the model.\n\n\n4.1.2 Normal Distribution\nIn the ice cream example it might be reasonable to expect that most people will get close to the amount of ice cream they should expect to get, based on the amount of time they hold down the dispenser, while a few people might get a lot more or less. When modeling noise, it is common to model noise using a symmetric, bell-shaped distribution, known as a normal distribution.\n\n\n\n\n\nDistribution of noise in ice cream dispenser\n\n\n\n\nWe can think of the noise associated with each person’s amount of ice cream as if it came from randomly picking a point somewhere under the bell curve. Since most of the area under the curve is close to the middle, most people’s deviation or noise, will be small, so they expect to get approximately the amount of ice cream they would expect, based on the amount of time they held down the dispenser. There is, however, some area in the tails of the bell curve, indicating that occasionally, a person will get much more or much less ice cream than expected.\nSpread in a Normal Distribution\nA normal distribution is denoted \\(\\mathcal{N}(\\mu, \\sigma)\\).\n\n\\(\\mu\\) represents the mean, or center, of the distribution. When modeling noise, we typically assume \\(\\mu=0\\).\n\n\\(\\sigma\\) represents the standard deviation in the normal distribution. More noise corresponds to higher values of \\(\\sigma\\).\n\nIn the ice cream example, \\(\\sigma\\) can be thought of as the standard deviation in amount of ice cream dispensed among people who hold the dispenser for the same amount of time.\nThe normal distribution seen previously is shown again below (Dist. A), along with two others. The graph shows how the amount of noise can vary between normal distributions. If our ice cream was very consistent and had little noise, it would behave more like Distribution B, with noise most often being close to 0. If there is a lot of variability between ice cream dispensed, even for people holding the dispenser for the same amount of time, it would behave more like Distribution C with more area in the tails resulting in a higher probability of unusually high or low ice cream amounts.\n\n\n\n\n\n\n\n\n\nHere, we’ll assume that the noise associated with our ice cream machine can be characterized by a normal distribution with mean \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\).\nNote that the square of the standard deviation \\(\\sigma^2\\) is called the variance. Some books denote the normal distribution as \\(\\mathcal{N}(\\mu, \\sigma^2)\\), instead of \\(\\mathcal{N}(\\mu,\\sigma)\\). We will use the \\(\\mathcal{N}(\\mu,\\sigma)\\) here, which is consistent with R.\nSimulating the Ice Cream Dispenser\nThe amount of ice cream a person gets is thus determined by both the amount of time they hold the dispenser and by noise, representing factors other than time. The illustration below shows the amount of ice cream people might get as a function of time holding the dispenser. The grey dots represent ice cream amounts for individual people. Notice that people who hold the dispenser for 2 seconds get 4 oz. of ice cream, on average, but some come out higher or lower due to noise.\n\n\n\n\n\nThe amount dispensed follows a normal distribution with mean equal to twice the time holding the dispenser and standard deviation 1\n\n\n\n\nWe’ll simulate the amount of ice cream dispensed for 15 people. We’ll assume the noise associated with each person’s ice cream amount is normally distributed, as in the above illustration. So, we’ll simulate the amount of ice cream they get by adding a random number from the normal distribution shown to each person’s expected amount.\nThe rnorm command below generates random numbers from a normal distribution. Here we tell it to generate 15 numbers from a normal distribution with with mean of 0, and standard deviation of 1, which is consistent with the illustrations we’ve seen.\nThe table shows the time each person pressed the dispenser, the signal (the amount they would be expected to receive based on time holding the dispenser), the noise, and the amount they actually receive.\n\nset.seed(10082020)\n# set times \ntime &lt;- c(2, 2.2, 2.5, 2.8, 3.1, 3.1, 3.3, 3.5, 3.6, 3.8, 3.9, 3.9, 4.1, 4.2, 4.6)\nexpected &lt;- 2*time  # expected amount\nnoise &lt;-rnorm(15, 0, 1) %&gt;% round(2)  #generate noise from normal distribution\namount &lt;- 2*time + noise  # calculate observed amounts\nIcecream &lt;- data.frame(time, signal, noise, amount) # set up data table\nkable((Icecream)) #display table\n\n\n\n\ntime\nsignal\nnoise\namount\n\n\n\n\n2.0\n4.0\n0.46\n4.46\n\n\n2.2\n4.4\n-0.98\n3.42\n\n\n2.5\n5.0\n1.16\n6.16\n\n\n2.8\n5.6\n-0.06\n5.54\n\n\n3.1\n6.2\n0.34\n6.54\n\n\n3.1\n6.2\n-1.86\n4.34\n\n\n3.3\n6.6\n0.10\n6.70\n\n\n3.5\n7.0\n-0.73\n6.27\n\n\n3.6\n7.2\n-0.93\n6.27\n\n\n3.8\n7.6\n0.34\n7.94\n\n\n3.9\n7.8\n-1.19\n6.61\n\n\n3.9\n7.8\n0.23\n8.03\n\n\n4.1\n8.2\n-0.01\n8.19\n\n\n4.2\n8.4\n1.33\n9.73\n\n\n4.6\n9.2\n0.10\n9.30\n\n\n\n\n\nThe scatterplot displays the amount dispensed compared to the time pressing the dispenser. The red line indicates the line \\(y=2x\\). If there was no random noise, then each person’s amount dispensed would lie exactly on this line.\n\nggplot(data=Icecream1, aes(x=time, y=amount)) + geom_point() + ggtitle(\"Icecream Dispensed\") + xlab(\"Time Pressing dispenser\") + ylab(\"Amount Dispensed\") + geom_abline(slope=2, intercept=0, color=\"red\") + xlim(c(1.5,5)) + ylim(c(2.5,10)) +\n  annotate(\"text\", label=\"y=2x\", x= 3.5, y=6.5, size=10, color=\"red\")  + theme_bw()\n\n\n\n\n\n\n\n\nMathematically, we write the amount of ice cream the \\(i-\\)th person gets as the sum of a linear function of time holding the dispenser (the signal), and a normal random variable \\(\\epsilon_i\\) (the noise).\n\\[\n\\text{Amount}_i = \\beta_0 + \\beta_1\\times\\text{Time}_i+\\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nIn this instance, we know that a person who does not press the dispenser at all will, of course, get no ice cream, and a person who holds the dispenser for two seconds is supposed to get 2 ounces of ice cream for each second they hold the dispenser. Thus, we know so \\(\\beta_0=0\\) and \\(\\beta_1=2\\). We also know that \\(\\sigma\\), the standard deviation in amounts between people holding the dispenser for the same amount of time, is \\(\\sigma=1\\). Thus, the true equation of the regression line is:\n\\[\n\\text{Amount}_i = 0 + 2\\times\\text{Time}_i+\\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, 1)\n\\]\n\n\n4.1.3 Estimating Signal\nIn a real situation, we would not see the signal and noise columns in the table or the red line on the graph. We would only know the time and amount, and points on the scatter plot. From these, we would need to estimate the location of the red line by fitting a least squares regression line to the data, as we’ve done before.\nWe can estimate the slope and intercept of the red line, \\(\\beta_0\\) and \\(\\beta_1\\) by fitting a regression line to the observed data. The blue line represents the regression line fit to the data from the 15 people in our simulation. Notice that it is close, but not identical to the red line.\n\nggplot(data=Icecream1, aes(x=time, y=amount)) + geom_point() + ggtitle(\"Icecream Dispensed\") + xlab(\"Time Pressing dispenser\") + ylab(\"Amount Dispensed\") + stat_smooth(method=\"lm\", se=FALSE) + geom_abline(slope=2, intercept=0, color=\"red\")  + xlim(c(1.5,5)) + ylim(c(2.5,10)) + theme_bw()\n\n\n\n\n\n\n\n\nWe fit the model to the simulated data.\n\nIC_Model &lt;- lm(data=Icecream1, lm(amount~time))\nIC_Model\n\n\nCall:\nlm(formula = lm(amount ~ time), data = Icecream1)\n\nCoefficients:\n(Intercept)         time  \n    -0.3223       2.0625  \n\n\nThe intercept and slope are \\(b_0=\\) -0.32 and 2.06.\nThe estimated regression equation is\n\\[\n\\begin{aligned}\n\\text{Amount}_i & = b_0 + b_1\\text{Time}_i + \\epsilon_i \\\\\n& = -0.32 + 2.06\\text{Time}_i + \\epsilon_i\n\\end{aligned}\n\\]\nThe estimates \\(b_0\\) and \\(b_1\\) are close, but not identical to the the true values of \\(\\beta_0\\) and \\(\\beta_1\\), which in this case, we know are \\(\\beta_0=0\\) and \\(\\beta_1=2\\). Typically, the values of \\(b_0\\) and \\(b_1\\) calculated from a sample will deviate from the true values \\(\\beta_0\\) and \\(\\beta_1\\) due to noise in the observed data.\nIt is common for people to be confused about the difference between \\(\\beta_0\\) and \\(\\beta_1\\) and \\(b_0\\) and \\(b_1\\). To reemphasize the distinction, the Greek letters \\(\\beta_0\\) and \\(\\beta_1\\) are parameters about the true mechanism or process, which we typically don’t know. The English letters \\(b_0\\) and \\(b_1\\) are statistics calculated from the sample of data we have. These will vary from sample to sample. As we did in Chapter 3, we’ll use \\(b_0\\) and \\(b_1\\) to estimate and draw inferences about \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n4.1.4 Estimating Noise\nAn estimate for \\(\\sigma\\), the amount of variability in amount of ice cream dispensed between people who hold the dispenser for the same amount of time (i.e. the amount of noise) can be estimated as a function of SSR, sample size (n), and number of parameters terms in the model (p), excluding the intercept.\n\\(s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\).\nThis quantity \\(s\\) is called the residual standard error.\nKeeping with the convention of using Greek letters to represent population parameters \\(\\sigma\\) and English letters to represent estimates calculated from samples, we use \\(s\\) to represent the estimate of \\(\\sigma\\) that we calculate from our sample.\nIn this case, \\(n=15\\), and \\(p=1\\) so we can calculate \\(s\\) as below.\n\ns &lt;- sqrt(sum(IC_Model$residuals^2)/(15-(1+1)))\ns\n\n[1] 0.905437\n\n\nWe notice that \\(s\\) is a reasonable estimate of \\(\\sigma\\), which we know to be 1. In most situations, we won’t know the true value of \\(\\sigma\\) and will use \\(s\\) as an estimate.\n\n\n4.1.5 Ordinary Linear Regression Model\nIn the ice cream example, the relationship between expected amount and time holding the dispenser was given by a linear equation involving a single numeric explanatory variable. We can generalize this to situations with multiple explanatory variables, which might be numeric or categorical.\nIndividual observations are then assumed to vary from their expectation in accordance with a normal distribution, representing random noise (or error).\nThe mathematical form of a linear regression model is\n\\(Y_i = \\beta_0 + \\beta_1X_{i1}+ \\ldots + \\beta_pX_{ip} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\).\nNote that in place of \\(X_{ip}\\), we could have indicators for categories, or functions of \\(X_{ip}\\), such as \\(X_{ip}^2\\), \\(\\text{log}(X_{ip})\\), or \\(\\text{sin}(X_{ip})\\).\n\nThe quantities \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are parameters, pertaining to the true but unknown data generating mechanism.\nThe estimates \\(b_0, b_1, \\ldots, b_p\\), are statistics, calculated from our observed data.\n\nWe use statistics \\(b_0, b_1, \\ldots, b_p\\) to obtain confidence intervals and hypothesis tests to make statements about parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\).\n\n\n\n4.1.6 Model Assumptions\nThe linear regression model depends on four assumptions, which we made when simulating the behavior of the ice cream dispenser.\n\nLinearity - The expected value of the response variable \\(Y\\) is a linear function of the explanatory variable(s).\nEx. the expected amount of ice cream dispensed (i.e the signal) is a linear function of time the dispenser was pressed.\nConstant Variance - the variance between individual values of the response variable is the same for any values/categories of the explanatory variable(s)\nEx. individual amounts dispensed varied from their expected amount with equal variability, regardless of the amount of time. That is, the amount of variability in individual amounts dispensed was the same for people who held the dispenser for 1 s. as for people who held it for 2 s. or 3 s., etc.\nNormality - for any values/categories of the explanatory variable(s) individual response values vary from their expectation in accordance with a normal distribution.\nEx. individual amounts dispensed varied from their expected amount in accordance with a normal distribution.\nIndependence - individual response values are not affected by one another\nEx. the amount of ice cream dispensed for one person was not affected by the amount dispensed for anyone else.\n\n\n\n4.1.7 Statistical Abstraction\nIf we really believe that data come about as the ordinary regression model describes, then probability theory tells us that regression coefficients \\(b_j\\)’s, representing differences between categories for categorical variables and rates of change for quantitative variables, follow symmetric and bell-shaped distributions. We can use this fact to create confidence intervals and perform hypothesis tests, without needing to perform simulation. This is, in fact what R does in it’s model summary output.\nThese methods are only valid, however, if data can reasonably be thought of as having come from a process consistent with the assumptions of the ordinary regression model process. If we don’t believe that our observed data can be reasonably thought of as having come from such a process, then the confidence intervals and p-values produced by R, and other places that rely on probability-based methods will not be reliable.\nThe process of formulating a real world situation as a statistical model is known as statistical abstraction.\nWe close the section with a philosophical question:\nDo data really come about from processes like the ordinary regression model? That is, do you think it is reasonable to believe that data we see in the real world (perhaps the amount of ice cream dispensed by an ice cream machine) represent independent outcomes of a process in which expected outcomes are a linear function of explanatory variables, and deviate from their expectation according to a normal distribution with constant variance?\nWe won’t attempt to answer that question here, but it is worth thinking about. After all, it is an assumption on which many frequently employed methods of statistical inference depends.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#theory-based-inference",
    "href": "Ch4.html#theory-based-inference",
    "title": "4  Inference from Models",
    "section": "4.2 Theory-Based Inference",
    "text": "4.2 Theory-Based Inference\nYou probably noticed that most of the bootstrap and sampling distributions we saw in Chapter 3 were symmetric and bell-shaped in nature. When the assumptions of the ordinary regression model are reasonably satisfied, the distribution of common model estimates, including means, differences in means, and regression slopes will approximately follow symmetric, bell-shaped distributions. In such situations, we can use statistical theory to calculate confidence intervals and perform hypothesis tests, without needing to actually perform the simulations that we did in Chapter 3.\nIn this Section, we’ll look at the formulas and probability distributions we can use when the ordinary linear regression model is appropriate, and see how they lead to results very similar to those we’ve obtain previously via bootstrapping and permutation tests.\n\n4.2.1 Common Standard Error Formulas\nIn Chapter 3, we used bootstrapping to estimate the standard error associated with statistics \\(b_0\\), \\(b_1\\), …\\(b_p\\) in a regression model. Depending on the model, \\(b_j\\) might represent an overall mean, means for different categories, or regression slopes.\nIt is important to realize that the standard error of a statistic \\(\\text{SE}(b_j)\\) is not the same thing as the residual standard error \\(s\\) that was used to estimate \\(\\sigma\\) in the previous section. The standard errors \\(\\text{SE}(b_j)\\), seen in Chapter 3, are estimates of much the statistic \\(b_j\\) could vary between different samples of the same size as ours. On the other hand, the residual standard error \\(s\\) is an estimate of the standard deviation in values of the response variable among individual observations with the same value(s) or category(ies) of all explanatory variables.\nThese quantities are, however, related. It might not be surprising to find out that when there is more variability between individual observations, there is also more variability in statistics between samples. Thus, \\(\\text{SE}(b_j)\\) is a function of \\(s\\).\nRecall\n\\(s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\).\nWe can use \\(s\\) to calculate standard errors \\(\\text{SE}(b_j)\\) when \\(b_j\\) represents a statistic like a mean, difference in means, or slope.\nTheory-Based Standard Error Formulas\n\n\n\n\n\n\n\nStatistic\nStandard Error\n\n\n\n\nSingle Mean\n\\(SE(b_0)=\\frac{s}{\\sqrt{n_{Group}}}\\)\n\n\nDifference in Means\n\\(SE(b_j)=s\\sqrt{\\frac{1}{n_{Groupj}}+\\frac{1}{n_{Comp.Group}}}\\)\n\n\nIntercept in Simple Linear Regression\n\\(SE(b_0)=s\\sqrt{\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}}\\)\n\n\nSlope in Simple Linear Regression\n\\(SE(b_1)=\\sqrt{\\frac{s^2}{\\sum(x_i-\\bar{x})^2}}=\\sqrt{\\frac{1}{n-2}\\frac{{\\sum(\\hat{y}_i-y_i)^2}}{\\sum(x_i-\\bar{x})^2}}\\)\n\n\n\nNotes:\n\n\\(n\\) represents the total sample size. In Formula 1, \\(n_{Group}\\) represents the sample size for the group corresponding to the coefficient \\(b_0\\). If there are no explanatory variables in the model, it will be \\(n\\), otherwise, it will be the size of the baseline group. \\(n_{\\text{Groupj}}\\) represents the sample size for the group represented by \\(b_j\\), and \\(n_{Comp.Group}\\) represents the sample size for the group it’s being compared to (i.e. the baseline group).\nFor the difference in means formula, the standard error estimate \\(s\\sqrt{\\frac{1}{n_1+n_0}}\\) is called a “pooled” estimate since it combines information from all groups to estimate \\(s\\). This is beneficial, as long as the constant variance assumption in the ordinary regression model is appropriate. When there is reason to believe standard deviation differs between groups, we can use an “unpooled” standard error estimate of \\(\\sqrt{\\frac{s_\\text{Groupj}^2}{n_\\text{Groupj}}+\\frac{s_{\\text{CompGroup}}^2}{n_{CompGroup}}}\\), where \\(s_1, s_0\\) represent the standard deviation for the two groups being compared. This can be used in situations where the explanatory variable is categorical and we have doubts about the constant variance assumption. By default, R assumes constant variance and uses a pooled estimate in its model summary output and related tests/intervals.\n\nThere is no theory-based formula for standard error associated with the median or standard deviation. For these, and many other statistics, we rely on simulation to estimate variability between samples.\nThere are formulas for standard errors associated with coefficients in multiple regression, but these require mathematics beyond what is assumed in this class. They involve linear algebra and matrix inversion, which you can read about here if you are interested.\n\n\n4.2.2 Confidence Intervals\nWhen the sampling distribution is symmetric and bell-shaped, approximate 95% confidence intervals can be calculated using the formula,\n\\[\n\\text{Statistic} \\pm 2\\times \\text{Standard Error},\n\\]\nwhere the standard error is estimated using a formula, rather than through bootstrapping.\nIn the next section, we’ll go through some examples to illustrate how to calculate and interpret \\(s\\) and \\(SE(b_j)\\).\nWe’ve now seen 3 different ways to obtain confidence intervals based on statistics calculated from data.\nThe table below tells us what must be true of the sampling distribution for a statistic in order to use each technique.\n\n\n\nTechnique\nNo Gaps\nBell-Shaped\nKnown SE Formula\n\n\n\n\nBootstrap Percentile\nx\n\n\n\n\nBootstrap Standard Error\nx\nx\n\n\n\nTheory-Based\nx\nx\nx\n\n\n\n\n\n4.2.3 t-tests\nA t-distribution is a symmetric, bell-shaped curve. Its shape is similar to that of the normal distribution, only it has more area in the tales, and is less concentrated around the center. When the sampling distribution for a statistic is symmetric and bell-shaped, it can be approximated by a t-distribution.\nThe t-distribution depends on a parameter called degrees of freedom, which determines the thickness of the distribution’s tails. The degrees of freedom is related to the sample size. As the sample size (and thus degrees of freedom) increase, the t-distributions gets closer and closer to a normal distribution with mean 0 and standard deviation 1, which is known as a standard normal distribution.\n\n\n\n\n\n\n\n\n\nWhen the sampling distribution of a statistic is symmetric and bell-shaped, then the ratio\n\\[\nt= \\frac{{\\text{Statistic}}}{\\text{SE}(\\text{Statistic})}  =\\frac{b_j}{\\text{SE}(b_j)}\n\\]\napproximately follows a t-distribution. The statistic \\(t\\) is called a standardized statistic.\nWe can use a t-distribution to perform a hypothesis test with a null hypothesis that there is no difference between groups, or no relationship between variables. The null hypothesis is \\(\\beta_j=0\\).\nThe standardized statistic tells us how many standard errors higher or lower our statistic \\(b_j\\) is than we would expect it to be if our null hypothesis were true.\nThe larger the t-statistic is, the less consistent it is with the null hypothesis. t-statistics more extreme than \\(\\pm 2\\) typically provide evidence against the null hypothesis.\nTo find a p-value, we use a t-distribution to find the probability of obtaining a t-statistic as or more extreme than the one calculated from our data.\nFor example, for a t-distribution with 20 degrees of freedom the probability of obtaining a t-statistic as or more extreme than 1.5 is about 0.14, while the probability of obtaining a t-statistic as or more extreme than 2.8 is only 0.01.\n\n\n\n\n\n\n\n\n\n\n\n4.2.4 F-tests\nJust as the ratio of a regression statistic to its standard error follows a t-distribution, F-statistics also follow a known probability distribution when the assumptions of the ordinary linear regression model are apropriate.\nAn F distribution is a right-skewed distribution. It is defined by two parameters, \\(\\nu_1, \\nu_2\\), called numerator and denominator degrees of freedom.\n\n\n\n\n\n\n\n\n\nWhen the assumptions of the ordinary linear regression model are reasonably satisfied, the F-statistic\n\\[\nF = \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}}\n\\]\napproximately follows an F-distribution with \\(p-q\\) and \\(n-(p+1)\\) degrees of freedom.\nLike with a t-statistic, we can calculate an F-statistic by looking at how extreme our F-statistic is in its F-distribution.\nFor an F-distribution with 2 and 20 degrees of freedom, the probability of getting an F-statistic more extreme than 1.8 is about 0.19, while the probability of getting and F-statistic as extreme or more than 4.6 is about 0.02.\n\n\n\n\n\n\n\n\n\n\n\n4.2.5 Limitations of Theory-Based Inference\nWe’ve seen that in situations where the sampling distribution for a regression coefficient \\(b_j\\) is symmetric and bell-shaped, we can create confidence intervals and perform hypothesis tests using theory-based standard error formulas, as well as t- and F-distributions. This means we won’t need to perform permutation-based hypothesis tests, or bootstrapping for confidence intervals as we did in Chapter 3.\nThere are, however, limitations to this approach, which underscore the importance of the simulation-based approaches seen in Chapter 3.\nThese include:\n\nThere are lots of statistics, like medians and standard deviations, that do not have known standard error formulas, and do not follow symmetric bell-shaped distributions. In more advanced and complicated models, it is common to encounter statistics of interest with unknown sampling distributions. In these cases, we can estimate p-values and build confidence intervals via simulation, even if we cannot identify the distribution by name.\nEven for statistics with known standard error formulas, the t-test is only appropriate when the sampling distribution for \\(b_j\\) is symmetric and bell-shaped. If the assumptions of the ordinary regression model are reasonably satisfied, then we can be assured that this will be the case. Sometimes, however, those assumptions will not be satisfied. There is probability theory which tells us that even when model assumptions do not hold, t-tests and and theory-based confidence intervals for regression coefficients will still be reasonable when the sample size is “large enough,” but there is no consistent definition of what “large enough.” The necessary sample size depends on the context and the degree to which assumptions are violated. Simulation-based approaches offer a more flexible method for performing inference in these situations, and also offer a way to check the results we get using theory-based methods. Furthermore, F-tests become invalid when model assumptions are violated regardless of the sample size.\nThe simulation-based approaches provide valuable insight to the logic behind hypothesis tests. When we permute values of an explanatory variable in a hypothesis test it is clear that we are simulating a situation where the null hypothesis is true. Likewise, when we simulate taking many samples in bootstrapping, it is clear that we are assessing the variability in a statistic across samples. Simply jumping to the t-based approximations of these distributions makes it easy to lose our sense of what they actually represent, and thus increases the likelihood of interpreting them incorrectly.\n\nIn fact, prominent statistician R.A. Fisher wrote of simulation-based methods in 1936:\n“Actually, the statistician does not carry out this very simple and very tedious process, but his conclusions have no justification beyond the fact that they agree with those which could have been arrived at by this elementary method.”\nFisher’s comment emphasizes the fact that probability-based tests, like the t-test are simply approximations to what we would obtain via simulation-based approaches, which were not possible in his day, but are now.\nProponents of simulation-based inference include Tim Hesterberg, Senior Statistician at Instacart, and former Senior Statistician at Google, which heavily used simulation-based tests associated with computer experiments associated with their search settings. Hesterberg wrote a 2015 paper, arguing for the use and teaching of simulation-based techniques.\nWe will move forward by using probability-based inference where appropriate, while understanding that we are merely approximating what we would obtain via simulation. Meanwhile, we’ll continue to employ simulation-based approaches where probability-based techniques are inappropriate or unavailable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#model-based-inference",
    "href": "Ch4.html#model-based-inference",
    "title": "4  Inference from Models",
    "section": "4.3 Model-Based Inference",
    "text": "4.3 Model-Based Inference\nIn this section, we’ll apply the theories and formulas from the previous section to the models won the Florida lakes data that we saw in Chapter 3. We’ll illustrate how to calculate all of the important peices of information provided in the model summary output produced by R.\n\n4.3.1 Difference in Means\nWe’ll begin by using the ordinary regression model to predict a lake’s mercury level, using location (N vs S) as the explanatory variable.\nThe regression model is:\n\\[\n\\text{Mercury} = \\beta_0 +\\beta_1 \\times\\text{Location}_{\\text{South}} + \\epsilon_i, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nThis model assumes:\n\nLinearity - there is no linearity assumption when the explanatory variable is categorical.\nConstant Variance - the variance between mercury levels of individual lakes is the same for Northern Florida, as for Southern Florida.\nNormality - mercury levels are normally distributed in Northern Florida and also in Southern Florida.\nIndependence - mercury levels of individual lakes are not affected by those of other lakes.\n\nA table summarizing the means and standard deviations among lakes in Northern and Southern Florida is shown below.\n\nkable(LakesTable)\n\n\n\n\nLocation\nMeanHg\nStDevHg\nN\n\n\n\n\nN\n0.4245455\n0.2696652\n33\n\n\nS\n0.6965000\n0.3838760\n20\n\n\n\n\n\nNote that the standard deviation in mercury levels is higher in Southern Florida than in Northern Florida, but the difference is not that big. As a rough guide we should be concerned about the constant variance assumption when the standard deviation in one group is more than twice as big as in another. Since that is not the case here, it seems reasonable to use the pooled standard deviation estimate as R does in its lm summary output, discussed next.\nlm Summary Output\nThe lm summary command in R returns estimates, standard errors, test statistics, and p-values pertaining to the model. These are calculated using the formulas in Section 4.2.\n\nM_Lakes_merc_loc &lt;- lm(data=FloridaLakes, Mercury ~ Location)\nsummary(M_Lakes_merc_loc)\n\n\nCall:\nlm(formula = Mercury ~ Location, data = FloridaLakes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.65650 -0.23455 -0.08455  0.24350  0.67545 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)  0.42455    0.05519   7.692 0.000000000441 ***\nLocationS    0.27195    0.08985   3.027        0.00387 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3171 on 51 degrees of freedom\nMultiple R-squared:  0.1523,    Adjusted R-squared:  0.1357 \nF-statistic: 9.162 on 1 and 51 DF,  p-value: 0.003868\n\n\n\nEstimate gives the least-squares estimates \\(b_0, b_1, \\ldots, b_p\\)\nStandard Error gives estimates of the standard deviation in the sampling distribution for estimate. It tells us how the estimate is expected to vary between different samples of the given size. These are computed using the formulas in Section 4.2.1.\nt value is the estimate divided by its standard error.\nPr(&gt;|t|) is a p-value for the hypothesis test associated with the null hypothesis \\(\\beta_j = 0\\), where \\(\\beta_j\\) is the regression coefficient pertaining to the given line. Note that \\(\\beta_j\\) is the unknown population parameter estimated by \\(b_j\\).\nThe Residual Standard Error is \\(s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\). This is an estimate of \\(\\sigma\\), which represents the standard deviation in the distribution of the response variable for given value(s) or category(ies) of explanatory variable(s). It tells us how much variability is expected in the response variable between different individuals with the same values/categories of the explanatory variables.\nThe degrees of freedom are \\(n-(p+1)\\).\nThe Multiple R-Squared value is the \\(R^2\\) value seen in Chapter 2. \\(R^2 = \\frac{\\text{SST} -\\text{SSR}}{\\text{SST}} = \\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y}_i)^2}\\)\nWe know that \\(R^2\\) can never decrease when additional variables are added to a model. The Adjusted-R^2 value is an alternate version of \\(R^2\\) that is designed to penalize adding variables that do little to explain variation in the response.\nThe F-statistic on the bottom line of the R-output corresponds to an F-test of the given model against a reduced model that include no explanatory variables. The p-value on this line is associated with the test of the null hypothesis that there is no relationship between the response variable and any of the explanatory variables. Since SSR for this reduced model is equal to SST, the F-statistic calculation simplifies to:\n\n\\[ F=\\frac{\\frac{SST - SSR}{p}}{\\frac{SSR}{n-(p+1)}} \\]\nThe degrees of freedom associated with the F-statistic are given by \\(p\\) and \\((n-(p+1))\\).\n\nThe p-value associated with the F-statistic tells us the probability of obtaining an F-statistic as extreme or more extreme than we did if there is really no relationship between the response variable and any explanatory variables in our model.\n\nCalculations in Summary Output\nLet’s go through how all of these quantities are calculated and discuss what they tell us.\nIn order to do the calculations, we’ll need to know the values of SSR and SST. These are\nSSR:\n\nsum(M_Lakes_merc_loc$residuals^2)\n\n[1] 5.126873\n\n\nSST:\n\nMeanMerc &lt;- mean(FloridaLakes$Mercury) \nsum((FloridaLakes$Mercury - MeanMerc)^2)\n\n[1] 6.047875\n\n\nWe’ll start with the calculations in the bottom part, underneath the coefficients table, namely residual standard error, Multiple R-squared, and the F-statistic.\n\nResidual Standard Error\n\\[\n\\begin{aligned}\ns & =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} \\\\\n& = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}} \\\\\n& =\\sqrt{\\frac{5.126873}{53-(1+1)}} \\\\\n& = 0.317\n\\end{aligned}\n\\]\nThe standard deviation in mercury concentrations among lakes in the same part of the state is estimated to be 0.317 ppm.\n\n\nMultiple R-Squared\n\\[ R^2 = \\frac{6.047875 - 5.126873}{6.047875} = 0.1523 \\]\n\n\nF-Statistic\n\\[ F=\\frac{\\frac{SST - SSR}{p}}{\\frac{SSR}{n-(p+1)}} = \\frac{\\frac{6.047875 - 5.126873}{1}}{\\frac{5.126873}{53-(1+1)}} = 9.162 \\]\nThis F-statistic is associated with 1 and 51 degrees of freedom.\n\n\np-value for F-Statistic\n\ngf_dist(\"f\", df1=1, df2=51, geom = \"area\", fill = ~ (x &lt; 9.162), show.legend=FALSE) + geom_vline(xintercept=9.162, color=\"red\")  + xlab(\"F\") + theme_bw() + xlim(c(0,10)) \n\n\n\n\n\n\n\n\nWe can use the pf() command to calculate the p-value. Note that pf() this command returns the probability of obtaining an F-statistic less extreme than we did, so we subtract it from 1 to get the probability of obtaining an F-statistic more extreme than we did.\n\n1 - pf(9.162, df1=1, df2=51)\n\n[1] 0.003867677\n\n\nThe large F-statistic and small p-value provide strong evidence against the null hypothesis that there is no relationship between a lake’s location and mercury level. We have strong evidence that there is a relationship between location and mercury level, or in other words, that mean mercury level differs between lakes in Northern and Southern Florida.\nWe’ll now move into the calculations in the coefficients table.\n\n\nEstimates\nThe estimates column returns the estimates of \\(b_0\\) and \\(b_1\\). The estimated regression equation is\n\\[\n\\widehat{\\text{Mercury}} = 0.4245455 + 0.2719545\\times\\text{Location}_{\\text{South}}\n\\]\n\n\nStandard Errors\nThe second column in the coefficients table gives standard errors associated with \\(b_0\\) and \\(b_1\\). These tell us how much these statistics are expected to vary between samples of the given size.\nEstimating \\(\\text{SE}(b_0)\\) and \\(\\text{SE}(b_1)\\)\nWe’ll use the theory-based formulas to calculate the standard errors for \\(b_0\\) and \\(b_1\\).\nIn this case, \\(b_0\\) represents a single mean, the mean mercury level for lakes in Northern Florida. Since there are 33 such lakes, the calculation is:\n\\(SE(b_0)=\\frac{s}{\\sqrt{n_\\text{North}}} = \\frac{0.317}{\\sqrt{33}} \\approx 0.055\\)\nThe standard error of intercept \\(b_0\\) is 0.055. This represents the variability in average mercury level in different samples of 33 lakes from Northern Florida.\n\\(SE(b_1)=s\\sqrt{\\frac{1}{n_{\\text{South}}}+\\frac{1}{n_{\\text{North}}}} = 0.317\\sqrt{\\frac{1}{20}+\\frac{1}{33}} = 0.0898\\)\nThe standard error of slope \\(b_1\\) is 0.0898. This represents the variability in average difference in mercury levels between samples of 33 lakes from Northern Florida and 20 lakes from Southern Florida.\nThese numbers match the values in the Std. Error column of the coefficients table of the lm summary output.\n\n\nConfidence Intervals\nWe can use these standard errors to calculate 95% confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\).\nA 95% confidence interval for \\(\\beta_0\\) is given by\n\\[\n\\begin{aligned}\n& b_0 \\pm 2\\times\\text{SE}(b_0) \\\\\n& = 0.42455 \\pm 2\\times{0.055} \\\\\n& = (0.314, 0.535)\n\\end{aligned}\n\\]\nWe are 95% confident that the mean mercury level among all lakes in Northern Florida is between 0.314 and 0.535 ppm.\nA 95% confidence interval for \\(\\beta_1\\) is given by\n\\[\n\\begin{aligned}\n& b_1 \\pm 2\\times\\text{SE}(b_1) \\\\\n& = 0.272 \\pm 2\\times{0.0898} \\\\\n& = (0.09, 0.45)\n\\end{aligned}\n\\]\nWe are 95% confident that the average mercury level in Southern Lakes is between 0.09 ppm and 0.45 ppm higher than in Northern Florida.\nThese intervals can be obtained directly in R using the confint command.\n\nconfint(M_Lakes_merc_loc, level=0.95)\n\n                 2.5 %    97.5 %\n(Intercept) 0.31374083 0.5353501\nLocationS   0.09157768 0.4523314\n\n\n\n\nHypotheses\nBefore interpreting t-values and p-values in the coefficients table, we should state the hypotheses associated with them.\nHypothesis Test for line (intercept)\nNull Hypothesis: The average mercury level among all lakes in North Florida is 0 (\\(\\beta_0=0\\)).\nAlternative Hypothesis: The average mercury level among all lakes in Northern Florida is not 0 (\\(\\beta_0\\neq 0\\)).\nWe already know the average mercury level among all lakes in North Florida is not 0, so this is a silly test. Not every test reported in the R output is a meaningful one, so we should not attempt to draw conclusions from this p-value.\nHypothesis Test for line LocationS\nNull Hypothesis: There is no difference in average mercury levels between Northern and Southern Florida (\\(\\beta_1=0\\)).\nAlternative Hypothesis: There is a difference in average mercury levels in Northern and Southern Florida (\\(\\beta_1\\neq 0\\)).\n\n\nt-value\nWe calculate standardized t-statistics associated with \\(b_0\\) and \\(b_1\\).\nt-statistic for \\(b_0\\):\n\\(t=\\frac{{b_0}}{\\text{SE}(b_0)} = \\frac{0.42455}{0.05519} = 7.692\\)\nIn our sample, the average mercury level among lakes in Northern Florida is 7.7 standard errors higher than we would expect it to be if the average mercury level among all lakes in Northern Florida is 0. Since we know that the mean mercury level among all lakes in Northern Florida is not 0, this is not a meaningful interpretation.\nt-statistic for \\(b_1\\):\n\\(t=\\frac{{b_1}}{\\text{SE}(b_1)} = \\frac{0.27195}{0.08985} = 3.027\\)\nIn our sample, the average mercury level among lakes in Southern Florida is 3 standard errors higher than we would expect it to be if the there was really no difference in average mercury levels between lakes in Northern and Southern Florida. Equivalently, we could say that the mean mercury level among lakes in Southern Florida is 3 standard errors higher than in Northern Florida.\n\n\nPr(&gt;|t|)\np-value for line (Intercept)\nWe already know the average mercury level among all lakes in North Florida is not 0, so this is a silly test. Not every test reported in the R output is a meaningful one, so we should not attempt to draw conclusions from this p-value.\np-value for line LocationS\nWe plot the t-statistic of 3.027 that we observed in our data and observe where it lies on a t-distribution.\n\nts=3.027\ngf_dist(\"t\", df=51, geom = \"area\", fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=\"red\")  + xlab(\"t\") + theme_bw()\n\n\n\n\n\n\n\n\nWe use the pt command to calculate the probability of getting a t-statistic as extreme or more extreme than the 3.027 we saw in our data. We multiply by 2 to get the area on both the left and right extremes.\n\n2*pt(3.027, df=51, lower.tail = FALSE)\n\n[1] 0.003866374\n\n\nThe low p-value gives us strong evidence of a difference in average mercury levels between lakes in Northern and Southern Florida.\nNote that the p-value from the t-test is the same as for the F-statistic. This will happen when there is only one variable in the model.\n\n\nComparison to Simulation\nLet’s compare the confidence intervals and p-values we got to the ones we obtained in Chapter 3 using bootstrapping and a permutation test.\nFormula-Based 95% Confidence Interval for \\(b_1\\)\n\nconfint(M_Lakes_merc_loc,  parm = c(\"LocationS\"), level=0.95)\n\n               2.5 %    97.5 %\nLocationS 0.09157768 0.4523314\n\n\nBootstrap 95% Confidence Interval for \\(b_1\\):\n\nconfint(M_Lakes_merc_loc_Bootstrap, parm = c(\"LocationS\"),\n              level = 0.95, method = \"se\")\n\n       name      lower     upper level method  estimate margin.of.error\n1 LocationS 0.08256301 0.4586334  0.95 stderr 0.2719545       0.1880352\n\n\np-value from t-distribution\n\nts=3.027\ngf_dist(\"t\", df=51, geom = \"area\", fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=\"red\")  + xlab(\"t\") + theme_bw()\n\n\n\n\n\n\n\n\n\n2*pt(3.027, df=51, lower.tail = FALSE)\n\n[1] 0.003866374\n\n\nPermutation - based p-value\n\nEstimate &lt;- 0.272\nExtreme &lt;- abs(Lakes_merc_loc_perm_test$LocationS) &gt; 0.272\n  \n  \nLakes_merc_loc_perm_test_plot &lt;- ggplot(data=Lakes_merc_loc_perm_test, aes(x=LocationS)) + \n  geom_histogram(aes(fill = Extreme), color=\"white\") + \n  scale_fill_manual(values = c(\"FALSE\" = \"blue\", \"TRUE\" = \"red\")) +\n  geom_vline(xintercept=c(Estimate, -1*Estimate), color=\"red\") + \n  xlab(\"Lakes: Simulated Value of b1\") + ylab(\"Frequency\") + \n  ggtitle(\"Distribution of b1 under assumption of no relationship\") +\n  theme(legend.position = \"none\") + theme_bw()\n\nLakes_merc_loc_perm_test_plot\n\n\n\n\n\n\n\n\n\nprop(~ (abs(LocationS) &gt; 0.272), data = Lakes_merc_loc_perm_test)\n\nprop_TRUE \n    0.003 \n\n\nThe formula-based confidence interval and p-value are very close to those produced by simulation. Both approximations, so they are not expected to be identical, but should be close. They lead us to the same conclusion, namely that there is a difference in mean mercury level between lakes in Northern and Southern Florida and that the difference is somewhere between about 0.10 and 0.45 ppm.\n\n\n\n4.3.2 Regression Slope and Intercept\nWe’ll use the ordinary regression model to predict a lake’s mercury level, using pH as the explanatory variable. In this case, the explanatory variable is quantitative, so \\(\\beta_1\\) represents a slope.\nThe regression model is:\n\\[\n\\text{Mercury} = \\beta_0 +\\beta_1 \\times\\text{pH} + \\epsilon_i, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nThis model assumes:\n\nLinearity - the mercury level is a linear function of pH.\nConstant Variance - the variance between mercury levels of individual lakes is the same for each pH level.\nNormality - for each pH, mercury levels are normally distributed.\nIndependence - mercury levels of individual lakes are not affected by those of other lakes.\n\nWe might have doubts about some of these assumptions. For example, if we believe there might be more variability in mercury levels among lakes with higher pH levels than lower ones (a violation of the constant variance assumption), or that lakes closer together are likely to have similar mercury levels (a violation of the independence assumption) then the results of the model might not be reliable.\nIn Chapter 5, we’ll learn ways to check the appropriateness of these assumptions. For now, we’ll assume that the model is a reasonable enough approximation of reality and use it accordingly.\nThe lm summary command in R returns information pertaining to the model.\n\nM_Lakes_merc_pH &lt;- lm(data=FloridaLakes, Mercury ~ pH)\nsummary(M_Lakes_merc_pH)\n\n\nCall:\nlm(formula = Mercury ~ pH, data = FloridaLakes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.48895 -0.19188 -0.05774  0.09456  0.71134 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)  1.53092    0.20349   7.523 0.000000000814 ***\npH          -0.15230    0.03031  -5.024 0.000006572811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2816 on 51 degrees of freedom\nMultiple R-squared:  0.3311,    Adjusted R-squared:  0.318 \nF-statistic: 25.24 on 1 and 51 DF,  p-value: 0.000006573\n\n\nLet’s go through the calculations in the coefficients table and the rows below it. We first need to calculate SST and SSR as well as \\(\\bar{x}\\) and \\(\\sum(x_i-\\bar{x})^2\\), where \\(x\\) represents the explanatory variable, \\(pH\\).\nSST:\n\nMean_merc &lt;- mean(FloridaLakes$Mercury)\nsum((FloridaLakes$Mercury-Mean_merc)^2)\n\n[1] 6.047875\n\n\nSSR:\n\nsum(M_Lakes_merc_pH$residuals^2)\n\n[1] 4.045513\n\n\n\\(\\bar{x}\\):\n\nmean(FloridaLakes$pH)\n\n[1] 6.590566\n\n\n\\(\\sum(x_i-\\bar{x})^2\\)\n\nMean_pH &lt;- mean(FloridaLakes$pH)\nsum((FloridaLakes$pH-Mean_pH)^2)\n\n[1] 86.32528\n\n\n\n4.3.2.1 Residual Standard Error\n\\[ s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{\\text{SSR}}{n-(p+1)}} = \\sqrt{\\frac{4.045513}{53-(1+1)}}=0.2816 \\]\nThe degrees of freedom associated with this estimate is \\(53-(1+1) = 51\\).\n\n\n4.3.2.2 Multiple R-Squared\n\\[ R^2 = \\frac{6.047875 - 4.045513}{6.047875} = 0.3311 \\]\n\n\n4.3.2.3 F-Statistic\nThe F-statistic is\n\\[ F=\\frac{\\frac{SST - SSR}{p}}{\\frac{SSR}{n-(p+1)}} = \\frac{\\frac{6.047875 - 4.045513}{1}}{\\frac{4.045513}{53-(1+1)}} = 25.24 \\]\nThis F-statistic is associated with 1 and 51 degrees of freedom.\n\n\np-value associated with F-statistic\n\ngf_dist(\"f\", df1=1, df2=51, geom = \"area\", fill = ~ (x &lt; 9.162), show.legend=FALSE) + geom_vline(xintercept=25.24, color=\"red\")  + xlab(\"F\") + theme_bw() + xlim(c(0,30)) \n\n\n\n\n\n\n\n\n\n1 - pf(25.24, df1=1, df2=51) |&gt; round(4)\n\n[1] 0\n\n\n\n\nEstimates\nThe estimates \\(b_0 = 1.53\\) and \\(b_1=-0.15\\) are calculated by fitting the regression line that minimizes the sum of the squared residuals.\n\n\nStandard Errors\n\\[ SE(b_0)=s\\sqrt{\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}} = 0.2816\\sqrt{\\frac{1}{53} + \\frac{6.59^2}{86.32528} } = 0.2034 \\]\n\\(SE(b_0)\\) represents the variability in mercury levels among lakes with pH of 0 between different samples of size 53. Since we don’t have any lakes with pH of 0, this is not a meaningful calculation.\n\\[ SE(b_1)=\\sqrt{\\frac{s^2}{\\sum(x_i-\\bar{x})^2}}=\\sqrt{\\frac{0.2816^2}{86.32528}} = 0.0303 \\]\n\\(SE(b_1)\\) represents the variability in rate of change in mercury level for each additional one unit increase in pH, between different samples of size 53.\n\n\nConfidence Interval\nA 95% confidence interval for \\(\\beta_0\\) is given by\n\\[\n\\begin{aligned}\n& b_0 \\pm 2\\times\\text{SE}(b_0) \\\\\n& = 1.53 \\pm 2\\times{0.203} \\\\\n& = (1.12, 1.94)\n\\end{aligned}\n\\] This, in theory, would be telling us that we are 95% confident that the mean mercury level among all lakes in Florida with pH 0 would be between 1.12 and 1.94 ppm. Since there are no such lakes, this is not a sensible interpretation.\nA 95% confidence interval for \\(\\beta_1\\) is given by\n\\[\n\\begin{aligned}\n& b_1 \\pm 2\\times\\text{SE}(b_1) \\\\\n& = -0.15 \\pm 2\\times{0.03} \\\\\n& = (-0.21, -0.09)\n\\end{aligned}\n\\]\nWe are 95% confident that for each 1-unit increase in pH, mercury level is expected to decrease between 0.09 and 0.21 ppm.\nThese intervals can be obtained directly in R using the confint command.\n\nconfint(M_Lakes_merc_pH, level=0.95)\n\n                 2.5 %      97.5 %\n(Intercept)  1.1223897  1.93944769\npH          -0.2131573 -0.09144445\n\n\n\n\nHypotheses\nHypothesis Test for Intercept Line\nNull Hypothesis: The average mercury level among all Florida lakes with pH = 0 is 0. (\\(\\beta_0=0\\)).\nAlternative Hypothesis: The average mercury level among all Florida lakes with pH = 0 not 0. (\\(\\beta_0 \\neq 0\\)).\nSince there are no lakes with pH level 0, this is not a meaningful test.\nHypothesis Test for pH Line\nNull Hypothesis: There is no relationship between mercury and pH level among all Florida lakes. (\\(\\beta_1=0\\)).\nAlternative Hypothesis: There is a relationship between mercury and pH level among all Florida lakes. (\\(\\beta_1 \\neq 0\\)).\n\n\nt-value\nt-statistic for b_0\n\\(t=\\frac{{b_j}}{\\text{SE}(b_j)} = \\frac{1.53}{2.03} = 7.72\\)\nIn our sample, the mean mercury level in lakes with pH 0 is estimated to be 7 standard errors lower than we would expect if the mean mercury level in lakes with pH 0 was 0. This is not a meaningful interpretation since no lakes have pH 0.\nt-statistic for b_1\n\\(t=\\frac{{b_j}}{\\text{SE}(b_j)} = \\frac{-0.15230}{0.03031} = -5.024\\)\nIn our sample, the estimated change in mercury for each on unit change in pH is 5 standard errors less than we would expect it to be if there were no relationship between mercury and pH.\n\n\nPr(&gt;|t|)\np-value for line intercept\nSince the null hypothesis on the intercept line is not a sensible one, we should not draw conclusions based on this p-value.\np-value for line pH\n\nts=5.024\ngf_dist(\"t\", df=51, geom = \"area\", fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=\"red\")  + xlab(\"t\") + theme_bw()\n\n\n\n\n\n\n\n\n\n2*pt(-abs(ts), df=51)\n\n[1] 0.000006578117\n\n\nThe p-value is extremely small, providing strong evidence of a relationship between a lake’s pH and mercury content.\n\n\nComparison to Simulation\nWe compare this standard error estimate and confidence interval to the ones we would obtain via bootstrapping.\nFormula-Based 95% Confidence Interval for \\(\\beta_1\\):\n\nconfint(M_Lakes_merc_pH, parm=c(\"pH\"), level=0.95)\n\n        2.5 %      97.5 %\npH -0.2131573 -0.09144445\n\n\nBootstrap 95% Confidence Interval for \\(\\beta_1\\):\n\nM_Lakes_merc_pH_Bootstrap &lt;- do(10000) * lm(Mercury ~  pH, data = resample(FloridaLakes))\n\nconfint(M_Lakes_merc_pH_Bootstrap, parm = c(\"pH\"),\n              level = 0.95, method = \"se\")\n\n  name      lower      upper level method   estimate margin.of.error\n1   pH -0.2049221 -0.1000458  0.95 stderr -0.1523009      0.05243817\n\n\np-value from t-distribution\n\nts=5.024\ngf_dist(\"t\", df=51, geom = \"area\", fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=\"red\")  + xlab(\"t\") + theme_bw()\n\n\n\n\n\n\n\n\n\n2*pt(-abs(ts), df=51)\n\n[1] 0.000006578117\n\n\nPermutation - based p-value\n\nLakes_merc_pH_perm_test_plot\n\n\n\n\n\n\n\n\n\nprop(~ (abs(pH) &gt; 0.15), data = Lakes_merc_pH_perm_test)\n\nprop_TRUE \n        0 \n\n\nThe interval and p-value produced by the theory based methods are again very similar those we obtained via simulation and lead us to the same conclusions.\n\n\n\n4.3.3 Single Mean\nNow, we’ll look at a simple model in which we seek to estimate the mean mercury level among all lakes in Florida, based on the 53 lakes in our sample.\nThe regression model is:\n\\[\n\\text{Mercury} = \\beta_0  + \\epsilon_i, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nThis model assumes:\n\nLinearity - there is no linearity assumption since there are no explanatory variables.\nConstant Variance - there is no linearity assumption since there are no explanatory variables.\nNormality - mercury levels among all lakes in Florida are normally distributed.\nIndependence - mercury levels of individual lakes are not affected by those of other lakes.\n\nThe lm summary command in R returns information pertaining to the model.\n\nM_Lakes_merc_0 &lt;- lm(data=FloridaLakes, Mercury ~ 1)\nsummary(M_Lakes_merc_0)\n\n\nCall:\nlm(formula = Mercury ~ 1, data = FloridaLakes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.48717 -0.25717 -0.04717  0.24283  0.80283 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  0.52717    0.04684   11.25 0.00000000000000151 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.341 on 52 degrees of freedom\n\n\nThe model estimates that the average mercury level is 0.527, which matches the sample mean, as we know it should.\nWe’ll again go through the calculations in the model summary output.\nWe first calculate SST, which is equal to SSR for a model with no explanatory variables.\n\nMean_merc &lt;- mean(FloridaLakes$Mercury)\nsum((FloridaLakes$Mercury-Mean_merc)^2)\n\n[1] 6.047875\n\n\n\n4.3.3.1 Residual Standard Error\nSince there are no explanatory variables in this model, p=0.\n\\[ s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}}  = \\sqrt{\\frac{6.047875}{53-(1+0)}}=0.341\\]\nThe degrees of freedom associated with this estimate is \\(53-(1+0) = 51\\).\n\n\nMultiple R-Squared\nSince there are no explanatory variables in the model, SSR=SST, so \\(R^2=0\\). Notice that \\(R^2\\) is not given in the model summary.\n\n\nF-Statistic\nSince there are no explanatory variables in the model, SSR=SST, so \\(F=0\\). Notice that F is not given in the model summary.\n\n\nEstimates\n\\(b_0 = \\bar{y} = 0.527\\)\nThe mean mercury level among all lakes in Florida is 0.527 ppm.\n\n\nStandard Errors\n\\(\\text{SE}(b_0) = \\frac{s}{\\sqrt{n}} =\\frac{0.341}{\\sqrt{53}} = 0.4684\\)\n\n\nConfidence Intervals\nA 95% confidence interval for \\(\\beta_0\\) is\n\\[\n\\begin{aligned}\n& b_0 \\pm 2\\times\\text{SE}(b_0) \\\\\n& = 0.527 \\pm 2\\times{0.0468} \\\\\n& = (0.43, 0.62)\n\\end{aligned}\n\\] We are 95% confident that the mean mercury level among all lakes in Florida is between 0.43 and 0.62 parts per million.\n\n\nHypotheses\nThe null hypothesis on the line (Intercept) is that the average mercury level among all lakes in Florida is 0. We know this isn’t true, so there is no reason to perform the hypothesis test.\n\n\nt-value\n\\(t=\\frac{b_0}{\\text{SE}(b_0)} = \\frac{0.527}{0.0468} = 11.25\\)\nThe mean mercury level among the lakes in the sample is 11 standard errors higher than we would expect it to be if the mean mercury level among all lakes in the state were actually 0, but since we know that’s not the case, this is not a meaningful interpretation.\n\n\nPr(&gt;|t|)\np-value for line (Intercept)\n\n2*pt(-abs(11.25), df=52)\n\n[1] 0.000000000000001523657\n\n\nThe p-value is extremely small, but it is not meaningful since the null hypothesis being tested in nonsensical.\n\n\nComparison to Simulation\nFormula-based confidence interval for mean mercury level\n\nconfint(M_Lakes_merc_0)\n\n                2.5 %    97.5 %\n(Intercept) 0.4331688 0.6211709\n\n\nBootstrap confidence interval for mean mercury level\n\nconfint(Lakes_Bootstrap_Mean, level = 0.95, parm= c(\"Intercept\"), method = \"stderr\")\n\n       name     lower    upper level method  estimate margin.of.error\n1 Intercept 0.4375052 0.617128  0.95 stderr 0.5271698      0.08981142\n\n\n\n\n\n4.3.4 Comparing Three Groups\nNow, we’ll look at a model comparing mercury levels between deep, medium, and shallow lakes.\nThe table shows the mean and standard deviation in mercury and number of lakes at each depth level.\n\nkable(Lakes_merc_cond_Tab)\n\n\n\n\nDepth\nMean_Hg\nSD_Hg\nN\n\n\n\n\nDeep\n0.604\n0.307\n15\n\n\nMedium\n0.591\n0.429\n16\n\n\nShallow\n0.428\n0.276\n22\n\n\n\n\n\nNote that the standard deviations within each group are pretty similar. This suggests that the constant variance model assumption appears reasonable, so the R lm summary output, which uses a pooled standard deviation estimate should be reliable.\nThe model equation is:\n\\[\n\\widehat{\\text{Mercury}} = b_0 + b_1\\times\\text{DepthMedium} + b_2\\times\\text{DepthShallow}\n\\]\nThe model summary output is shown below.\n\nM_Lakes_merc_depth &lt;- lm(data=FloridaLakes, Mercury~Depth)\nsummary(M_Lakes_merc_depth)\n\n\nCall:\nlm(formula = Mercury ~ Depth, data = FloridaLakes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55125 -0.24818 -0.08818  0.23875  0.72600 \n\nCoefficients:\n             Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)   0.60400    0.08701   6.942 0.0000000074 ***\nDepthMedium  -0.01275    0.12111  -0.105        0.917    \nDepthShallow -0.17582    0.11284  -1.558        0.126    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.337 on 50 degrees of freedom\nMultiple R-squared:  0.06115,   Adjusted R-squared:  0.02359 \nF-statistic: 1.628 on 2 and 50 DF,  p-value: 0.2065\n\n\nSSR is\n\nsum(M_Lakes_merc_depth$residuals^2)\n\n[1] 5.678062\n\n\nSST is\n\nMean_merc &lt;- mean(FloridaLakes$Mercury)\nsum((FloridaLakes$Mercury - Mean_merc)^2)\n\n[1] 6.047875\n\n\n\nResidual Standard Error\nSince there are 2 explanatory variables in this model (DepthMedium and DepthShallow), p=2.\n\\[ s =\\sqrt{\\frac{\\text{SSR}}{n-(p+1)}}  = \\sqrt{\\frac{5.678062}{53-(1+2)}}=0.337\\]\n\n\nMultiple R-Squared\n\\(R^2 = \\frac{6.047875 - 5.678062}{6.047875} =0.06115\\)\n6% of the total variation in mercury level is explained by depth.\n\n\nF-Statistic\n\\[ F=\\frac{\\frac{SST - SSR}{p}}{\\frac{SSR}{n-(p+1)}} = \\frac{\\frac{6.047875 - 5.678062}{2}}{\\frac{5.678062}{53-(1+2)}} = 1.628 \\]\n\n\np-value for F-Statistic\n\ngf_dist(\"f\", df1=2, df2=50, geom = \"area\", fill = ~ (x &lt; 1.628), show.legend=FALSE) + geom_vline(xintercept=1.628, color=\"red\")  + xlab(\"F\") + theme_bw() + xlim(c(0,5)) \n\n\n\n\n\n\n\n\n\n1 - pf(1.628, df1=2, df2=50) |&gt; round(4)\n\n[1] 0.2066\n\n\nThe large p-value tells us we do not have enough evidence to conclude that a model accounting for depth better explains variability in mercury level than one that does not, implying we don’t have enough evidence to conclude that average mercury level differs between shallow, medium, and deep lakes.\n\n\nEstimates\n\\(b_0 = 0.604\\) - the average mercury content in deep lakes in Florida is 0.604 ppm.\n\\(b_1 = 0.013\\) - the average mercury content in medium lakes in Florida is 0.013 ppm less than in deep lakes.\n\\(b_2 = 0.18\\) - the average mercury content in shallow lakes in Florida is 0.18 ppm less than in deep lakes.\n\n\nStandard Errors\n\\(SE(b_0)=\\frac{s}{\\sqrt{n_\\text{Deep}}} = \\frac{0.604}{\\sqrt{15}} \\approx 0.087\\)\n\\(SE(b_1)=s\\sqrt{\\frac{1}{\\text{Deep}}+\\frac{1}{\\text{Medium}}} = 0.317\\sqrt{\\frac{1}{15}+\\frac{1}{16}} = 0.1211\\)\n\\(SE(b_2)=s\\sqrt{\\frac{1}{\\text{Deep}}+\\frac{1}{\\text{Shallow}}} = 0.317\\sqrt{\\frac{1}{15}+\\frac{1}{22}} = 0.1128\\)\n\n\nConfidence Intervals\nWe can use these standard errors to calculate 95% confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\).\nA 95% confidence interval for \\(\\beta_0\\) is given by\n\\[\n\\begin{aligned}\n& b_0 \\pm 2\\times\\text{SE}(b_0) \\\\\n& = -0.01275 \\pm 2\\times{0.087} \\\\\n& = (0.43, 0.78)\n\\end{aligned}\n\\]\nWe are 95% confident that the mean mercury level among all deep lakes in Florida is between 0.43 and 0.78 ppm.\nA 95% confidence interval for \\(\\beta_1\\) is given by\n\\[\n\\begin{aligned}\n& b_1 \\pm 2\\times\\text{SE}(b_1) \\\\\n& = -0.01275 \\pm 2\\times{0.1211} \\\\\n& = (-0.26, 0.23)\n\\end{aligned}\n\\]\nWe are 95% confident that the average mercury level in lakes of medium depth is between 0.26 ppm lower and 0.23 ppm higher than in deep lakes.\nA 95% confidence interval for \\(\\beta_2\\) is given by\n\\[\n\\begin{aligned}\n& b_1 \\pm 2\\times\\text{SE}(b_1) \\\\\n& = -0.17582 \\pm 2\\times{0.1128} \\\\\n& = (0.09, 0.45)\n\\end{aligned}\n\\]\nWe are 95% confident that the average mercury level in shallow lakes is between 0.40 ppm lower and 0.05 ppm higher than in deep lakes.\nWe confirm these intervals using the confint command.\n\nconfint(M_Lakes_merc_depth, level=0.95)\n\n                  2.5 %     97.5 %\n(Intercept)   0.4292352 0.77876484\nDepthMedium  -0.2560124 0.23051236\nDepthShallow -0.4024618 0.05082548\n\n\n\n\nHypotheses\nHypothesis Test for line (intercept)\nNull Hypothesis: The average mercury level among all deep lakes in Florida is 0 (\\(\\beta_0=0\\)).\nAlternative Hypothesis: The average mercury level among all deep lakes in Florida is not 0 (\\(\\beta_0\\neq 0\\)).\nWe know the average mercury level is not 0, so there is no reason to do this hypothesis test.\nHypothesis Test for line DepthMedium\nNull Hypothesis: There is no difference in average mercury levels between deep and medium lakes in Florida (\\(\\beta_1=0\\)).\nAlternative Hypothesis: There is a difference in average mercury levels between deep and medium lakes in Florida (\\(\\beta_1\\neq 0\\)).\nHypothesis Test for line DepthShallow\nNull Hypothesis: There is no difference in average mercury levels between shallow and deep lakes in Florida (\\(\\beta_2=0\\)).\nAlternative Hypothesis: There is a difference in average mercury levels between shallow and deep lakes in Florida (\\(\\beta_2\\neq 0\\)).\n\n\nt-value\nt-statistic for \\(b_0\\):\n\\(t=\\frac{{b_0}}{\\text{SE}(b_0)} = \\frac{0.60400}{0.08701} = 6.942\\)\nIn our sample, the average mercury level among deep lakes in Florida is 6.9 standard errors higher than we would expect it to be if the average mercury level among all deep lakes in Florida is 0. Since we know that the mean mercury level among all deep lakes in Florida is not 0, this is not a meaningful interpretation.\nt-statistic for \\(b_1\\):\n\\(t=\\frac{{b_1}}{\\text{SE}(b_1)} = \\frac{-0.01275}{0.12111} = -0.105\\)\nIn our sample, the average mercury level among all medium lakes in Florida is 0.1 standard errors lower than we would expect it to be if the there was really no difference in average mercury levels between deep and medium lakes in Florida. Equivalently, we could say that the mean mercury level among lakes in medium depth lakes is 0.1 standard errors lower than in deep lakes.\nt-statistic for \\(b_2\\):\n\\(t=\\frac{{b_2}}{\\text{SE}(b_2)} = \\frac{-0.17582}{0.11284} = -1.558\\)\nIn our sample, the average mercury level among all shallow lakes in Florida is 1.56 standard errors lower than we would expect it to be if the there was really no difference in average mercury levels between deep and shallow lakes in Florida. Equivalently, we could say that the mean mercury level among lakes in shallow depth lakes is 1.56 standard errors lower than in deep lakes.\n\n\nPr(&gt;|t|)\np-value for line (Intercept)\nWe already know the average mercury level among all deep lakes in Florida is not 0, so this is a silly test and we won’t elaborate on it.\np-value for line DepthMedium\n\nts=-0.105\ngf_dist(\"t\", df=50, geom = \"area\", fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=\"red\")  + xlab(\"t\") + theme_bw()\n\n\n\n\n\n\n\n\n\n2*pt(0.105, df=50, lower.tail = FALSE)\n\n[1] 0.9167959\n\n\nThe p-value is very large, indicating we have no evidence of differences between in average mercury level between deep and medium lakes.\np-value for line DepthShallow\n\nts=1.558\ngf_dist(\"t\", df=50, geom = \"area\", fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=\"red\")  + xlab(\"t\") + theme_bw()\n\n\n\n\n\n\n\n\n\n2*pt(1.558, df=50, lower.tail = FALSE)\n\n[1] 0.125541\n\n\nThe p-value is too big to conclude that there is evidence of differences in mean mercury level between deep and shallow lakes.\nWe did observe an average difference of about 0.18 ppm, which could potentially be meaningful, but given the size in our sample and amount of variability in the data, we cannot rule out the possibility that this difference was just a result of which lakes got randomly selected for the study.\n\n\nPairwise Comparisons\nThe model summary output allows us to compare mercury levels in medium and deep lakes to shallow ones, but it does not directly compare shallow and medium lakes. The pairs function in the emmeans command performs t-tests for all pairs of a categorical variable.\n\nlibrary(emmeans)\nM_Lakes_merc_depth &lt;- lm(data=FloridaLakes, Mercury~Depth)\nGroupMeans &lt;- emmeans(M_Lakes_merc_depth, ~ Depth)\npairs(GroupMeans, adjust = \"none\")\n\n contrast         estimate    SE df t.ratio p.value\n Deep - Medium      0.0127 0.121 50   0.105  0.9166\n Deep - Shallow     0.1758 0.113 50   1.558  0.1255\n Medium - Shallow   0.1631 0.111 50   1.473  0.1471\n\n\nWe see that the t-ratios and p-values for the first two lines match those in the lm output, and we also get a comparison for medium and shallow lakes.\n\n\nMultiple Testing Error\nIn the previous table, we compared three groups simultaneously. Running multiple hypothesis tests at the same time increases the risk of a false positive - that is, detecting a difference when there actually isn’t one.\nIf we only ran one test and rejected the null hypothesis if the p-value is less than 0.05, then we would only have a 0.05 chance of rejecting the null hypothesis when it is actually true. If we run three tests though, and reject the null hypothesis for any test with a p-value less than 0.05, we will have a \\(1 - 0.95^3 = 0.14\\) probability of rejecting a null hypothesis that is actually true. This problem gets worse the more tests we run, making it a big problem in areas like genomics, where hundreds of thousands of tests (or more) are run simultaneously.\nOne popular strategy for avoiding false positives when performing multiple tests is to simply multiple each p-value by the number of tests being run. This approach is called the Bonferroni Correction. In this case, since we are running three tests, instead of rejecting a null hypothesis when the p-value for the individual test is less than 0.05, we would need it to be less than \\(0.05/3 = 0.0167\\). This ensures that the probability of wrongly rejecting the null hypothesis in any of our tests remains 0.05. The downside to the Bonferroni Correction is that it is a conservative method and can prevent us from detecting differences that are actually there. Other, more advanced methods, have been suggested in specific situations to try to improve on Bonferroni’s approach and controlling multiple testing error remains an active area of research in statistics and biostatistics.\nIn R, we can implement the Bonferroni correction using adjust=\"Bonferroni in the pairs command.\n\nlibrary(emmeans)\nM_Lakes_merc_depth &lt;- lm(data=FloridaLakes, Mercury~Depth)\nGroupMeans &lt;- emmeans(M_Lakes_merc_depth, ~ Depth)\npairs(GroupMeans, adjust = \"Bonferroni\")\n\n contrast         estimate    SE df t.ratio p.value\n Deep - Medium      0.0127 0.121 50   0.105  1.0000\n Deep - Shallow     0.1758 0.113 50   1.558  0.3765\n Medium - Shallow   0.1631 0.111 50   1.473  0.4412\n\nP value adjustment: bonferroni method for 3 tests \n\n\nNotice that the p-values are tripled. In this case, we did not have evidence of differences between shallow, medium, and deep lakes even without using the Bonferroni correction, but the evidence grows even weaker after we apply the correction.\nThis finding is consistent with the F-test we performed on the entire model, which did not suggest evidence of differences in mercury levels between the three different depths of lakes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#confidence-and-prediction-intervals",
    "href": "Ch4.html#confidence-and-prediction-intervals",
    "title": "4  Inference from Models",
    "section": "4.4 Confidence and Prediction Intervals",
    "text": "4.4 Confidence and Prediction Intervals\n\n4.4.1 Intervals for Expected Response\nIn Chapter 3, we saw two different types of confidence intervals. One type was for regression parameters, \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\). For example, how much is a lake’s mercury level expected to change when its pH goes up by 1? The other type was for expected responses, which involved estimating linear functions of these parameters, for example \\(\\beta_0 + 7\\beta_1\\). For example, what is the expected mercury level among lakes with pH of 7?\nWe’ve seen how to calculate the first type of intervals using standard error formulas and the t-distribution. In this section, we’ll talk about the second kind of interval.\n\n\n4.4.2 Estimation and Prediction\nThink about the ice cream machine in Section 4.1. Suppose we did not know that the machine is manufactured to dispense 2 oz of ice cream per second, and had only the data on the 15 people, shown below.\n\n\n\n\n\n\n\n\n\nSuppose we are asked the following two questions:\n\nGive a range for the average amount of ice cream dispensed among people who hold the dispenser for 3 seconds.\n\nGive a range for the amount of ice cream an individual person should expect to get if they hold the dispenser for 3 seconds.\n\nTo answer the first question, we only need to estimate the location of the red line in the diagram below. Since we would only have sample data, the regression line estimated from our sample would not exactly match the “true” regression line relating time and amount dispensed, due to sampling variability. So we would need to include a margin of error accounting for uncertainty in the location of our estimated regression line.\nWhen answering the second question, we would need to account for variability between individual people, in addition to sampling variability. Even if we knew the exact location of the red line, we would need to account for the fact that some people would get more ice cream than expected, and others less, even if they hold the dispenser for the same amount of time. Thus, we will need to create a wider interval that is able to capture 95% of all individuals.\n\n\n\n\n\n\n\n\n\nAn interval for the expected response among all individuals with given value(s) of explanatory variable(s) is called a confidence interval.\nAn interval for an individual observation with given value(s) of explanatory variable(s) is called a prediction interval.\nA confidence interval only needs to account for uncertainty associated with sampling variability. A prediction interval needs to account for uncertainty associated with sampling variability and variability between individuals, hence it needs to be wider than a confidence interval.\n\n\n4.4.3 Calculating Confidence Intervals\nAn estimate for the expected among of ice cream when a person holds the dispenser for three seconds is \\(b_0 + 3b_1\\). In our model, we estimated \\(b_0\\) = -0.32 and \\(b_1\\) = 2.06, so the estimated amount of ice cream is \\[-0.32+ 3\\times2.06 = 5.87\\] oz.\nIn order to calculate a 95% confidence interval using the standard error formula in Section 4.2, we’ll need the following quantities:\nSSR:\n\nsum((IC_Model)$residuals^2) |&gt; round(2)\n\n[1] 10.66\n\n\n\\(\\bar{x}\\)\n\nmean(Icecream$time) |&gt; round(2)\n\n[1] 3.37\n\n\n\\(\\displaystyle\\sum(x_i-\\bar{x})^2\\)\n\nsum((Icecream1$time - mean(Icecream1$time))^2) |&gt; round(2)\n\n[1] 8.03\n\n\nWe begin by calculating the residual standard error \\(s\\).\n\\[\ns=\\sqrt\\frac{{SSR}}{n-(p+1)} = \\sqrt{\\frac{10.66}{15-(1+1)}} = 0.91\n\\]\nA 95% confidence interval for the average amount of ice cream dispensed only needs to account for the amount of uncertainty associated with using \\(b_0\\) and \\(b_1\\), which are calculated from our sample to estimate the true, typically unknown, regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\).\n95% confidence interval for the average amount of ice cream dispensed when the dispenser is held for three seconds is\n\\[\n\\begin{aligned}\n& b_0 + 3 b_1 \\pm 2 \\times \\text{SE}(b_0 + 3b_1) \\\\\n& = b_0 + 3 b_1 \\pm 2s\\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}}  \\\\\n& = -0.32 + 2.06 \\pm 20.91 \\sqrt{\\frac{1}{15}+ \\frac{(3-3.37)^2}{8.03}} \\\\\n& = (5.3322,  6.3878)\n\\end{aligned}\n\\]\nThis interval can be calculated using the predict command in R, entering the desired time in the newdata argument, and specifying interval=confidence.\n\npredict(IC_Model, newdata=data.frame(time=3), interval = \"confidence\", level=0.95)\n\n       fit      lwr      upr\n1 5.865178 5.298168 6.432188\n\n\nWe are 95% confident that the average amount of ice cream dispensed when the dispenser is held for 3 seconds is between 5.3 and 6.4 oz.\nNote there are small differences in the calculation, due to rounding and technicalities. R actually uses a number slightly different than 2 in its calculation, but the results are similar and these are all approximations anyway.\nThe plot shows 95% the estimated regression line from our sample (in blue), along with “true” regression line in read (which we wouldn’t know in a real situation). The shaded gray area represents a 95% confidence interval for the average amount of ice cream dispensed for a given amount of time the dispenser is held.\n\n\n\n\n\n\n\n\n\n\n\n4.4.4 Calculating Prediction Interval\nWhen calculating an interval for the amount of ice cream an individual person receives, we need to account for variability between individual people in addition to variability associated with using \\(b_0\\) and \\(b_1\\) to estimate \\(\\beta_0\\) and \\(\\beta_1\\).\nThe residual standard error \\(s\\) represents variability in ice cream amount between individuals who hold the dispenser for the same amount of time. So essentially, we need to add an extra \\(s\\) into our calculation to account for variability between individuals.\nA good first idea might be to add \\(s\\) to \\(\\text{SE}(b_0 + 3b_1)\\) when calculating the margin of error needed for the prediction interval. It turns out that probability theory tells us that adding standard deviations isn’t a good idea. But instead, we can add the variances, which are the squares of the standard deviations. So, we’ll add \\((\\text{SE}(b_0 + 3b_1))^2+s^2\\), when calculating the margin of error for the prediction interval, and then take the square root to get back to the scale of oz., rather than oz.\\(^2\\).\nThus, a 95% prediction interval for the amount of ice cream an individual person should expect to receive when holding the dispenser for 3 seconds is\n\\[\n\\begin{aligned}\n& b_0+3b_1 \\pm 2\\sqrt{(\\text{SE}(b_0 + 3b_1))^2 + s^2} \\\\\n& b_0 + 3b_1 \\pm 2 \\sqrt{s^2\\left(\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}\\right) + s^2} \\\\\n&  \\\\\n& b_0 + 3b_1 \\pm 2 s\\sqrt{\\left(\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}\\right) + 1} \\\\\n&  = -0.32 + 2.06 \\pm 20.91 \\sqrt{\\frac{1}{15}+ \\frac{(3-3.37)^2}{8.03}+1} \\\\\n& = (5.314,  7.68)\n\\end{aligned}\n\\]\nWhile the calculation itself might look intimidating, the key intuition here is to note the extra \\(s^2\\) being added under the square root, which accounts for the variability in amount dispensed between people who hold the dispenser for the same amount of time.\nTo perform the calculation in R, we simply change interval=\"confidence\" to interval=\"prediction\" in the predict command.\n\npredict(IC_Model, newdata=data.frame(time=3), interval = \"prediction\", level=0.95)\n\n       fit      lwr      upr\n1 5.865178 3.828578 7.901778\n\n\nWe are 95% confident that in individual who holds the dispenser for 3 seconds will get between 3.83 and 7.9 oz of ice cream.\nWe add the red line, displaying prediction intervals for the amount of ice cream an individual person is expected to receive for a given amount of time holding the dispenser. Notice that the prediction intervals are considerably wider than the confidence intervals for the average amount, due to variability between individuals.\n\n\n\n\n\n\n\n\n\n\n\n4.4.5 Mercury and pH in Florida Lakes\nWe’ll calculate a confidence interval for the average mercury content among all Florida lakes with pH of 7 and for the mercury content of an individual lake with pH of 7.\nThese could be done using the same formulas as above, but we’ll calculate them using the predict command in R.\nFirst, we fit the model in R.\n\nLakes_M_merc_pH &lt;- lm(data=FloridaLakes, Mercury~pH)\n\nConfidence Interval\n\npredict(Lakes_M_merc_pH, newdata=data.frame(pH=7), interval=\"confidence\", level=0.95)\n\n        fit       lwr       upr\n1 0.4648127 0.3832466 0.5463788\n\n\nWe are 95% confident that the average mercury level among all Florida lakes with pH of 7 is between 0.38 and 0.55 ppm.\nPrediction Interval\n\npredict(Lakes_M_merc_pH, newdata=data.frame(pH=7), interval=\"prediction\", level=0.95)\n\n        fit        lwr      upr\n1 0.4648127 -0.1064657 1.036091\n\n\nWe are 95% confident that the mercury level for an individual lake in Florida with pH of 7 is between -0.11 and 1.04 ppm.\nThis interval is very wide, and a bit nonsensical, since mercury content can’t be negative, but we can say that we are confident such a lake will have mercury content below 1.04 ppm.\n\ntemp_var &lt;- predict(M_Lakes_merc_pH, interval=\"prediction\")\nnew_df &lt;- cbind(FloridaLakes, temp_var)\nggplot(data=new_df, aes(x=pH, y=Mercury)) + geom_point() + labs(x=\"pH\",\n          y=\"Amount\") + stat_smooth(method=lm, se=TRUE) +\n    geom_line(aes(y=lwr), color = \"red\", linetype = \"dashed\") +\n    geom_line(aes(y=upr), color = \"red\", linetype = \"dashed\") + theme_bw() \n\n\n\n\n\n\n\n\n\n\n4.4.6 Mercury in Northern and Southern Lakes\nWe can also calculate confidence and prediction intervals when the explanatory variable is categorical. Here we’ll calcuate confidence intervals for the mean mercury content among all lakes in Northern and Southern Florida, as well as prediction intervals for individual lakes in these regions.\n\nM_Lakes_merc_loc &lt;- lm(data=FloridaLakes, Mercury ~ Location)\n\nConfidence Intervals\n\npredict(M_Lakes_merc_loc, newdata=data.frame(Location=c(\"N\", \"S\")), interval=\"confidence\", level=0.95)\n\n        fit       lwr       upr\n1 0.4245455 0.3137408 0.5353501\n2 0.6965000 0.5541689 0.8388311\n\n\nWe are 95% confident that the mean mercury level in North Florida is between 0.31 and 0.54 ppm.\n\nWe are 95% confident that the mean mercury level in South Florida is between 0.55 and 0.84 ppm.\n\nThese are confidence intervals for \\(\\beta_0\\), and \\(\\beta_0 + \\beta_1\\), respectively.\nPrediction Intervals\n\npredict(M_Lakes_merc_loc, newdata=data.frame(Location=c(\"N\", \"S\")), interval=\"prediction\", level=0.95)\n\n        fit         lwr      upr\n1 0.4245455 -0.22155101 1.070642\n2 0.6965000  0.04425685 1.348743\n\n\nWe are 95% confident that an individual lake in North Florida will have mercury level between 0 and 1.07 ppm.\nWe are 95% confident that an individual lake in South Florida will have mercury level between 0.04 and 1.35 ppm.\nConfidence intervals for the mean are shown in blue, and prediction intervals for individual lakes in red, for each part of the state.\n\ntemp_var &lt;- predict(M_Lakes_merc_loc, interval=\"prediction\")\nnew_df &lt;- cbind(FloridaLakes, temp_var)\nggplot(data=new_df, aes(x=Location, y=Mercury)) + geom_jitter(width=0.2) + labs(x=\"Location\",\n          y=\"Amount\") + stat_smooth(method=lm, se=TRUE)  +\n    geom_errorbar(aes(x=\"N\", ymin=0, ymax=1.07), color = \"red\") + theme_bw() + \n      geom_errorbar(aes(x=\"S\", ymin=0.04, ymax=1.35), color = \"red\") + theme_bw() +\n    geom_errorbar(aes(x=\"N\", ymin=0.31, ymax=0.54), color = \"blue\") + theme_bw() + \n      geom_errorbar(aes(x=\"S\", ymin=0.55, ymax=0.84), color = \"blue\") + geom_jitter(width=0.2) + labs(x=\"Location\",\n          y=\"Amount\") + stat_smooth(method=lm, se=TRUE)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#the-regression-effect",
    "href": "Ch4.html#the-regression-effect",
    "title": "4  Inference from Models",
    "section": "4.5 The Regression Effect",
    "text": "4.5 The Regression Effect\n\n4.5.1 The Regression Effect\nYou might be wondering how regression gets its name. It comes from a well known phenomenon, known as “regression to the mean”, or the “regression effect”. While the word “regression” is often construed with a negative context (i.e. getting worse), it could also refer to movement in the positive direction.\nThe scatterplot below shows student scores on two exams given in an introductory statistics class at another college. The data are part of the Lock5Data package.\n\n\n\n\n\n\n\n\n\nThe blue line represents the least squares regression line. The black line represents the line \\(y=x\\). A student who scored exactly the same on both exams would lie on the black line.\nWe do see a positive relationship between the exam scores, as we would expect. On average students who scored higher on the first exam also did on the second one. If we look deeper, however, we notice some interesting results.\n\nThere were 6 students who scored below a 70 on the first exam. Of these, 5 improved their score on the second exam (as indicated by the fact that they lie above the black line).\nThere were 7 students who scored a 90 or above on the first exam. Of these, only 2 improved on the second exam.\n\nAt first glance, a professor might think that the students who struggled on the first exam had begun studying harder, strengthened their understanding of the material, and thus improved on the second exam. Meanwhile, the professor might conclude that the students who had done well on the first exam had gotten lazy and not prepared or learned the material as well for the second exam. While these explanations may be true, there is another explanation we should consider here.\nWe can think of a student’s score on an exam as being a combination of their skill (or understanding) and luck. A certain with a strong understanding of the material will certainly be better positioned to score higher than a student with a weak understanding. Still, there are other factors that will affect an exam score, such as how the person is feeling on the day of the exam, or which questions are on the exam.\nA student who scores very high on the first exam likely knew the material well, and also experienced some good luck. Perhaps they got a good night’s sleep before the exam and the questions that were on the test were over the topics they knew best. If the student keeps studying and preparing just as well for the second exam, we would expect their skill level and understanding of the material to be just as good, but they may still see a slip in their score if their luck doesn’t hold up as well. Perhaps they aren’t feeling as well during the exam, or the professor happens to choose an exam question on their least favorite topic. Conversely, a student who scores very low on the first exam likely had a low understanding of the material and also experienced some bad luck. Unless they take action to improve their work in the class, it is unlikely that their level of understanding will be any better on the second exam, but their score may still improve if they get even average luck.\n\n\n4.5.2 Simulating Test Scores\nWe’ll conduct a simulation of the test scores under the assumption that a student’s score is a combination of skill and luck. We’ll assume there are 25 students in a class, and that the students’ levels of understanding are normally distributed with an average understanding equivalent to a score of 80 on the exam. We’ll assume that the standard deviation in understanding level between students is 10. We’ll also assume that each student’s score on the exam will deviate from their actual level of understanding in accordance with a normal distribution with standard deviation 5. (This means that most students will score within \\(\\pm10\\)) points of their true level of understanding. We’ll assume that all of the students have the same level of understanding for both exams (none get any stronger or weaker), but that their luck might differ between the exams.\nThe figure below shows each student’s simulated scores. The number on the diagram represents their actual level of understanding. For example, the top center student understood the material well enough to merit an 83 on both exams, but actually scored a 79 on the first exam and a 95 on the second exam, due to somewhat poor luck on the first, and very good luck on the second.\n\nset.seed(110322018)\nUnderstanding &lt;-rnorm(25, 80, 10)\nScore1 &lt;- Understanding + rnorm(25, 0, 5)\nScore2 &lt;- Understanding + rnorm(25, 0, 5)\nUnderstanding &lt;- round(Understanding,0)\nTestSim &lt;- data.frame(Understanding, Score1, Score2)\nggplot(data=TestSim, aes(y=Score2, x=Score1))+ geom_point() + stat_smooth(method=\"lm\", se=FALSE) +\n  geom_abline(slope=1) + geom_text(aes(label=Understanding), vjust = 0, nudge_y = 0.5)  + theme_bw()\n\n\n\n\n\n\n\n\nNotice that the behavior of the simulated scores mirrors those of the real ones we saw. Students who scored very low on the first exam tended to improve, and students who scored very high tended to drop off, even though this simulation was conducted under the assumption that their true level of understanding stayed the same!\nThe regression effect says that people who achieve very high performance in one event are likely to see a decrease in their next one, not because they got any worse, but because the excellent performance was probably a combination of both skill and good luck. Even if the skill persists, they are unlikely to experience the same good luck again, and thus are more likely to deteriorate or “regress” to their true ability level. Conversely, those who achieved very poor performance likely experience both poor skill and bad luck, and even if they don’t get any better, their performance is bound to improve based on better luck alone. Thus, a person who achieves very high or low performance is likely to “regress” toward their true ability level in the next attempt. This is known as the regression effect, or regression to the mean.\n\n\n4.5.3 NFL Wins\nThe graphic plots the number of games won by each team in the National Football League in 2021 and 2022.\n\n\n\n\n\n\n\n\n\nWe see that among the teams that won 5 or fewer games in 2021 4 of the 5 improved the following year. Meanwhile, among the 7 teams that won 10 or more, only 2 improved, while one stayed the same.\nWhile it may be the case that the weak teams got better and the good teams got worse, it is also possible that the teams that won fewer games in 2021 experienced some bad luck. Perhaps they had a lot of injuries, played difficult opponents, or had questionable calls that went against them. In that case, they may have won more games in 2022 simply because their luck wasn’t so bad. Likewise, the teams that did the best in 2021 may have been fortunate to stay healthy, play an easier schedule, or have calls go their way. They might see a decrease in wins the next year if their good luck reverts to being only average, even if they didn’t actually get any worse as a team. This is another example of the regression effect.\n\n\n4.5.4 Pilot Training\nA 1973 article by Kahneman, D. and Tversky, A., “On the Psychology of Prediction,” Pysch. Rev. 80:237-251 describes an instance of the regression effect in the training of air force pilots.\nTrainees were praised after performing well and criticized after performing badly. The flight instructors observed that “high praise for good execution of complex maneuvers typically results in a decrements of performance on the next try.”\nKahneman and Tversky write that :\n“We normally reinforce others when their behavior is good and punish them when their behavior is bad. By regression alone, therefore, they [the trainees] are most likely to improve after being punished and most likely to deteriorate after being rewarded. Consequently, we are exposed to a lifetime schedule in which we are most often rewarded for punishing others, and punished for rewarding.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#responsible-statistical-inference",
    "href": "Ch4.html#responsible-statistical-inference",
    "title": "4  Inference from Models",
    "section": "4.6 Responsible Statistical Inference",
    "text": "4.6 Responsible Statistical Inference\nWhile statistics offer insight that have lead to great advances in science, medicine, business and economics and many other areas, they have also been misused, (either intentionally or unintentionally) in ways that have resulted in harm. In an effort to better educate the public, the American Statistical Association released a report in 2016, highlighting common misuses of p-values, and the notion of statistical significance. Kenneth Rothman, Professor of Epidemiology at Boston University School of Public Health wrote:\n\n“(S)cientists have embraced and even avidly pursued meaningless differences solely because they are statistically significant, and have ignored important effects because they failed to pass the screen of statistical significance…It is a safe bet that people have suffered or died because scientists (and editors, regulators, journalists and others) have used significance tests to interpret results, and have consequently failed to identify the most beneficial courses of action.” -ASA statement on p-values, 2016\n\nIn this section, we’ll look at some real scenarios where it might be tempting to make an improper conclusion based on data. Think carefully about each scenario and identify possible misinterpretations that might arise. Then think about what conclusions or actions would be appropriate in each scenario.\nKeep in mind the following guidelines for responsible statistical inference.\nWhat a p-value tells us\nPerforming responsible statistical inference requires understanding what p-values do and do not tell us, and how they should and should not be interpreted.\n\nA low p-value tells us that the data we observed are inconsistent with our null hypothesis or some assumption we make in our model.\nA large p-value tells us that the data we observed could have plausibly been obtained under our supposed model and null hypothesis.\nA p-value never provides evidence supporting the null hypothesis, it only tells us the strength of evidence against it.\nA p-value is impacted by\n\nthe size of the difference between group, or change per unit increase (effect size)\n\nthe amount of variability in the data\n\nthe sample size\n\nSometimes, a p-value tells us more about sample size, than relationship we’re actually interested in.\nA p-value does not tell us the “size” of a difference or effect, or whether it is practically meaningful.\nWe should only generalize results from a hypothesis test performed on a sample to a larger population or process if the sample is representative of the population or process (such as when the sample is randomly selected).\nA correlation between two variables does not necessarily imply a causal relationship.\n\n\n4.6.1 Multiple Testing Error\nIn Section 4.3.4, we talked about the risk of detecting false positives when performing multiple hypothesis tests at the same time. We noted that this could be controlled using methods like the Bonferroni correction.\nAn obvious example of multiple testing error occurred in a 2008 article by New Scientist whose title made the surprising claim: “Breakfast cereals boost chances of conceiving boys.” Although the article lacks details on the underlying study, researchers, in fact, tracked the eating habits of women who were attempting to become pregnant. They tracked 133 different food items, and tested whether there was a difference in the proportion of baby boys conceived between women who ate the food, compared to those who didn’t. Of the 133 foods tested, only breakfast cereal showed a significant difference.\nIf none of the food items had any effect on the likelihood of a baby being born male, we should expect to obtain p-values less than 0.05 for \\(0.05 \\times 133 = 6.6\\) food items, just by chance. The probability of getting at least one p-value less than 0.05 is \\(1-0.95^{133} = 99.9\\%\\). The link between sex at birth and the mother’s consumption of breakfast cereal was almost certainly one of these false positives, resulting from performing 133 tests simultaneously. This xkcd comic? illustrates the faulty reasoning at play here.\nIt is important to use the Bonferroni correction, or some other kind of correction to control for false positives when performing multiple tests simultaneously.\n\n\n4.6.2 Flights from New York to Chicago\nA traveler lives in New York and wants to fly to Chicago. They consider flying out of two New York airports:\n\nNewark (EWR)\n\nLaGuardia (LGA)\n\nWe have data on the times of flights from both airports to Chicago’s O’Hare airport from 2013 (more than 14,000 flights).\nAssuming these flights represent a random sample of all flights from these airports to Chicago, consider how the traveler might use this information to decide which airport to fly out of.\n\nlibrary(nycflights13)\ndata(flights)\nflights$origin &lt;- as.factor(flights$origin)\nflights$dest &lt;- as.factor(flights$dest)\n\nWe’ll create a dataset containing only flights from Newark and Laguardia to O’Hare, and only the variables we’re interested in.\n\nFlights_NY_CHI &lt;- flights %&gt;% \n  filter(origin %in% c(\"EWR\", \"LGA\") & dest ==\"ORD\") %&gt;%\n  select(origin, dest, air_time)\n\nThe plot and table compare the duration of flights from New York to Chicago from each airport.\n\np1 &lt;- ggplot(data=Flights_NY_CHI, aes(x=air_time, fill=origin, color=origin)) + geom_density(alpha=0.2) + ggtitle(\"Flight Time\")  + theme_bw()\np2 &lt;- ggplot(data=Flights_NY_CHI, aes(x=air_time, y=origin)) + geom_boxplot() + ggtitle(\"Flight Time\")  + theme_bw()\ngrid.arrange(p1, p2, ncol=2) \n\n\n\n\n\n\n\n\n\nlibrary(knitr)\nT &lt;- Flights_NY_CHI %&gt;% group_by(origin) %&gt;% \n  summarize(Mean_Airtime = mean(air_time, na.rm=TRUE), \n            SD = sd(air_time, na.rm=TRUE), n=sum(!is.na(air_time)))\nkable(T)\n\n\n\n\norigin\nMean_Airtime\nSD\nn\n\n\n\n\nEWR\n113.2603\n9.987122\n5828\n\n\nLGA\n115.7998\n9.865270\n8507\n\n\n\n\n\nWe fit a model to test whether there is evidence of a difference in average flight time.\n\nM_Flights &lt;- lm(data=Flights_NY_CHI, air_time~origin)\nsummary(M_Flights)\n\n\nCall:\nlm(formula = air_time ~ origin, data = Flights_NY_CHI)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-26.26  -7.26  -1.26   5.20  84.74 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 113.2603     0.1299  872.06 &lt;0.0000000000000002 ***\noriginLGA     2.5395     0.1686   15.06 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.915 on 14333 degrees of freedom\n  (622 observations deleted due to missingness)\nMultiple R-squared:  0.01558,   Adjusted R-squared:  0.01551 \nF-statistic: 226.9 on 1 and 14333 DF,  p-value: &lt; 0.00000000000000022\n\n\nConfidence Interval for Flights:\n\nconfint(M_Flights)\n\n                2.5 %     97.5 %\n(Intercept) 113.00572 113.514871\noriginLGA     2.20905   2.869984\n\n\n\nFlights from LGA are estimated to take 2.5 minutes longer than flights from EWR on average.\nThe very low p-value provides strong evidence of a difference in mean flight time.\nWe are 95% confident that flights from LGA to ORD take between 2.2 and 2.9 minutes longer, on average, than flights from EWR to ORD.\n\nWe obtained a small p-value, and thus a statistically discernible difference in mean flight time to Chicago between the two airports. And yet, the difference is very small (between 2-3 minutes). Most travelers would not make their decision on this basis, and would likely prioritize other factors, such as price and convenience. In this case, the small p-value is due to the fact that we have a very large sample size, rather than a meaningful difference in outcomes.\n\n\n4.6.3 Smoking and Birthweight Example\nWe consider data on the relationship between a pregnant mother’s smoking and the birth weight of the baby. Data come from a sample of 80 babies born in North Carolina in 2004. Thirty of the mothers were smokers, and fifty were nonsmokers.\nThe plot and table show the distribution of birth weights among babies whose mothers smoked, compared to those who didn’t.\n\np1 &lt;- ggplot(data=NCBirths, aes(x=weight, fill=habit, color=habit)) + geom_density(alpha=0.2) + ggtitle(\"Birthweight and Smoking\")  + theme_bw()\np2 &lt;- ggplot(data=NCBirths, aes(x=weight, y=habit)) + geom_boxplot() + ggtitle(\"Birthweight and Smoking\") + theme_bw()\ngrid.arrange(p1, p2, ncol=2)  \n\n\n\n\n\n\n\n\n\nlibrary(knitr)\nT &lt;- NCBirths %&gt;% group_by(habit) %&gt;% summarize(Mean_Weight = mean(weight), SD = sd(weight), n=n())\nkable(T)\n\n\n\n\nhabit\nMean_Weight\nSD\nn\n\n\n\n\nnonsmoker\n7.039200\n1.709388\n50\n\n\nsmoker\n6.616333\n1.106418\n30\n\n\n\n\n\nWe fit a model and test for differences in average birth weight.\n\nM_Birthwt &lt;- lm(data=NCBirths, weight~habit)\nsummary(M_Birthwt)\n\n\nCall:\nlm(formula = weight ~ habit, data = NCBirths)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0392 -0.6763  0.2372  0.8280  2.4437 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)   7.0392     0.2140   32.89 &lt;0.0000000000000002 ***\nhabitsmoker  -0.4229     0.3495   -1.21                0.23    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.514 on 78 degrees of freedom\nMultiple R-squared:  0.01842,   Adjusted R-squared:  0.005834 \nF-statistic: 1.464 on 1 and 78 DF,  p-value: 0.23\n\n\n\nconfint(M_Birthwt)\n\n                2.5 %    97.5 %\n(Intercept)  6.613070 7.4653303\nhabitsmoker -1.118735 0.2730012\n\n\n\nThe average birth weight of babies whose mothers are smokers is estimated to be about 0.42 lbs less than the average birthweight for babies whose mothers are nonsmokers.\nThe large p-value of 0.23, tells us that there is not enough evidence to say that a mother’s smoking is associated with lower birth weights. It is plausible that this difference could have occurred by chance.\nWe are 95% confident that the average birtweight of babies whose mothers are smokers is between 1.12 lbs less and 0.27 lbs more than the average birthweight for babies whose mothers are nonsmokers.\n\nMany studies have shown that a mother’s smoking puts a baby at risk of low birthweight. Do our results contradict this research? Should we conclude that smoking has no impact on birthweights?\n\nLarger Study\nIn fact, this sample of 80 babies is part of a larger dataset, consisting of 1,000 babies born in NC in 2004. When we consider the full dataset, notice that the difference between the groups is similar, but the p-value is much smaller, providing stronger evidence of a relationship between a mother’s smoking and lower birthweight.\nWe’ll now fit a model to the larger dataset.\n\nM_Birthwt_Full &lt;- lm(data=ncbirths, weight~habit)\nsummary(M_Birthwt_Full)\n\n\nCall:\nlm(formula = weight ~ habit, data = ncbirths)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.1443 -0.7043  0.1657  0.9157  4.6057 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  7.14427    0.05086 140.472 &lt;0.0000000000000002 ***\nhabitsmoker -0.31554    0.14321  -2.203              0.0278 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.503 on 997 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.004846,  Adjusted R-squared:  0.003848 \nF-statistic: 4.855 on 1 and 997 DF,  p-value: 0.02779\n\n\nConfidence interval based on larger dataset:\n\nconfint(M_Birthwt_Full)\n\n                 2.5 %      97.5 %\n(Intercept)  7.0444697  7.24407557\nhabitsmoker -0.5965648 -0.03452013\n\n\nNotice that the estimated difference in birth weights is actually smaller in the full dataset than in the subset (though 0.3 lbs is still a pretty big difference for babies), and yet now the p-value is much smaller. Why do you think this happened? What should we learn from this?\nWe saw that a difference that was not statistically discernible for the sample of 80 babies became discernible when we had a larger sample. The large p-value from the initial study was due to a small sample size, rather than the lack of an effect of smoking. It wasn’t that smoking didn’t have an effect on birth weight, only that the sample was too small to detect it conclusively, given the amount of variability in the data. Had the researchers stopped and “accepted” the null hypothesis, concluding that smoking during pregnancy had no effect on the baby’s health, they would have been making a disasterous mistake!\n\n\n\n4.6.4 Cautions and Advice\np-values are only (a small) part of a statistical analysis.\n\nFor small samples, real differences might not be statistically significant.\n-Don’t accept null hypothesis. Gather more information.\n\nFor large, even very small differences will be statistically significant.\n-Look at confidence interval. Is difference practically important?\n\nWhen many hypotheses are tested at once (such as many food items) some will produce a significant result just by change.\n-Use a multiple testing correction, such as Bonferroni\n\nInterpret p-values on a “sliding scale”\n\n0.049 is practically the same as 0.051\n\nIs sample representative of larger population?\n\nWere treatments randomly assigned (for experiments)?\n\nAre there other variables to consider?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html#practice-questions",
    "href": "Ch4.html#practice-questions",
    "title": "4  Inference from Models",
    "section": "4.7 Practice Questions",
    "text": "4.7 Practice Questions\n\n1)\nFor each of the three plots, state whether it would be appropriate to use the ordinary linear regression model of the form\n\\[\ny = \\beta_0 + \\beta_1x + \\epsilon_i, \\text{ where } \\epsilon_i\\sim{\\text{N}}(0,\\sigma)\n\\]\nExplain why or why not. (Hint: Think about whether the assumptions of the ordinary regression model in Section 4.1.6 appear to be satisfied. )\n\n\n\n\n\n\n\n\n\n\n\n2)\nWe use the ordinary linear regression model\n\\[\ny = \\beta_0 + \\beta_1x + \\epsilon_i, \\text{ where } \\epsilon_i\\sim{\\text{N}}(0,\\sigma).\n\\]\nShown below are histograms showing the distribution of the explanatory variable \\(x\\), the response variable, \\(y\\), and a scatterplot displaying the relationship between \\(x\\) and \\(y\\).\n\nset.seed(10082020)\n# set times \nn &lt;- 60\nx &lt;- rexp(n, rate=1)\ny &lt;- x + rnorm(n, 0, 0.75)\n  \n  \ndf &lt;- data.frame(x, y) # set up data table\n\nP1 &lt;- ggplot(data=df, aes(x=x)) + geom_histogram()  + \n  xlim(c(0,5))  + ggtitle(\"A) Histogram of x\") + xlab(\"x\")  + \n  theme_bw()\n\nP2 &lt;- ggplot(data=df, aes(x=y)) + geom_histogram()  + \n  xlim(c(0,5))  + ggtitle(\"B) Histogram of y\") + xlab(\"y\")  + \n  theme_bw()\n\nP3 &lt;- ggplot(data=df, aes(x=x, y=y)) + geom_point()  + \n  xlim(c(0,5)) + ylim(c(0,5)) + ggtitle(\"C) Scatterplot of y and x\") + xlab(\"x\") + ylab(\"y\") + \n  theme_bw() + stat_smooth(method=\"lm\", se=FALSE)\n\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nBased on these graphs, should we be concerned about the normality assumption in the ordinary linear regression model? Why or why not?\n\n\n3)\nConsider the data shown in each of the four plots, and the ordinary linear regression model\n\\[\ny = \\beta_0 + \\beta_1x + \\epsilon_i, \\text{ where } \\epsilon_i\\sim{\\text{N}}(0,\\sigma).\n\\]\n\n\n\n\n\n\n\n\n\n\na) Order the plots from the one with the smallest value of \\(\\beta_1\\) to the largest.\n\n\nb) Order the plots from the one with the smallest value of \\(\\sigma\\) to the largest.\n\n\nc) Order the plots from the one with the smallest value of \\(R^2\\) to the largest.\n\n\n\n4)\nConsider the data shown in each of the four plots, and the ordinary linear regression model\n\\[\ny = \\beta_0 + \\beta_1\\text{GroupB}+ \\epsilon_i, \\text{ where } \\epsilon_i\\sim{\\text{N}}(0,\\sigma).\n\\]\n\n\n\n\n\n\n\n\n\n\na) Order the plots from the one with the smallest value of \\(\\beta_1\\) to the largest.\n\n\nb) Order the plots from the one with the smallest value of \\(\\sigma\\) to the largest.\n\n\nc) Order the plots from the one with the smallest F-statistic to the largest.\n\n\nFor Questions 5 and 6,recall that in Chapter 3 practice problems, we worked with a dataset pertaining to a sample of 200 high school seniors. Data were collected from student responses to the High School and Beyond Survey.We’ll continue to work with those data, this time using theory based on the ordinary regression model to perform inference.\n\n\n\n5)\nConsider a regression model of the form\n\\[\n\\text{Science Score} = \\beta_0 + \\beta_1\\times\\text{SchoolTypePrivate} + \\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\n\na)\nState the four assumptions of the ordinary regression model and explain what they mean in this context.\n\n\nb)\nThe model output is shown below.\n\nM_Seniors_science_schtype &lt;- lm(data=HSSeniors, science~schtyp)\n\n The table shows the mean and standard deviation in science scores for public and private schools, as well as SSR and SST for the model.\n\n\n\n\n\nschtyp\nMean_Science\nSD_Science\nN\n\n\n\n\npublic\n51.57\n10.19\n168\n\n\nprivate\n53.31\n8.19\n32\n\n\n\n\n\nSSR:\n\nsum(M_Seniors_science_schtype$residuals^2)\n\n[1] 19426.02\n\n\nSST:\n\nsum((HSSeniors$science-mean(HSSeniors$science))^2)\n\n[1] 19507.5\n\n\nCalculate the values that go in blanks (A)-(L) in the model output. Show your calculations. You may use the values in the tables and R output below.\n\n\nc)\nCalculate 95% confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\nd)\nGive an estimate of \\(\\sigma\\). Explain what this number represents in context. It might be helpful to phrase the statement in the form “this quantity represents the variability in ________ between __________.”\n\n\ne)\nExplain what the standard errors of \\(b_0\\) and \\(b_1\\) represent in context. It might be helpful to phrase these statements in the form “this quantity represents the variability in ________ between __________.”\n\n\nf)\nWrite the null hypothesis associated with the p-value on the line intercept. What (if anything) should we conclude from this p-value?\n\ng)\nWrite the null hypothesis associated with the p-value on the line schtypprivate. What (if anything) should we conclude from this p-value?\n\n\nh)\nWrite sentences interpreting each of the following intervals in context. If the command produces two intervals, interpret both of them.\n\n\n\n\nconfint(M_Seniors_science_schtype)\n\n                  2.5 %   97.5 %\n(Intercept)   50.064421 53.07844\nschtypprivate -2.026447  5.50859\n\n\n\n\n\n\npredict(M_Seniors_science_schtype, newdata=data.frame(schtyp=\"private\"), interval=\"confidence\")\n\n      fit      lwr      upr\n1 53.3125 49.85951 56.76549\n\n\n\n\n\n\npredict(M_Seniors_science_schtype, newdata=data.frame(schtyp=\"private\"), interval=\"prediction\")\n\n      fit      lwr      upr\n1 53.3125 33.47659 73.14841\n\n\n\n\n\n\n6)\nNow consider a model of the form:\n\\[\n\\text{Science Score} = \\beta_0 + \\beta_1\\times\\text{WritingScore} + \\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\n\na)\nState the four assumptions of the ordinary regression model and explain what they mean in this context.\n\n\nb)\nThe model output is shown below, along with some quantities that can be used to calculate the model estimates.\n\nM_Seniors_science_write &lt;- lm(data=HSSeniors, lm(data=HSSeniors, science~write))\n\n\n\n\nM2 Output\n\n\nSSR:\n\nsum(M_Seniors_science_write$residuals^2)\n\n[1] 13159.69\n\n\nSST:\n\nsum((HSSeniors$science-mean(HSSeniors$science))^2)\n\n[1] 19507.5\n\n\n\\(\\bar{x}\\):\n\nmean(HSSeniors$write)\n\n[1] 52.775\n\n\n\\(\\bar{y}\\):\n\nmean(HSSeniors$science)\n\n[1] 51.85\n\n\n\\(\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2\\):\n\nsum((HSSeniors$write-mean(HSSeniors$write))^2)\n\n[1] 17878.87\n\n\nCalculate the values that go in blanks (A)-(J) in the model output. Show your calculations. You may use the values in the tables and R output below.\n\n\nc)\nCalculate 95% confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\nd)\nGive an estimate of \\(\\sigma\\). Explain what this number represents in context. It might be helpful to phrase the statement in the form “this quantity represents the variability in ________ between __________.”\n\n\ne)\nExplain what the standard errors of \\(b_0\\) and \\(b_1\\) represent in context. It might be helpful to phrase these statements in the form “this quantity represents the variability in ________ between __________.”\n\n\nf)\nWrite the null hypothesis associated with the p-value on the line intercept. What (if anything) should we conclude from this p-value?\n\n\ng)\nWrite the null hypothesis associated with the p-value on the line wrire. What (if anything) should we conclude from this p-value?\n\nh)\nWrite sentences interpreting each of the following intervals in context. If the command produces two intervals, interpret both of them.\n\n\n\n\nconfint(M_Seniors_science_write)\n\n                 2.5 %     97.5 %\n(Intercept) 13.9572262 26.8500841\nwrite        0.4756219  0.7160918\n\n\n\n\n\n\npredict(M_Seniors_science_write, newdata=data.frame(write=60), interval=\"confidence\")\n\n       fit      lwr      upr\n1 56.15507 54.72435 57.58579\n\n\n\n\n\n\npredict(M_Seniors_science_write, newdata=data.frame(write=60), interval=\"prediction\")\n\n       fit      lwr      upr\n1 56.15507 40.01468 72.29545\n\n\n\n\n\n\n7)\n\nUsing the public/private schools data, suppose we randomly select a single public school student and find that they have a science score of 55. Should we conclude that this person is exceptionally strong at science, relative to other students? Why or why not?\nSuppose we take a random sample of 168 public school students and find that the average science score in the group was 55. Should we conclude that this group was exceptionally strong at science, relative to other groups of students? Why or why not?\n\n\n\n8)\nThe residual standard error \\(s\\) is different for the model in Question 1 than in Question 2. Suppose a person says “these are both meant to be the standard deviation in science test scores, so how can they be different?” How should you respond?\n\n\n9)\nSuppose the sample had consisted of 400 total students (instead of 200), of which 336 were from public schools and 64 from private schools (so the proportion from each school type is the same as in Question 1). Suppose the variability in the 400 students was similar to the variability in the original 200. For the quantities (A) - (L) in the Question 1 R Output, state whether each would be expected to increase, decrease, or stay about the same.\n\n\n10)\nThe following data and description come from the alr4 R package.\n“Karl Pearson organized the collection of data on over 1100 families in England in the period 1893 to 1898. This particular data set gives the Heights in inches of mothers and their daughters, with up to two daughters per mother. All daughters are at least age 18, and all mothers are younger than 65.”\nA scatterplot of the heights of the mothers and their daughters is shown below. The black line represents the line “y=x” and the blue line is the least squares regression line.\n\nlibrary(alr4)\ndata(Heights)\nggplot(data=Heights, aes(x=mheight, y=dheight)) + geom_point() + \n  stat_smooth(method=\"lm\", se=FALSE) + \n  geom_abline(slope=1, intercept=0) + theme_bw() + \n  xlab(\"Mother Height\") + ylab(\"Daughter Height\")\n\n\n\n\n\n\n\n\n\na)\nDescribe the strength and direction of the relationship between heights of mothers and their children? Approximately what is the value of \\(R^2\\).\n\n\nb)\nAmong mothers who are less than 60 inches tall, how do their daughters’ heights compare to the mothers’? What about for mothers who are more than 67 inches tall? Explain why this might be happening and give the statistical name for the behavior we see here.\n\n\nc)\nSuppose a mother is 60 inches tall. What is the predicted height of her daughter?\n\n\nd)\nSuppose a mother is 60 inches tall. Use the graph the approximate a 95% prediction interval for her daughter’s height.\n\n\n\n11)\nWhen a particular brain area becomes more active, it uses more oxygen. Functional Magnetic Resonance Imaging (fMRI) involves taking a scan of the brain to measure changes in blood flow to different regions. The fMRI machine can record the activity in a region of the brain, but it is subject to measurement error (noise).\nIn a 2009 study by Bennett et. al, an 18-inch, 3.8 lb. salmon was shown a series of photographs depicting human individuals in social situations with a specified emotional valence. Statistics measuring activity in voxels, tiny 3D regions of the brain, were calculated for both when the salmon was resting, and when it was viewing the photograph. t-tests were used on each voxel to test whether there was evidence of differences in brain activation when viewing the photograph, compared to at rest. Out of 8,064 voxels tested, 16 yielded p-values less than 0.001, thus showing evidence of differences in brain activity.\nThere was, however, a trick. The salmon was dead, and thus was experiencing no brain activity at all!\n\nEven if we hadn’t known that the salmon was dead, why shouldn’t we conclude that the salmon’s brain was responding differently when viewing the photo, compared to when at rest, based on these results?\nWhat could we do in this situation to reduce that risk of detecting a false positive when testing for change in a voxel of the brain?\n\n\n\n12)\nA 2015 study by Rohrer, Egloff, and Schmukle examined whether older siblings exhibited differences in intelligence and personality, compared to their younger siblings. The study used data from three large national panels from the United States (n = 5,240), Great Britain (n = 4,489), and Germany (n = 10,457).\nThe article states:\nThe question of whether a person’s position among siblings has a lasting impact on that person’s life course has fascinated both the scientific community and the general public for &gt;100 years. By combining large datasets from three national panels, we confirmed the effect that firstborns score higher on objectively measured intelligence and additionally found a similar effect on self-reported intellect. However, we found no birth-order effects on extraversion, emotional stability, agreeableness, conscientiousness, or imagination. This finding contradicts lay beliefs and prominent scientific theories alike and indicates that the development of personality is less determined by the role within the family of origin than previously thought.\n\nThe study tested six different outcomes (intelligence, extraversion, emotional stability, agreeableness, conscientiousness, and imagination). For each outcome, the null hypothesis is that there is no relationship between that outcome and the sibling’s birth order. Suppose that for each outcome there is a 0.05 probability of a hypothesis test resulting in a false positive (concluding an outcome is related to birth order when it really isn’t). If hypothesis tests are run on all six outcomes, what is the probability of getting at least one false positive? How can we ensure that the probability of getting one or more false positives when running six tests simultaneously is no higher than 0.05?\nTo test intellect, the authors used scores on Intelligence Quotient (IQ) tests. The mean score on IQ tests is 100, with a standard deviation of 15 between individual scores. The authors found that an older sibling scored, on average 1.5 points higher than the next younger sibling in the family. An ANOVA F-test with the null hypothesis that there is no relationship between birth order and intelligence resulted in an F-statistic of 11.80 and p-value of less than 0.001. Is the relationship between birth order and intelligence statistically discernible? Do you think it is practically meaningful? Explain your answer.\n\n\n\n13)\nDuchenne muscular dystrophy (DMD). DMD is a severe and progressive genetic disease that leads to muscle weakness, worsening over time. It is more common in boys than in girls. In 2016, a drug called Eteplirsen (now known as Exondys 51) was developed for the purpose of treating DMD symptoms and improving quality of life. Because the disease is rare, it is difficult to collect a large sample of participants for a randomized clinical trial to test the effectiveness of the drug. In a sample of 12 boys, 6 were assigned to a treatment group and took Eteplirsen for 62 weeks. The other 6 were assigned to a control group and took a placebo for 24 weeks, followed by Eteplirsen for 38 weeks. The boys were randomly assigned to groups. Researchers recorded the distance each boy walked in 6 minutes at the beginning and end of the study. Two of the boys in the placebo group withdrew from the study due to health complications. The average distance walked by the remaining four boys in the placebo group decreased by 63.8 m. over the 62 weeks, compared to an average decrease of only 4.2 meters for boys in the treatment group. A hypothesis test with the null hypothesis that there is no difference in distance walked between the groups returned a p-value of 0.012.\n\nIs the difference between the groups statistically discernible? Is it practically meaningful?\nThere was disagreement within the Federal Drug Administration about whether to to approve the Eteplirsen drug based on this study. Why might it be a good idea to approve the drug? What concerns might you have?\nSuppose the same difference in walking distances had been observed, but the p-value had been 0.072 instead of 0.012? Would this change your thoughts on whether the drug should be approved? Why or why not?\n\nNOTE: In the end, the FDA approved the drug, while requiring its producer to conduct additional future tests and noting that approval could be withdrawn if those tests did not confirm a clinical benefit. Results and discussion of the study are available here, and here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference from Models</span>"
    ]
  },
  {
    "objectID": "Ch5.html",
    "href": "Ch5.html",
    "title": "5  Building and Assessing Models",
    "section": "",
    "text": "Learning Outcomes\nConceptual Learning Outcomes\n19. State the assumptions of the normal error regression model and check their validity using residual plots, qq plots, and other information.\n20. Interpret regression coefficients and calculate predictions using models that involve log transformations.\n21. Interpret regression coefficient estimates and calculate predictions using models with interaction.\n22. Draw conclusions about models involving polynomial terms and interactions based on graphical representations of data.\n23. Determine whether it is appropriate to add terms to a model considering factors such as confounding variables, Simpson’s paradox, multicollinearity, \\(R^2\\), F-statistic, and residual plots.\nComputational Learning Outcomes\nI. Build regression models involving nonlinear terms and interactions in R.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Building and Assessing Models</span>"
    ]
  },
  {
    "objectID": "Ch5.html#regression-assumptions-checks",
    "href": "Ch5.html#regression-assumptions-checks",
    "title": "5  Building and Assessing Models",
    "section": "5.1 Regression Assumptions Checks",
    "text": "5.1 Regression Assumptions Checks\nWe’ve seen that tests and intervals based on the normal error regression model depend on four assumptions. If these assumptions are not reasonable then the tests and intervals may not be reliable.\nThe statement \\(Y_i = \\beta_0 + \\beta_1X_{i1}+ \\ldots + \\beta_pX_{ip} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\) implies the following:\n\nLinearity: the expected value of \\(Y\\) is a linear function of \\(X_1, X_2, \\ldots, X_p\\). (This assumption is only relevant for models including at least one quantitative explanatory variable.)\nNormality: Given the values of \\(X_1, X_2, \\ldots, X_p\\), \\(Y\\) follows a normal distribution.\nConstant Variance: Regardless of the values of \\(X_1, X_2, \\ldots, X_p\\), the variance (or standard deviation) in the normal distribution for \\(Y\\) is the same.\nIndependence: The response value for each observation is not affected by any of the other observations (expect due to explanatory variables included in the model).\n\nIllustration of Model Assumptions\n\n\n\n\n\n\n\n\n\nWe know that these assumptions held true in the ice cream example, because we generated the data in a way that was consistent with these.\nIn practice, we will have only the data, without knowing the exact mechanism that produced it. We should only rely on the t-distribution based p-values and confidence intervals in the R output if these appear to be reasonable assumptions.\nOf course, these assumptions will almost never be truly satisfied, but they should at least be a reasonable approximation if we are to draw meaningful conclusions.\n\n5.1.1 Residual Diagnostic Plots\nThe following plots are useful when assessing the appropriateness of the normal error regression model.\n\nScatterplot of residuals against predicted values\nHistogram of standardized residuals\n\nheavy skewness indicates a problem with normality assumption\n\nNormal quantile plot\n\nsevere departures from diagonal line indicate problem with normality assumption\n\n\nResidual vs Predicted Plots\nA residual vs predicted plot is useful for detecting issues with the linearity or constant variance assumption.\n\ncurvature indicates a problem with linearity assumption\n\n“funnel” or “megaphone” shape indicates problem with constant variance assumption\n\n\n\n\n\n\n\n\n\n\nIf there is only one explanatory variable, plotting the residuals against that variable reveals the same information as a residual vs predicted plot.\nHistogram of Residuals\nA histogram of the residuals is useful for assessing the normality assumption.\n\nSevere skewness indicates violation of normality assumption\n\n\n\n\n\n\n\n\n\n\nNormal Quantile-Quantile (QQ) Plot\nSometimes histograms can be inconclusive, especially when sample size is smaller.\nA Normal quantile-quantile plot displays quantiles of the residuals against the expected quantiles of a normal distribution.\n\nSevere departures from diagonal line indicate a problem with normality assumption.\n\n\n\n\n\n\n\n\n\n\nChecking Model Assumptions - Independence\nThe independence assumption is often difficult to assess through plots, because there are many different ways in which independence could be violation. One common type of independence violation is periodic (or seasonal) behavior. For example if a company’s sales peak in the same months every year, this would be seasonal behavior and would violate the independence assumption. If the data are listed in the order they are taken, we can plot residuals against index number (row of the dataset) and check whether there are any patterns.\n\n\n\n\n\n\n\n\n\nUnlike with the other assumptions, checking the residual vs. index plot does not catch all types of independence violations. First, it is only useful if the data are listed by date (or location) they occurred. The independence assumption can also be violated in other ways that a residual vs index plot will not catch.\nAnything that causes some observations to be more alike than others for reasons other than the explanatory variables in the model would cause a violation of the independence assumption.\nFor example:\n\nPeople in the study who are related.\n\nSome plants grown in the same greenhouse and others in different greenhouses.\n\nSome observations taken in same time period and others at different times.\n\nIt is important to use your knowledge about the data and how it was collected, in addition to plots when assessing the independence assumption.\nWhen the independence assumption is violated we need to use more advanced kinds of statistical models, beyond the ordinary regression model, in order to properly analyze the data.\n\nSummary of Checks for Model Assumptions\n\n\n\n\n\n\n\nModel assumption\nHow to detect violation\n\n\n\n\nLinearity\nCurvature in residual plot\n\n\nConstant Variance\nFunnel shape in residual plot\n\n\nNormality\nSkewness in histogram of residuals or departure from diag. line in QQ plot\n\n\nIndependence\nResidual vs index plot and info about data collection\n\n\n\n\n\n\n5.1.2 Example: N v S Lakes\nRecall our sample of 53 Florida Lakes, 33 in the north, and 20 in the south.\n\\(\\text{Mercury}_i = \\beta_0 + \\beta_1\\times{\\text{South}_i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\).\nWhen we use the normal error regression model, we are assuming the following:\n\nLinearity: there is an expected mercury concentration for lakes in North Florida, and another for lakes in South Florida.\nNormality: mercury concentrations of individual lakes in the north are normally distributed, and so are mercury concentrations in the south. These normal distributions might have different means.\nConstant Variance: the normal distribution for mercury concentrations in North Florida has the same standard deviation as the normal distribution for mercury concentrations in South Florida\nIndependence: no two lakes are any more alike than any others, except for being in the north or south, which we account for in the model. We might have concerns about this, do to some lakes being geographically closer to each other than others.\n\nWe should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable.\nThe resid_panel function in the ggResidpanel package produces the four plots shown above.\n\nresid_panel(model = M_Lakes_merc_loc)\n\n\n\n\n\n\n\n\nNotice that we see two lines of predicted values and residuals. This makes sense since all lakes in North Florida will have the same predicted value, as will all lakes in Southern Florida.\nThere appears to be a little more variability in residuals for Southern Florida (on the right), than Northern Florida, causing some concern about the constant variance assumption.\nOverall, though, the assumptions seem mostly reasonable.\nWe shouldn’t be concerned about using theory-based hypothesis tests or confidence intervals for the mean mercury level or difference in mean mercury levels. There might be some concern that prediction intervals could be either too wide or too narrow, but this is not a major concern, since the constant variance assumption is not severe.\n\n\n5.1.3 Example: pH Model\nRecall the regression line estimating the relationship between a lake’s mercury level and pH.\n\\(\\text{Mercury}_i = \\beta_0 + \\beta_1\\times\\text{pH}_i + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\).\nThe model assumes:\n\nLinearity: the expected mercury level of a lake is a linear function of pH.\nNormality: for any given pH, the mercury levels of lakes with that pH follow a normal distribution. For example, mercury levels for lakes with pH of 6 is are normally distributed, and mercury levels for lakes with pH of 9 are normally distributed, though these normal distributions may have different means.\nConstant Variance: the variance (or standard deviation) in the normal distribution for mercury level is the same for each pH. For example, there is the same amount of variability associated with lakes with pH level 6, as pH level 8.\nIndependence: no two lakes are any more alike than any others, except with respect to pH, which is accounted for in the model. This may not be a reasonable assumption, but it’s unclear what the effects of such a violation would be.\n\nWe should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable.\nThe plots for checking these assumptions are shown below.\n\nresid_panel(model = M_Lakes_merc_pH, smoother=TRUE)\n\n\n\n\n\n\n\n\nThe residual vs predicted plot does not show any linear trend, and variability appears to be about the same for low predicted values as for high ones. Thus, the linearity and constant variance assumptions appear reasonable.\nThe histogram shows some right-skewness, and the right-most points on the normal-qq plot are above the line, indicating a possible concern with the normality assumption. There is some evidence of right-skewness, which might impact the appropriateness of the normal error regression model.\nWe saw that the confidence interval and p-value associated with \\(b_1\\) when we used theory-based formulas was similar to those we obtained using simulation. Normality violations can, but don’t always have a heavy impact on intervals associated with model coefficients. They do, however, lead to unreliable intervals for an expected response (i.e. the average mercury level in lakes with a pH of 7), and cause prediction intervals to be especially unreliable.\n\n\n5.1.4 Example: House Prices\nRecall the model for estimating price of a house, using size, waterfront status, and an interaction term.\n\\(\\text{Price}_i = \\beta_0 + \\beta_1\\text{Sq.Ft.}_{i}+ \\beta_2\\text{Waterfront}_{i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\).\nThe model assumes:\n\nLinearity: The expected price of a house is a linear function of its size. Waterfront houses may be priced higher or lower than non-waterfront houses, but both types increase at the same rate with respect to size.\nNormality: Prices of houses of a given size and waterfront status are normally distributed.\nConstant Variance: The variance (or standard deviation) in the normal distribution for prices is the same for all sizes and waterfront statuses.\nIndependence: No two houses are any more alike than any others, except with respect to size and waterfront status.\n\nWe should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable.\nSeveral reasons come to mind that might cause us to doubt the validity of these assumptions, but let’s investigate them empirically, using our data on 200 houses in the dataset.\nThe plots for checking these assumptions are shown below.\n\nresid_panel(model = M_House_price_sqft_wf, smoother=TRUE)\n\n\n\n\n\n\n\n\nThe plots reveal several concerns. In the residual plot, there is some sign of a nonlinear trend, though this is not too severe. Of greater concern is the megaphone shape, as we see much more vertical spread in residuals for houses with higher predicted prices on the right side of the graph than for ones with lower predicted prices on the left.\nThe Q-Q plot also raises concerns about normality. There is a tail above the diagonal line on the right side of the plot, and a smaller one below the line on the left side. These indicate that the most expensive houses tend to be more expensive than we would expect using a normal model, and the least expensive houses tend to be less expensive than expected. It’s not unusual to have a few outliers deviate from the diagonal line on either end of the QQ-plot. The histogram of residuals does generally show a symmetric, bell-shaped, pattern and most of the points do follow the line closely so the normality concern may not be too serious.\nThe index plot does not raise any concerns, but we might still have some concerns about independence. For example, houses in the same neighborhood might be more similarly priced than houses in different neighborhoods.\n\n\n5.1.5 Impact of Model Assumption Violations\nIn this chapter, we’ve studied the normal error regression model and its underlying assumptions. We’ve seen that when these assumptions are realistic, we can use distributions derived from probability theory, such as t and F distributions to approximate sampling distributions, in place of the simulation-based methods seen in Chapters 3 and 4.\nOf course, real data don’t come exactly from processes like the fictional ice cream dispenser described in Section 4.1, so it’s really a question of whether this model is a realistic approximation (or simplification) of the true mechanism that led to the data we observe. We can use diagnostics like residual and Normal-QQ plots, as well as our intuition and background knowledge to assess whether the normal error regression model is a reasonable approximation.\nThe p-values provided by the lm summary output, and anova commands, and the and intervals produced by the confint, and predict command, as well as many other R commands, depend on the assumptions of the normal error regression model, and should only be used when these assumptions are reasonable.\nIn situations where some model assumptions appear to be violated, we might be okay using certain tests/intervals, but not others. In general, we should proceed with caution in these situations.\nThe table below provides guidance on the potential impact of model assumption violation on predicted values, confidence intervals, and prediction intervals.\n\n\n\n\n\n\n\n\n\nModel assumption Violated\nPredicted Values\nConfidence Intervals\nPrediction Intervals\n\n\n\n\nLinearity\nUnreliable\nUnreliable\nUnreliable\n\n\nConstant Variance\nReliable\nSomewhat unreliable - Some too wide, others too narrow\nVery unreliable - Some too wide, others too narrow\n\n\nNormality\nReliable\nPossibly unreliable - might be symmetric when they shouldn’t be. Might be okay when skewness isn’t bad and sample size is large.\nVery unreliable - will be symmetric when they shouldn’t be\n\n\nIndependence\nmight be reliable\nunreliable - either too wide or too narrow\nunreliable - either too wide or too narrow\n\n\n\nWhen model assumptions are a concern, consider a using a transformation of the data, a more advanced model, or a more flexible technique, such as a nonparametric approach or statistical machine learning algorithm.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Building and Assessing Models</span>"
    ]
  },
  {
    "objectID": "Ch5.html#transformations",
    "href": "Ch5.html#transformations",
    "title": "5  Building and Assessing Models",
    "section": "5.2 Transformations",
    "text": "5.2 Transformations\nWhen there are violations of model assumptions, we can sometimes correct for these by modeling a a function of the response variable, rather than the response variable itself. When the histogram of residuals and normal qq plot show signs of right-skewness, modeling a logarithm of the response variable is often helpful.\n\n5.2.1 Example: Modeling Car Prices\nWe’ll work with data on a set of 110 new cars, released in 2020. The data come from the LockData5 package in R.\n\nlibrary(Lock5Data)\ndata(Cars2020)\nCars2020 &lt;- Cars2020 |&gt; rename(Price = LowPrice) |&gt; \n  select(Make, Model, Price, Acc060, everything() ) |&gt; select(-HighPrice)\nhead(Cars2020)\n\n   Make Model Price Acc060   Type CityMPG HwyMPG Seating Drive Acc030 QtrMile\n1 Acura   MDX  44.4    6.8    SUV      14     31       7   AWD    2.8    15.3\n2 Acura   RLX  54.9    6.5  Sedan      15     36       5   AWD    2.7    15.0\n3  Audi    A3  33.3    8.3  Sedan      18     40       5   AWD    3.2    16.4\n4  Audi    A4  37.4    6.3 Sporty      18     40       5   AWD    2.7    14.9\n5  Audi    A6  54.9    6.8  Sedan      17     39       5   AWD    2.8    15.3\n6  Audi    A8  83.8    6.1  Wagon      20     27       5   AWD    2.4    14.5\n  Braking FuelCap Length Width Height Wheelbase UTurn Weight     Size\n1     135    19.5    196    77     67       111    40   4200 Midsized\n2     128    18.5    198    74     58       112    40   3930 Midsized\n3     124    13.2    175    70     56       104    37   3135    Small\n4     135    15.3    186    73     56       111    40   3630    Small\n5     129    19.3    195    74     57       115    38   4015 Midsized\n6     133    21.7    209    77     59       123    43   4810    Large\n\n\nWe’ll begin by examining the relationship between price (in thousands) and the amount of time it takes a car to accelerate from 0 to 60 mph (Acc060).\n\nggplot(data = Cars2020, aes(x=Acc060, y=Price)) + geom_point() + \n  geom_text(data = Cars2020 |&gt; filter(Price &gt; 80 | Acc060 &lt; 5), \n            aes(label=Model), nudge_y=3) + geom_text(data = Cars2020 |&gt;\n            filter(Model==\"Spark\"), aes(label=Model), nudge_y=3, nudge_x=-0.3) +\n            geom_text(data = Cars2020 |&gt; filter(Model==\"Mirage\"), aes(label=Model), \n            nudge_y=3, nudge_x=0.3) + stat_smooth(method=\"lm\", se=FALSE) + \n  theme_bw() \n\n\n\n\n\n\n\n\nWe’ll look at the assumptions associated with the model for predicting car price, using acceleration time as the explanatory variable.\nWe fit a model of the form\n\\[\n\\text{Price} = \\beta_0 +\\beta_1 \\times\\text{Acc060} + \\epsilon_i, \\text{where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nWe fit the model in R and display the summary below.\n\nM_Cars_price_acc060 &lt;- lm(data=Cars2020, Price~Acc060)\nsummary(M_Cars_price_acc060)\n\n\nCall:\nlm(formula = Price ~ Acc060, data = Cars2020)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.895  -7.410  -1.172   4.947  52.280 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  90.0583     6.2721  14.359 &lt; 0.0000000000000002 ***\nAcc060       -7.0826     0.7851  -9.021   0.0000000000000078 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.85 on 108 degrees of freedom\nMultiple R-squared:  0.4297,    Adjusted R-squared:  0.4244 \nF-statistic: 81.37 on 1 and 108 DF,  p-value: 0.000000000000007802\n\n\nThe most relevant estimate here is \\(b_1=\\)-7.08, which tells us that for each additional second it takes to accelerate from 0 to 60 mph, we estimate price of a car to decrease by about 7 thousand dollars. A confidence interval for this expected decrease is shown below.\n\nconfint(M_Cars_price_acc060, level=0.95, parm=\"Acc060\")\n\n           2.5 %    97.5 %\nAcc060 -8.638874 -5.526288\n\n\nBefore we draw conclusions from this model, however, we should check the assumptions associated with it. The assumptions are:\n\nLinearity: There is a linear relationship between the price of a car and the amount of time it takes to accelerate from 0 to 60 mph.\nNormality: For a given acceleration time, prices of cars are normally distributed.\nConstant Variance: The amount of variability in car prices is the same, for all acceleration times.\nIndependence: No two car prices are any more alike than any others for any reason other than acceleration time.\n\nDiagnostic plots are shown below.\n\nresid_panel(M_Cars_price_acc060, smoother=TRUE)\n\n\n\n\n\n\n\n\nThere is a funnel-shape in the residual plot, indicating a concern about the constant variance assumption. There appears to be more variability in prices for more expensive cars than for cheaper cars. There is also some concern about the normality assumption, as the histogram and QQ plot indicate slight right-skewness, though it is not too severe.\n\n\n5.2.2 Log Transformation\nWhen residual plots yield model inadequacy, we might try to correct these by applying a transformation to the response variable.\nWhen working a nonnegative, right-skewed response variable, it is often helpful to work with the logarithm of the response variable.\nNote: In R, log() denotes the natural (base e) logarithm, often denoted ln(). We can actually use any logarithm, but the natural logarithm is commonly used.\nThe log(Price) for the first 6 rows of the data are shown below.\n\nCars2020 &lt;- Cars2020 |&gt; mutate(LogPrice = log(Price)) \nCars2020 |&gt; select(Model, Make, Price, LogPrice, Acc060) |&gt; head()\n\n  Model  Make Price LogPrice Acc060\n1   MDX Acura  44.4 3.793239    6.8\n2   RLX Acura  54.9 4.005513    6.5\n3    A3  Audi  33.3 3.505557    8.3\n4    A4  Audi  37.4 3.621671    6.3\n5    A6  Audi  54.9 4.005513    6.8\n6    A8  Audi  83.8 4.428433    6.1\n\n\nWe’ll use the model:\n\\[\n\\text{Log Price} = \\beta_0 + \\beta_1\\times \\text{Acc060} + \\epsilon_i , \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nThe plot shows log(price) on the y-axis. We see that the relationship appears more linear than when we plot price itself.\n\nggplot(data=Cars2020, aes(x=Acc060, y=log(Price))) + geom_point() + \n  xlab(\"Acceleration Time\") + ylab(\"Log of Price\") + \n  ggtitle(\"Acceleration Time and Log Price\") + \n  stat_smooth(method=\"lm\", se=FALSE) + theme_bw()\n\n\n\n\n\n\n\n\nWe fit the model using log(Price) as the response variable.\n\nM_Cars_logprice_acc060 &lt;- lm(data=Cars2020, log(Price) ~ Acc060)\nsummary(M_Cars_logprice_acc060)\n\n\nCall:\nlm(formula = log(Price) ~ Acc060, data = Cars2020)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.90992 -0.19686  0.00148  0.15566  0.63774 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  5.00390    0.14547   34.40 &lt;0.0000000000000002 ***\nAcc060      -0.19889    0.01821  -10.92 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.298 on 108 degrees of freedom\nMultiple R-squared:  0.5248,    Adjusted R-squared:  0.5204 \nF-statistic: 119.3 on 1 and 108 DF,  p-value: &lt; 0.00000000000000022\n\n\nBefore interpreting the coefficients, we’ll check the residual plots for the log model.\nAssumption Check for Model on Log Price\n\nresid_panel(M_Cars_logprice_acc060, smoother=TRUE)\n\n\n\n\n\n\n\n\nThere is still some concern about constant variance, though perhaps not as much as before. The normality assumption appears more reasonable.\nWe’ll proceed, noting that the slight concern about constant variance might raise questions about confidence intervals for an expected response, and especially prediction intervals.\n\n\n5.2.3 Inference for Log Model\nThe estimated regression equation is:\n\\[\n\\begin{aligned}\n\\widehat{\\text{LogPrice}} & = 5.0039 + -0.1989 \\times \\text{Acc060}\n\\end{aligned}\n\\] Thus,\n\\[\n\\begin{aligned}\n\\widehat{\\text{Price}} & = e^{5.0039 + -0.1989 \\times \\text{Acc060}}\n\\end{aligned}\n\\]\nPredicted price for car that takes 7 seconds to accelerate:\n\\[\n\\begin{aligned}\n\\widehat{\\text{Price}} & = e^{5.0039 + -0.1989 \\times 7}\n\\end{aligned}\n\\]\nPredicted price for car that takes 10 seconds to accelerate:\n\\[\n\\begin{aligned}\n\\widehat{\\text{Price}} & = e^{5.0039 + -0.1989 \\times 10}\n\\end{aligned}\n\\]\nWhen using the predict command, R gives predictions are for log(Price), so we need to exponentiate.\n\n#predicted log of price\npredict(M_Cars_logprice_acc060, newdata=data.frame(Acc060=c(7)))\n\n      1 \n3.61169 \n\n\n\nexp(predict(M_Cars_logprice_acc060, newdata=data.frame(Acc060=c(7))))\n\n       1 \n37.02859 \n\n\nA car that accelerates from 0 to 60 mph in 7 seconds is expected to cost about 37 thousand dollars.\n\n\n5.2.4 Log Model Interpretations\nWhen we use a model with a log transformation of the response variable, our interpretations of the response variable change. Let’s look at some algebra that will help us see how to interpret the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in this situation.\n\\[\n\\begin{aligned}\n\\text{Log of Expected Price} & = \\beta_0 + \\beta_1\\times \\text{Acc060}\\  \\text{, Thus:} \\\\\n\\text{ Expected Price} & = e^{\\beta_0 + \\beta_1\\times \\text{Acc060} } \\\\\n& e^{\\beta_0}e^{\\beta_1 \\times \\text{Acc060}} \\\\\n& e^{\\beta_0}(e^{\\beta_1})^\\text{Acc060}\n\\end{aligned}\n\\]\n\nWe see that when \\(\\text{Acc060}=0\\), the expected price is \\(e^{\\beta_0}\\). So, \\(e^{\\beta_0}\\) is theoretically the expected price of a car that can accelerate from 0 to 60 mph in no time, but this is not a meaningful interpretation.\nWe see that for each additional second it takes to accelerate from 0 to 60 mph, the exponent on \\(e^{\\beta_1}\\), increases by one, meaning price is expected to multiply by an additional \\(e^{\\beta_1}\\). For each additional second it takes a car to accelerate, price is expected to multiply by a factor of \\(e^{b_1}\\).\n\nWe calculate \\(e^{b_0}\\) and \\(e^{b_1}\\) in R, and interpret the estimates.\n\nexp(M_Cars_logprice_acc060$coefficients)\n\n(Intercept)      Acc060 \n148.9925267   0.8196429 \n\n\n\nFor each additional second in acceleration time, price is expected to multiply by a a factor of \\(e^{-0.1989}\\) = 0.82. Thus, each 1-second increase in acceleration time is estimated to be associated with a 18% drop in price, on average.\n\nConfidence Interval for \\(e^{\\beta_1}\\)\n\nexp(confint(M_Cars_logprice_acc060, level=0.95, parm=\"Acc060\"))\n\n           2.5 %    97.5 %\nAcc060 0.7905858 0.8497679\n\n\n\nWe are 95% confident that for each additional second in acceleration time, the price of a car multiplies by a factor between 0.79 and 0.85, an estimated decrease between 15 and 21 percent.\n\nLog Model CI for Expected Response\nIf we just use the predict function, we get a confidence interval for log(price).\n\npredict(M_Cars_logprice_acc060, newdata=data.frame(Acc060=c(7)), interval=\"confidence\")\n\n      fit      lwr      upr\n1 3.61169 3.547818 3.675563\n\n\nTo get an interval for price itself, we exponentiate, using exp.\n\nexp(predict(M_Cars_logprice_acc060, newdata=data.frame(Acc060=c(7)), interval=\"confidence\"))\n\n       fit      lwr      upr\n1 37.02859 34.73743 39.47087\n\n\nWe are 95% confident that the mean price amoung all cars that accelerate from 0 to 60 mph in 7 seconds is between \\(e^{3.55\n} =\\) 34.7 and \\(e^{3.68}=\\) 39.5 thousand dollars.\nLog Model Prediction Interval\n\npredict(M_Cars_logprice_acc060, newdata=data.frame(Acc060=c(7)), interval=\"prediction\")\n\n      fit      lwr      upr\n1 3.61169 3.017521 4.205859\n\n\n\nexp(predict(M_Cars_logprice_acc060, newdata=data.frame(Acc060=c(7)), interval=\"prediction\"))\n\n       fit      lwr      upr\n1 37.02859 20.44056 67.07822\n\n\nWe are 95% confident that the price for an individual car that accelerates from 0 to 60 mph in 7 seconds is between \\(e^{3.02\n} =\\) 20.4 and \\(e^{4.21}=\\) 67.1 thousand dollars.\n\n\n5.2.5 Model Comparisons\nWe’ll compare the intervals we obtain using the log transformation to those from the model without the transformation.\n95% Confidence interval for average price of cars that take 7 seconds to accelerate:\nOriginal Model:\n\npredict(M_Cars_price_acc060, newdata=data.frame(Acc060=7), interval=\"confidence\", level=0.95)\n\n       fit      lwr      upr\n1 40.48028 37.72628 43.23428\n\n\nTransformed Model:\n\nexp(predict(M_Cars_logprice_acc060, newdata=data.frame(Acc060=c(7)), interval=\"confidence\", level=0.95))\n\n       fit      lwr      upr\n1 37.02859 34.73743 39.47087\n\n\n95% Prediction interval for price of an individual car that takes 7 seconds to accelerate:\nOriginal Model:\n\npredict(M_Cars_price_acc060, newdata=data.frame(Acc060=7), interval=\"prediction\", level=0.95)\n\n       fit     lwr      upr\n1 40.48028 14.8614 66.09916\n\n\nTransformed Model:\n\nexp(predict(M_Cars_logprice_acc060, newdata=data.frame(Acc060=c(7)), interval=\"prediction\", level=0.95))\n\n       fit      lwr      upr\n1 37.02859 20.44056 67.07822\n\n\nNotice that the transformed interval is not symmetric and allows for a longer “tail” on the right than the left.\n\n\n5.2.6 Log Model Visualization\n\n\n\n\n\n\n\n\n\nThe log model suggests an nonlinear trend in price with respect to acceleration time and gives wider confidence and prediction intervals for cars that accelerate faster and tend to be more expensive. It also gives non-symmetric intervals. These results appear to be consistent with the observed data.\n\n\n5.2.7 Comments on Transformations\nViolations of model assumptions often come from right skewness, which is especially common in economic and financial data. Log transformations are often helpful in fitting models to such data. Log models are also easily interpretable, allowing us to interpret estimates as percent change.\nWe could have used another transformation, such as \\(\\sqrt{\\text{Price}}\\), and in some cases, other transformations might better correct for violations of model assumptions. That said, other transformations are harder to interpret than the log transformation. It is important to think about both the appropriateness of our model assumptions as well as how easily we will be able to interpret and explain our conclusions when choosing a transformation.\nIn this section, we looked at a transformation involving a single quantitative explanatory variable.\nIf the explanatory variable is categorical:\n\n\n\\(e^{\\beta_0}\\) represents the expected response in the baseline category\n\n\\(e^{\\beta_j}\\) represents the number of times larger the expected response in category \\(j\\) is, compared to the baseline category.\n\nWhen working with multiple regression models, it is still important to mention holding other variables constant when interpreting parameters associated with one of the variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Building and Assessing Models</span>"
    ]
  },
  {
    "objectID": "Ch5.html#polynomial-regression",
    "href": "Ch5.html#polynomial-regression",
    "title": "5  Building and Assessing Models",
    "section": "5.3 Polynomial Regression",
    "text": "5.3 Polynomial Regression\n\n5.3.1 Modeling Wages\nIn this section, we’ll work with a dataset containing information on a sample of 250 male employees living in the Mid-Atlantic region. These data are a subset of a larger dataset available in the ISLR R package.\nData were collected in the years 2003 through 2009. We’ll fit a model to predict the employee’s salary using the employee’s age and education level, the year the data was collected, and whether the job was industrial or informational in nature as explanatory variables.\nThe plots below show the relationship between wage and each of the four explanatory variables.\n\nP1 &lt;- ggplot(data=WageSample, aes(y = wage, x = year)) + geom_point() + theme_bw()\nP2 &lt;- ggplot(data=WageSample, aes(y = wage, x = age)) + geom_point() + theme_bw()\nP3 &lt;- ggplot(data=WageSample, aes(y = wage, x = education)) + geom_boxplot() + theme_bw()  +  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) \nP4 &lt;- ggplot(data=WageSample, aes(y = wage, x = jobclass)) + geom_boxplot() + theme_bw()  +  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) \ngrid.arrange(P1, P2, P3, P4, ncol=2)\n\n\n\n\n\n\n\n\nThe most obvious relationship appears to be that people with higher levels of education tend to make more money. We’ll use a statistical model to investigate this and other relationships.\nWe first fit a model for price, using each of the explanatory variables shown above.\nThe model equation is\n\\[\n\\begin{aligned}\n\\text{Salary}_i & = \\beta_0 + \\beta_1\\times\\text{Year}_i + \\beta_2\\times\\text{Age}_i   \\\\\n& + \\beta_3\\times\\text{HSGrad}_i + \\beta_4\\times\\text{SomeColl}_i + \\beta_5\\times\\text{CollegeGrad}_i   \\\\\n& + \\beta_6\\times\\text{AdvDeg}_i + \\beta_7\\times\\text{JobInf}_i   \\\\\n& + \\epsilon_i  \\\\\n\\text{where} \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\end{aligned}\n\\]\nWe fit the model in R.\n\nM_Wage_price_yr_age_ed_job &lt;- lm(data=WageSample, wage ~ year + age + education + jobclass)\n\nBefore we examine model output, let’s check the model assumptions using residual plots.\n\nresid_panel(M_Wage_price_yr_age_ed_job, smoother=TRUE)\n\n\n\n\n\n\n\n\nWe notice some right-skewness and large outliers in the qq-plot and histogram of residuals.\nLet’s model log(wage) to try to correct for the right skewness.\n\nM_logWage_price_yr_age_ed_job &lt;- lm(data=WageSample, log(wage) ~ year + age + \n                                                          education + jobclass)\n\nWe check the residual plots on the model for log(wage).\n\nresid_panel(M_logWage_price_yr_age_ed_job, smoother=TRUE)\n\n\n\n\n\n\n\n\nAlthough not perfect, these look better. There are only a few points out of line at each end of the qq-plot, and the histogram of residuals looks pretty symmetric.\n\n\n5.3.2 Residual by Predictor Plots\nWhen working with multiple explanatory variables, we should not only plot residuals vs predicted values, but also vs all explanatory variables. This will help us see whether there are any explanatory variables whose relationship could be better explained in the model.\nThe resid_xpanel() function in the ggResidplot package creates these plots.\n\nresid_xpanel(M_logWage_price_yr_age_ed_job, smoother=TRUE)\n\n\n\n\n\n\n\n\nThe plots for year, education, and jobclass don’t raise any concerns, but we notice a quadratic trend in the residuals for age.\n\n\n5.3.3 Add Quadratic Term\nWhen only one or some variables show a violation, we can improve the model using a transformation on just that variable. Since we see a quadratic trend, we can fit a quadratic term for the age variable. To do this, we add I(age^2) to the model. We still leave the linear term for age in the model as well.\nThe model equation is\n\\[\n\\begin{aligned}\n\\text{log(Salary)}_i & = \\beta_0 + \\beta_1\\times\\text{Year}_i + \\beta_2\\times\\text{Age}_i  \\beta_2\\times\\text{Age}^2_i + \\\\\n& + \\beta_3\\times\\text{HSGrad}_i + \\beta_4\\times\\text{SomeColl}_i + \\beta_5\\times\\text{CollegeGrad}_i  \\\\\n& + \\beta_6\\times\\text{AdvDeg}_i + \\beta_7\\times\\text{JobInf}_i  \\\\\n& + \\epsilon_i,  \\\\\n\\text{where} \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\end{aligned}\n\\]\n\nM_logWage_price_yr_age2_ed_job &lt;- lm(data=WageSample, \n                                     log(wage) ~ year + age + I(age^2) + \n                                                education + jobclass)\n\nWe again check the residual plots.\n\nresid_panel(M_logWage_price_yr_age2_ed_job, smoother=TRUE)\n\n\n\n\n\n\n\n\nWe also check residual by predictor plots.\n\nresid_xpanel(M_logWage_price_yr_age_ed_job, smoother=TRUE)\n\n\n\n\n\n\n\n\nNotice that the quadratic trend in residuals has gone away in the age plot, telling us that the model with age^2 has properly accounted for the quadratic trend.\n\n\n5.3.4 Model Output\nNow that we’ve found a model that appears to reasonably satisfy model assumptions, let’s look at the model summary output.\n\nsummary(M_logWage_price_yr_age_ed_job)\n\n\nCall:\nlm(formula = log(wage) ~ year + age + education + jobclass, data = WageSample)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.29905 -0.14183  0.00267  0.16571  1.19099 \n\nCoefficients:\n                              Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)                 -23.203853  21.150224  -1.097        0.273689    \nyear                          0.013615   0.010551   1.290        0.198117    \nage                           0.005593   0.001758   3.182        0.001655 ** \neducation2. HS Grad           0.186614   0.079941   2.334        0.020395 *  \neducation3. Some College      0.296369   0.079602   3.723        0.000245 ***\neducation4. College Grad      0.422839   0.083145   5.086 0.0000007346731 ***\neducation5. Advanced Degree   0.603682   0.087811   6.875 0.0000000000524 ***\njobclass2. Information        0.039636   0.043484   0.912        0.362937    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3319 on 242 degrees of freedom\nMultiple R-squared:  0.2776,    Adjusted R-squared:  0.2567 \nF-statistic: 13.28 on 7 and 242 DF,  p-value: 0.00000000000001769\n\n\nSince we modeled log(wage), we’ll exponentiate the model coefficients to make our interpretations.\n\nexp(M_logWage_price_yr_age_ed_job$coefficients) |&gt;round(3)\n\n                (Intercept)                        year \n                      0.000                       1.014 \n                        age         education2. HS Grad \n                      1.006                       1.205 \n   education3. Some College    education4. College Grad \n                      1.345                       1.526 \neducation5. Advanced Degree      jobclass2. Information \n                      1.829                       1.040 \n\n\nInterpretations\nSince the intercept would refer to things like year 0 and a person of age 0, it does not make sense to interpret. The interpretation of the age coefficient gets tricky since it involves a quadratic term, so we’ll interpret the other variables first.\n\nFor each year since 1993, average salary is expected to multiply by 1.014, a 1.4 percent increase, assuming all other variables are held constant. Since the p-value on year is large, we cannot conclusively say that salaries are changing over time.\nWorkers with a high school degree are estimated to make 34.5 percent more than workers with less than a high school degree, assuming all other variables are held constant.\nWorkers with some college are estimated to make 52.6 percent more than workers with less than a high school degree, assuming all other variables are held constant.\nWorkers with a college degree are estimated to make 82.9 percent more than workers with less than a high school degree, assuming all other variables are held constant.\nWorkers with a college degree are estimated to make 4 percent more than workers with less than a high school degree, assuming all other variables are held constant.\n\nThe differences in education levels are all statistically discernible.\n\nWorkers with an informational job are estimated to make NA percent more than workers with an industrial one, assuming all other variables are held constant. This difference is not big enough to be statistically discernible.\n\n\n\n5.3.5 Interpreting the Quadratic Term\nFirst, notice that the p-value on the I(age^2) term is small. This tells us that there is evidence of a quadratic relationship between age and salary, so adding the age^2 term was a good idea.\nWhen we have a quadratic term in the model, we can’t make interpretations on the age or age^2 estimates individually, since we can’t change one while holding the other constant. Instead, we’ll look at the two estimates together.\nFocusing on only the terms relating to age, we get the expression\n\\[\n-0.00033 \\times \\text{Age}^2 + 0.03392 \\times \\text{Age}\n\\]\nWe see that this expression has a negative quadratic coefficient, meaning it will have the shape of a downward facing parabola. A sketch of the equation \\[y=0.18661\\times \\text{Age}^2 +0.00559 \\times \\text{Age}\\] is shown below.\n\na &lt;- summary(M_logWage_price_yr_age2_ed_job)$coefficients[4]\nb &lt;- summary(M_logWage_price_yr_age2_ed_job)$coefficients[3] \n\nggplot(data.frame(x = seq(from=18, to=80, by=1)), aes(x)) +\n   geom_function(fun = function(x){a*x^2+b*x}, colour = \"red\") + theme_bw() + \n  theme(axis.line=element_blank(),\n          axis.text.y=element_blank(),\n          axis.title.y=element_blank())\n\n\n\n\n\n\n\n\nWe see that the parabola appears to hit its max between 50 and 60 years, suggesting that on average workers wages increase until this age, and then start to decrease. This makes sense, as workers will often see pay raises with experience and promotions for most of their career, until some begin to retire or take less intensive jobs in their 50’s and 60’s.\nUsing a fact from high school algebra, a quadratic function \\(f(x) = ax^2 + bx + c\\) attains it’s max or min at \\(x=-b/(2a)\\), so in this case, the model estimates that salaries are at their highest, on average, at the age of\n\\[\n- 0.03392 / (2\\times -0.00033) = 51.9\n\\]\nyears.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Building and Assessing Models</span>"
    ]
  },
  {
    "objectID": "Ch5.html#models-with-interaction",
    "href": "Ch5.html#models-with-interaction",
    "title": "5  Building and Assessing Models",
    "section": "5.4 Models with Interaction",
    "text": "5.4 Models with Interaction\n\n5.4.1 Definition of Interaction\nIn Chapter 2, we modeled the price of a house in King County, Washington, using its size in square feet, and whether or not it was on the waterfront as explanatory variable. We used a multiple regression model of the form\n\\[\n\\widehat{Price} = b_0 + b_1\\times\\text{SqFt} + b_2\\times\\text{Waterfront}\n\\]\nRecall that this model assumes the slope relating price and square footage is the same (\\(b_1\\)) for houses on the waterfront as for houses not on the waterfront. An illustration of the model is shown below.\n\nPlot_House_price_sqft_wf\n\n\n\n\n\n\n\n\nThis assumption of the rate of change in price with respect to living space being the same for waterfront houses, as for non-waterfront houses might be unrealistic.\nLet’s fit separate lines for waterfront and non-waterfront houses, without requiring them to have the same slope.\n\nggplot(data=Houses, aes(x=sqft_living, y=price, color=waterfront)) + geom_point()+stat_smooth(method=\"lm\", se=FALSE) + theme_bw()\n\n\n\n\n\n\n\n\nIt appears that the prices of the houses on the waterfront are increasing more rapidly, with respect to square feet of living space, than the non-waterfront houses. The effect of additional square feet on the price of the house appears to depend on whether or not the house is on the waterfront. This is an example of an interaction between square footage and waterfront status.\nAn interaction between two explanatory variables occurs when the effect of one explanatory variable on the response depends on the other explanatory variable.\n\n\n5.4.2 Interaction Model Equations\nIf we want to allow for different slopes between waterfront and non-waterfront houses, we’ll need to change the mathematical equation of our model. To do that, we’ll add a coefficient \\(b_3\\), multiplied by the product of our two explanatory variables.\nThe model equation is\n\\[\n\\widehat{Price} = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{waterfront} + b_3\\times\\text{Sq.Ft}\\times\\text{Waterfront}\n\\]\nThe last term is called an interaction term.\nFor a house on the waterfront (\\(\\text{waterfront}=1\\)), the equation relating price to square feet is\n\\[\n\\begin{aligned}\n\\widehat{Price} & = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{1} + b_3\\times\\text{Sq.Ft}\\times\\text{1} \\\\\n& = (b_0+b_2) + (b_1+b_3)\\times{\\text{Sq. Ft.}}\n\\end{aligned}\n\\] For a house not on the waterfront (\\(\\text{waterfront}=0\\)), the equation relating price to square feet is\n\\[\n\\begin{aligned}\n\\widehat{Price} & = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{0} + b_3\\times\\text{Sq.Ft}\\times\\text{0} \\\\\n& = b_0 + b_1\\times{\\text{Sq. Ft}}\n\\end{aligned}\n\\]\nThe intercept is \\(b_0\\) for non-waterfront houses, and \\(b_0 + b_2\\) for waterfront houses.\nThe slope is \\(b_1\\) for non-waterfront houses, and \\(b_1 + b_3\\) for waterfront houses.\nThus, the model allows both the slope and intercept to differ between waterfront and non-waterfront houses.\n\n\n5.4.3 Interaction Models in R\nTo fit a model with variables var1 and var2 and their interaction, you can do either of the following.\n\ny ~ var1 + var2 + var1:var2 OR\ny ~ var1*var2\n\nvar1:var2 means include the interaction term, and var1*var2 means include both variables plus the interaction. (Note: we should never include an interaction term without also including the variables individually.)\n\nM_House_price_sqft_wf_int &lt;- lm(data=Houses, price~sqft_living*waterfront)\nsummary(M_House_price_sqft_wf_int)\n\n\nCall:\nlm(formula = price ~ sqft_living * waterfront, data = Houses)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1474.37  -151.07   -31.29   106.77  1331.87 \n\nCoefficients:\n                            Estimate Std. Error t value             Pr(&gt;|t|)\n(Intercept)               -114.37347   59.00418  -1.938                0.054\nsqft_living                  0.30987    0.02496  12.417 &lt; 0.0000000000000002\nwaterfrontYes              -90.11528  168.67011  -0.534                0.594\nsqft_living:waterfrontYes    0.27292    0.04856   5.620          0.000000065\n                             \n(Intercept)               .  \nsqft_living               ***\nwaterfrontYes                \nsqft_living:waterfrontYes ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 322.8 on 196 degrees of freedom\nMultiple R-squared:  0.7771,    Adjusted R-squared:  0.7737 \nF-statistic: 227.7 on 3 and 196 DF,  p-value: &lt; 0.00000000000000022\n\n\nThe regression equation is\n\\[\n\\widehat{Price} = -114.37 + 0.31\\times\\text{Sq. Ft.}  + -90.12\\times\\text{Waterfront} + 0.27\\times\\text{Sq.Ft}\\times\\text{Waterfront}\n\\]\nFor a house on the waterfront (\\(\\text{waterfront}=1\\)), the equation is\n\\[\n\\begin{aligned}\n\\widehat{Price} & = -114.37 + 0.31\\times\\text{Sq. Ft.} -90.12 \\times\\text{1} + 0.27\\times\\text{Sq.Ft}\\times\\text{1} \\\\\n& = (-114.37 + -90.12) + (0.31+0.27)\\times{\\text{Sq. Ft.}} \\\\\n& = -204.49 + 0.58\\times{\\text{Sq. Ft.}}\n\\end{aligned}\n\\]\nFor a house not on the waterfront (\\(\\text{waterfront}=0\\)), the equation is\n\\[\n\\begin{aligned}\n\\widehat{Price} & = -114.37 + 0.31\\times\\text{Sq. Ft.} -90.12 \\times\\text{0} + 0.27\\times\\text{Sq.Ft}\\times\\text{0} \\\\\n& = -114.37 + 0.31\\times{\\text{Sq. Ft.}}\n\\end{aligned}\n\\] Interpretation\nWhen interpreting \\(b_0\\) and \\(b_1\\), we need to state that the interpretations apply only to the “baseline” category (in this case non-waterfront houses).\nIn a model with interaction, it does not make sense to talk about holding one variable constant when interpreting the effect of the other, since the effect of one variable depends on the value or category of the other. Instead, we must state the value or category of one variable when interpreting the effect of the other.\nInterpretations:\n\n\\(b_0\\) - On average, a house with 0 square feet that is not on the waterfront is expected to cost -114.37 thousand dollars. This is not a sensible interpretation since there are no houses with 0 square feet.\n\\(b_1\\) - For each additional square foot in size, the price of a non-waterfront house is expected to increase by 0.31 thousand dollars.\n\\(b_2\\) - On average, the price of a waterfront house with 0 square feet is expected to be -90.12 thousand dollars less than the price of a non-waterfront house with 0 square feet. This is not a sensible interpretation in this case.\n\\(b_3\\) - For each additional square foot in size, the price of a waterfront house is expected to increase by 0.27 thousand dollars more than a non-waterfront house.\n\nAlternatively, we could interpret \\(b_0+b_2\\) and \\(b_1+b_3\\) together.\n\n\\(b_0 + b_2\\) - On average, a house with 0 square feet that is on the waterfront is expected to cost -204.49 thousand dollars. This is not a sensible interpretation since there are no houses with 0 square feet.\n\\(b_1 + b_3\\) - For each additional square foot in size, the price of a waterfront house is expected to increase by 0.58 thousand dollars.\n\nPrediction\nWe calculate predicted prices for the following houses:\n\nHouses[c(1,16), ] %&gt;% select(ID, price, sqft_living, waterfront)\n\n# A tibble: 2 × 4\n     ID price sqft_living waterfront\n  &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;     \n1    90  335.        1030 No        \n2  2141 1700         2500 Yes       \n\n\n\\[\n\\widehat{Price}_1 =  -114.37 + 0.31\\times 1030  + -90.12\\times0 + 0.27\\times 1030 \\times 0 = -204 \\text{ thousand dollars}\n\\]\n\\[\n\\widehat{Price}_2 =  -114.37 + 0.31\\times 1030  + -90.12\\times1 + 0.27\\times 1030 \\times 1 = 392.91 \\text{ thousand dollars}\n\\]\nWe’ll calculate \\(R^2\\) for the model with and without the interaction term.\nModel with Interaction\n\nsummary(M_House_price_sqft_wf_int)$r.squared\n\n[1] 0.7770731\n\n\nModel without Interaction\n\nsummary(M_House_price_sqft_wf)$r.squared\n\n[1] 0.7411526\n\n\nWe see that adding an interaction term improved the proportion of variability in house price explained by the model from 0.78 to 0.74. This is a fairly notable increase.\nWe can also perform an ANOVA F-test to compare a full model that includes the interaction term to a reduced model that doesn’t.\n\nanova(M_House_price_sqft_wf, M_House_price_sqft_wf_int)\n\nAnalysis of Variance Table\n\nModel 1: price ~ sqft_living + waterfront\nModel 2: price ~ sqft_living * waterfront\n  Res.Df      RSS Df Sum of Sq      F        Pr(&gt;F)    \n1    197 23720774                                      \n2    196 20429024  1   3291749 31.582 0.00000006502 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe large F-statistic and small p-value provides strong evidence that the model with interaction better explains variability in price than the model without interaction. There is strong reason for including the interaction term in the model.\n\n\n5.4.4 When to Include Interaction Terms\nWe should only include interaction terms in a model when we have good reason to believe that the effect of one explanatory variable on the response depends on another explanatory variable. We’ve seen that adding interaction terms to a model make it harder to interpret. Making a model more complex than necessary also increases the risk of overfitting. We can use graphs, our background knowledge and domain are expertise, and model comparison tests to guide us.\nLet’s look at another example, involving the 2020 cars data.\nThe plot below shows the relationship between log(price), acceleration time, and drive type for a sample of new cars release in 2020.\n\nggplot(data=Cars2020, aes(x=Acc060, y=log(Price), color=Drive)) + geom_point()+stat_smooth(method=\"lm\", se=FALSE) + theme_bw()\n\n\n\n\n\n\n\n\nLet’s consider three different models:\nModel 1 (Acc060 only):\n\\[\n\\widehat{LogPrice} = b_0 + b_1\\times\\text{Acc060} + b_3\\times\\text{DriveFWD} + b_2\\times\\text{DriveRWD}\n\\] Model 1 assumes that log(price) changes linearly with acceleration time and that drive type has no effect on price.\nModel 2 (Acc060 and Drive - No interaction):\n\\[\n\\widehat{LogPrice} = b_0 + b_1\\times\\text{Acc060} + b_3\\times\\text{DriveFWD} + b_2\\times\\text{DriveRWD}\n\\] Model 2 assumes log(price) changes linearly with acceleration time. Some drive types might be more or less expensive than others, but the rate of change in price with respect to acceleration time (i.e. the slope) is the same for each drive type.\nModel 3 (Acc060 and Drive - with Interaction):\n\\[\n\\begin{aligned}\n\\widehat{LogPrice} &= b_0 + b_1\\times\\text{Acc060} + b_2\\times\\text{DriveFWD} + b_3\\times\\text{DriveRWD} \\\\ & + b_4\\times\\text{Acc060}\\times\\text{DriveFWD}  + b_5\\times\\text{Acc060}\\times\\text{DriveRWD}\n\\end{aligned}\n\\]\nModel 2 assumes log(price) changes linearly with acceleration time. It allows for the possibility that some drive types might be more expensive than others, and that price might change at different rates with respect to acceleration time for some drive types than others.\nWhich Model to Use?\nWe see in the plot that AWD cars tend to be more expensive than FWD and RWD. This suggests that drive type does have an effect on log(price), so Model 2 appears more appropriate than Model 1.\nHowever, the slopes aren’t that different. Price seems to decrease at roughly the same rate with respect to acceleration time for all three drive types. There doesn’t seem to be a reason to add interaction terms to the model.\nModel 2 seems most appropriate.\nWe can use ANOVA F-tests and \\(R^2\\) to compare the models. First, let’s compare Models 1 and 2.\nANOVA F-Test for Model 1 vs Model 2\n\nM_Cars_logprice_acc060 &lt;- lm(data=Cars2020, log(Price) ~ Acc060)\nM_Cars_logprice_acc060_drive &lt;- lm(data=Cars2020, log(Price) ~ Acc060 + Drive)\nanova(M_Cars_logprice_acc060, M_Cars_logprice_acc060_drive)\n\nAnalysis of Variance Table\n\nModel 1: log(Price) ~ Acc060\nModel 2: log(Price) ~ Acc060 + Drive\n  Res.Df    RSS Df Sum of Sq      F       Pr(&gt;F)    \n1    108 9.5921                                     \n2    106 7.3242  2    2.2679 16.411 0.0000006179 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\\(R^2\\) for Model 1\n\nsummary(M_Cars_logprice_acc060)$r.squared\n\n[1] 0.5248405\n\n\n\\(R^2\\) for Model 2\n\nsummary(M_Cars_logprice_acc060_drive)$r.squared\n\n[1] 0.6371841\n\n\nThe large F-statistic and small p-value suggest that Model 2 better explains variability in price than Model 1. Furthermore, Model 2 explains more than 10% more of the variability in price. Model 2 is preferred over Model 1.\nNow, we’ll compare Model 2 to Model 3.\nANOVA F-Test for Model 2 vs Model 3\n\nM_Cars_logprice_acc060_drive &lt;- lm(data=Cars2020, log(Price) ~ Acc060 + Drive)\nM_Cars_logprice_acc060_drive_int &lt;- lm(data=Cars2020, log(Price) ~ Acc060 + Drive + Acc060:Drive)\nanova(M_Cars_logprice_acc060_drive, M_Cars_logprice_acc060_drive_int)\n\nAnalysis of Variance Table\n\nModel 1: log(Price) ~ Acc060 + Drive\nModel 2: log(Price) ~ Acc060 + Drive + Acc060:Drive\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    106 7.3242                           \n2    104 7.1277  2   0.19649 1.4335 0.2431\n\n\n\\(R^2\\) for Model 1\n\nsummary(M_Cars_logprice_acc060_drive)$r.squared\n\n[1] 0.6371841\n\n\n\\(R^2\\) for Model 2\n\nsummary(M_Cars_logprice_acc060_drive_int)$r.squared\n\n[1] 0.6469176\n\n\nThe small F-statistic and large p-value provide no evidence that Model 3 is better than Model 2. Furthermore, \\(R^2\\) only increases slightly when the interaction terms are added. There is not justification for choosing Model 3 over Model 2.\nModel 2 appears to be the best model.\nWhen using model comparison tests, it’s important to remember that p-values are only part of the picture. We shouldn’t make decisions based solely on the p-value. When sample size is large a model comparison test might yield a small p-value even if the difference between the models is not practically meaningful. It’s important to also consider graphical intuition, real-life knowledge about the situation, and change in \\(R^2\\) when deciding whether to add an interaction term to a model.\n\n\n5.4.5 Interaction vs Correlation\nIt is easy to confuse the concept of interaction with that of correlation. These are, in fact, very different concepts.\nA correlation between two variables means that as one increases, the other is more likely to increase or decrease. We only use the word correlation to describe two quantitative variables, but we could discuss the similar notion of a relationship between categorical variables.\nAn interaction between two explanatory variables means that the effect of one on the response depends on the other.\nExamples of Correlations (or relationships)\n\nHouses on the waterfront tend to be bigger than houses not on the waterfront, so there is a relationship between square feet and waterfront status.\nHouses with large amounts of living space in square feet are likely to have more bedrooms, so there is a correlation between living space and bedrooms.\nSuppose that some genres of movies (drama, comedy, action, etc.) tend to be longer than others. This is an example of a relationship between genre and length.\n\nThe fact that there is a correlation between explanatory variables is NOT a reason to add an interaction term involving those variables in a model. Correlation is something entirely different than interaction!\nExamples of Interactions\n\nAs houses on the waterfront increase in size, their price increases more rapidly than for houses not on the waterfront. This means there is an interaction between size and waterfront location.\nSuppose that the effect of additional bedrooms on price is different for houses with lots of living space than for houses with little living space. This would be an example of an interaction between living space and number of bedrooms.\nSuppose that audiences become more favorable to dramas as they get longer, but less favorable to comedies as they get longer. In this scenario, the effect of movie length on audience rating depends on the genre of the movie, indicating an interaction between length and genre.\n\n\n\n5.4.6 Interaction More Generally\nIn this section, we’ve seen examples of models with interaction involving a categorical and quantitative explanatory variable. These are easiest to picture graphically. It is possibly, however, to have interactions between two quantitative variables, or two categorical ones.\nFor example, suppose that for light weight cars those that can accelerate faster tended to be more expensive, but for heavier cars, there was no relationship between price and acceleration time. This would be an example of an interaction between weight and acceleration time, both quantitative variables.\nOr, suppose that sedans with all wheel drive were more expensive than sedans with only front wheel drive, but that trucks with all wheel drive were the same price as trucks with only front wheel drive. This would be an example of an interaction between type of car and drive type.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Building and Assessing Models</span>"
    ]
  },
  {
    "objectID": "Ch5.html#building-a-model-for-interpretation",
    "href": "Ch5.html#building-a-model-for-interpretation",
    "title": "5  Building and Assessing Models",
    "section": "5.5 Building a Model for Interpretation",
    "text": "5.5 Building a Model for Interpretation\nSo far, we’ve dealt with models with 2 or fewer variables. Some real questions require accounting for more than two variables. In these situations, we’ll need to develop a model that is complex enough to capture the important aspects of the mechanism we’re modeling, but also simple enough for us to be able to explain and interpret. We’ll need to decide how many variables to include in the model, and whether to use transformations, or to include interaction terms.\nWe’ll examine strategies for modeling in two different contexts. In this chapter, we’ll focus on building models for situations when we want to make interpretations and draw conclusions about relationships between variables. In Chapter 7, we focus on modeling solely for the purpose of prediction, when we are not interested in making interpretations or conclusions about relationships between variables.\nWe’ll need to think about things like:\n\nwhich explanatory variables should we include in the model, and how many?\n\nshould we include any interaction terms?\n\nshould we use any nonlinear terms?\n\nshould we use a transformation of the response variable?\n\nIn this section we’ll go through a set of steps to build a model that will help us understand factors that contribute to the price of a car.\n\n5.5.1 Exploratory Data Analysis\nThe Cars2020 dataset from the Lock5Data R package contains several variables with information pertaining to the price of a new 2020 car. These include:\n\nMake\n\nManufacturer (e.g. Chevrolet, Toyota, etc.)\n\nModel - Car model (e.g. Impala, Highlander, …)\n\nType - Vehicle category (Hatchback, Minivan, Sedan, Sporty, SUV, or Wagon)\n\nPrice - Lowest MSRP (in $1,000)\n\nCityMPG - City miles per gallon (EPA)\n\nHwyMPG - Highway miles per gallon (EPA)\n\nSeating - Seating capacity\n\nDrive - Type of drive (AWD, FWD, or RWD)\n\nAcc030 - Time (in seconds) to go from 0 to 30 mph\n\nAcc060 - Time (in seconds) to go from 0 to 60 mph\n\nQtrMile - Time (in seconds) to go 1/4 mile\n\nBraking - Distance to stop from 60 mph (dry pavement)\n\nFuelCap - Fuel capacity (in gallons)\n\nLength - Length (in inches)\n\nWidth - Width (in inches)\n\nHeight - Height (in inches)\n\nWheelbase - Wheelbase (in inches)\n\nUTurn - Diameter (in feet) needed for a U-turn\n\nWeight - Curb weight (in pounds)\n\nSize - Large, Midsized, or Small\n\nWe’ll start by creating some graphs and tables to explore the data.\nWe’ll look at a summary of the categorical variables in the dataset.\n\nCars_Cat &lt;- select_if(Cars2020, is.factor)\nsummary(Cars_Cat)\n\n        Make         Model            Type    Drive          Size   \n Chevrolet: 8   3       :  1   Hatchback: 4   AWD:80   Large   :21  \n Toyoto   : 7   3 Series:  1   Minivan  : 4   FWD:25   Midsized:41  \n Audi     : 6   300     :  1   Sedan    :38   RWD: 5   Small   :48  \n Hyundai  : 6   4Runner :  1   Sporty   :11                         \n Nissan   : 6   5 series:  1   SUV      :50                         \n BMW      : 5   500L    :  1   Wagon    : 3                         \n (Other)  :72   (Other) :104                                        \n\n\nWe examine the correlation matrix and plot of quantitative variables.\n\nCars_Num &lt;- select_if(Cars2020, is.numeric)\nC &lt;- cor(Cars_Num, use = \"pairwise.complete.obs\")\nround(C,2)\n\n          Price Acc060 CityMPG HwyMPG Seating Acc030 QtrMile Braking FuelCap\nPrice      1.00  -0.66   -0.48  -0.47    0.16  -0.66   -0.67   -0.02    0.57\nAcc060    -0.66   1.00    0.52   0.45    0.02   0.95    0.98    0.27   -0.50\nCityMPG   -0.48   0.52    1.00   0.88   -0.58   0.52    0.49   -0.22   -0.84\nHwyMPG    -0.47   0.45    0.88   1.00   -0.52   0.48    0.43   -0.19   -0.78\nSeating    0.16   0.02   -0.58  -0.52    1.00   0.01    0.07    0.63    0.64\nAcc030    -0.66   0.95    0.52   0.48    0.01   1.00    0.95    0.26   -0.51\nQtrMile   -0.67   0.98    0.49   0.43    0.07   0.95    1.00    0.34   -0.46\nBraking   -0.02   0.27   -0.22  -0.19    0.63   0.26    0.34    1.00    0.35\nFuelCap    0.57  -0.50   -0.84  -0.78    0.64  -0.51   -0.46    0.35    1.00\nLength     0.45  -0.48   -0.69  -0.52    0.63  -0.43   -0.43    0.41    0.81\nWidth      0.50  -0.49   -0.78  -0.73    0.63  -0.47   -0.44    0.28    0.80\nHeight     0.12   0.09   -0.60  -0.65    0.81   0.06    0.16    0.63    0.63\nWheelbase  0.43  -0.43   -0.69  -0.55    0.62  -0.39   -0.39    0.44    0.80\nUTurn      0.51  -0.47   -0.79  -0.69    0.61  -0.42   -0.41    0.35    0.81\nWeight     0.58  -0.45   -0.86  -0.82    0.72  -0.46   -0.41    0.44    0.92\nLogPrice   0.96  -0.72   -0.62  -0.58    0.23  -0.72   -0.71    0.01    0.67\n          Length Width Height Wheelbase UTurn Weight LogPrice\nPrice       0.45  0.50   0.12      0.43  0.51   0.58     0.96\nAcc060     -0.48 -0.49   0.09     -0.43 -0.47  -0.45    -0.72\nCityMPG    -0.69 -0.78  -0.60     -0.69 -0.79  -0.86    -0.62\nHwyMPG     -0.52 -0.73  -0.65     -0.55 -0.69  -0.82    -0.58\nSeating     0.63  0.63   0.81      0.62  0.61   0.72     0.23\nAcc030     -0.43 -0.47   0.06     -0.39 -0.42  -0.46    -0.72\nQtrMile    -0.43 -0.44   0.16     -0.39 -0.41  -0.41    -0.71\nBraking     0.41  0.28   0.63      0.44  0.35   0.44     0.01\nFuelCap     0.81  0.80   0.63      0.80  0.81   0.92     0.67\nLength      1.00  0.73   0.46      0.92  0.81   0.79     0.54\nWidth       0.73  1.00   0.64      0.75  0.76   0.86     0.64\nHeight      0.46  0.64   1.00      0.48  0.58   0.72     0.21\nWheelbase   0.92  0.75   0.48      1.00  0.82   0.82     0.53\nUTurn       0.81  0.76   0.58      0.82  1.00   0.84     0.62\nWeight      0.79  0.86   0.72      0.82  0.84   1.00     0.69\nLogPrice    0.54  0.64   0.21      0.53  0.62   0.69     1.00\n\n\n\nlibrary(corrplot)\nC &lt;- corrplot(C)\n\n\n\n\n\n\n\n\nWe see there is strong correlation between many of the variables. We’ll want to include variables that are highly correlated with price in the model. At the same time, we’ll see that including too many explanatory variables, especially when they are highly correlated, makes the model hard to interpret. We’ll need to carefully select explanatory variables that help us answer the questions we’re most interested in.\nSo far, we’ve explored the relationship between price and acceleration time. We found that modeling log(Price) as a function of acceleration time was more consistent with regression model assumptions than modeling price itself, so we’ll start there.\n\nM_Cars1 &lt;- lm(data=Cars2020, log(Price) ~ Acc060)\nsummary(M_Cars1)\n\n\nCall:\nlm(formula = log(Price) ~ Acc060, data = Cars2020)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.90992 -0.19686  0.00148  0.15566  0.63774 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  5.00390    0.14547   34.40 &lt;0.0000000000000002 ***\nAcc060      -0.19889    0.01821  -10.92 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.298 on 108 degrees of freedom\nMultiple R-squared:  0.5248,    Adjusted R-squared:  0.5204 \nF-statistic: 119.3 on 1 and 108 DF,  p-value: &lt; 0.00000000000000022\n\n\nWe see that just over 50% of the total variability in log(Price) is explained by acceleration time alone. Let’s investigate what other variables we should add to the model to better understand the factors contributing to the price of a car.\nAlong the way, we’ll discover some statistical considerations we should think about when building models.\n\n\n5.5.2 Simpson’s Paradox\nLet’s next investigate the effect of a car’s gas mileage on price. There are two gas mileage variables in the dataset, CityMPG and HwyMPG. We’ll use HwyMPG, which is the number of miles the car can go on one mile of gas when driving on the highway. We’ll also account for the weight of the car in lbs. The plot below shows the relationships between each of these variables with log(Price), as well as the relationship between Weight and Hwy MPG.\n\nP1 &lt;- ggplot(data=Cars2020, aes(x=HwyMPG, y=log(Price))) + geom_point() + stat_smooth(method=\"lm\", se=FALSE) + theme_bw() \n\nP2 &lt;- ggplot(data=Cars2020, aes(x=HwyMPG, y=log(Price))) + geom_point() + stat_smooth(method=\"lm\", se=FALSE) + theme_bw() \n\nP3 &lt;- ggplot(data=Cars2020, aes(x=Weight, y=HwyMPG)) + geom_point() + stat_smooth(method=\"lm\", se=FALSE) + theme_bw() \n\ngrid.arrange(P1, P2, P3, ncol=3)\n\n\n\n\n\n\n\n\nWe see that both highway MPG and weight are negatively associated with price, and that there is also a negative relationship between weight and highway MPG.\nLet’s add highway gas mileage to our model, along with acceleration time.\n\nM_Cars2 &lt;- lm(data=Cars2020, log(Price) ~ Acc060 + HwyMPG)\nsummary(M_Cars2)\n\n\nCall:\nlm(formula = log(Price) ~ Acc060 + HwyMPG, data = Cars2020)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88944 -0.14122 -0.00675  0.15266  0.64910 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  5.339932   0.151927  35.148 &lt; 0.0000000000000002 ***\nAcc060      -0.160005   0.018699  -8.557   0.0000000000000918 ***\nHwyMPG      -0.018843   0.004075  -4.624   0.0000105782710393 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2733 on 107 degrees of freedom\nMultiple R-squared:  0.604, Adjusted R-squared:  0.5966 \nF-statistic: 81.59 on 2 and 107 DF,  p-value: &lt; 0.00000000000000022\n\n\nAs the plot showed, highway MPG has a negative coefficient. Assuming acceleration time is held constant, price is expected to multiply by a factor of \\[e^{-0.02} =0.98\\], a 2 percent decrease.\nIt may be surprising to learn that cars that get better gas mileage tend to be less expensive than those with lower gas mileage. We see in the summary output that the p-value associated with HwyMPG is small, indicating a statistically discernible relationship.\nWe’ve learned, however, that when something seems unusual, we should think deeper. We noticed that highway gas mileage was negatively associated with weight, and that both are negatively associated with price. Since heaviest cars tend to also be the most expensive, it may be that cars that get better gas mileage are less expensive because they they are smaller and weigh less.\nIn the figure below we break the cars into three categories, based on their weight.\nLight = less than 3,500 lbs.\nMedium = between 3,500 and 4,500 lbs.\nHeavy = heavier than 4,500 lbs.\n\nCars2020 &lt;- mutate(Cars2020, Weight_Group = cut(Weight, \n      breaks=c(0, 3500, 4500, 6500), \n      labels=c(\"Light Weight\", \"Medium Weight\", \"Heavy Weight\")))\n\nggplot(data=Cars2020, aes( y=Price, x=HwyMPG )) +geom_point() + facet_wrap(facets = ~Weight_Group) +\nstat_smooth(method=\"lm\", se=FALSE) + xlab(\"Highway MPG\") + theme_bw()\n\n\n\n\n\n\n\n\nOnce we compare cars of similar weights, we see that there is little to no relationship between highway MPG and price. The negative trend for low weight cars is due primarily to one outlier, while the other two weights show slightly positive to no trend. This is different than the strong negative relationship we saw in the previous graph.\nLet’s add weight to the model and see what happens to to estimate associated with highway MPG.\n\nM_Cars3 &lt;- lm(data=Cars2020, log(Price) ~ Acc060 + HwyMPG + Weight)\nsummary(M_Cars3)\n\n\nCall:\nlm(formula = log(Price) ~ Acc060 + HwyMPG + Weight, data = Cars2020)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.65689 -0.16159 -0.01278  0.13494  0.91927 \n\nCoefficients:\n               Estimate  Std. Error t value           Pr(&gt;|t|)    \n(Intercept)  3.40468799  0.37981919   8.964 0.0000000000000120 ***\nAcc060      -0.14489669  0.01683344  -8.608 0.0000000000000751 ***\nHwyMPG       0.00500984  0.00567833   0.882               0.38    \nWeight       0.00025961  0.00004763   5.451 0.0000003294667341 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2427 on 106 degrees of freedom\nMultiple R-squared:  0.6907,    Adjusted R-squared:  0.6819 \nF-statistic: 78.89 on 3 and 106 DF,  p-value: &lt; 0.00000000000000022\n\n\nNotice that when weight is added to the model, the coefficient on HwyMPG changes signs and is now positive. This means that assuming we are comparing cars of the same acceleration time and weight, one with higher highway gas mileage is expected to cost more than one with lower gas mileage. The difference is small, with price expected to multiply by about \\[e^{0.01} =1.01\\] (a -0.5022407) percent increase), and not statistically discernible (high p-value). So, there really doesn’t appear to be much of a relationship at all between price and gas mileage, after accounting for acceleration time and weight.\nStill, this is a different conclusion than we would have drawn if we had not accounted for weight in the model. It had originally appeared that highway MPG had a strong negative association with price, but we see that once we compare cars of similar weights, there is little to no relationship between price and highway MPG. This is an example of a phenomenon know as Simpson’s Paradox. In instances of Simpson’s Paradox, the relationship between two variables changes directions, or disappears once a third variable is accounted for. The weight variable is called a confounding variable. If we don’t account for weight, we get a misleading picture of the relationship between highway gas mileage and price.\nWe saw that accounting for weight helps us better understand the relationship between price and gas mileage. We should also discuss the relationship between price and weight itself. Since weights of cars range from 2,000 to 6,000 lbs, it would be silly to talk about the effect of a single additional pound. Rather, let’s examine the effect of a 1,000 pound increase on price of a car. For each additional 1,000 lbs., price is expected to multiply by a factor of \\[e^{0.00026\\times 1000} =1000.259645\\], a 29.6426102 percent increase.\nOur model with weight and highway gas mileage, in addition to acceleration time, now explains about 69.0673511 percent of the variability in log(Price), up from 52.4840462 for the model with only acceleration time.\nWe saw in the preceding graph, that the direction of the relationship between price and gas mileage was negative for light weight cars and positive for medium weight ones, suggesting a possible interaction between weight and gas mileage. So, we might consider adding an interaction term to the model. The : command adds just the interaction term.\n\nM_Cars4 &lt;- lm(data=Cars2020, log(Price) ~ Acc060 + HwyMPG + Weight + HwyMPG:Weight)\nsummary(M_Cars4)\n\n\nCall:\nlm(formula = log(Price) ~ Acc060 + HwyMPG + Weight + HwyMPG:Weight, \n    data = Cars2020)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63443 -0.14677 -0.01321  0.13097  0.94847 \n\nCoefficients:\n                  Estimate   Std. Error t value         Pr(&gt;|t|)    \n(Intercept)    3.879767385  0.490968739   7.902 0.00000000000286 ***\nAcc060        -0.135947988  0.017745763  -7.661 0.00000000000960 ***\nHwyMPG        -0.014894324  0.014310665  -1.041            0.300    \nWeight         0.000095708  0.000118187   0.810            0.420    \nHwyMPG:Weight  0.000006049  0.000003997   1.514            0.133    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2412 on 105 degrees of freedom\nMultiple R-squared:  0.6973,    Adjusted R-squared:  0.6857 \nF-statistic: 60.46 on 4 and 105 DF,  p-value: &lt; 0.00000000000000022\n\n\nThe coefficient estimate on the interaction term is small and the p-value is large, telling us we don’t have much evidence of an interaction between weight and MPG. Also notice that \\(R^2\\) barely changed, moving from 0.691 to 0.697 when the interaction term is included. Since interactions make the model harder to interpret, we shouldn’t include one unless we have good reason to. So, in this case, we’ll stick with the simpler model with no interaction term.\n\n\n5.5.3 Multicollinearity\nLet’s continue to add variables to our model that might help us learn about factors affecting the price of a car. Another variable in the dataset was the amount of time it takes a car to drive a quarter mile. We’ll add that variable to the model.\n\nM_Cars5 &lt;- lm(data=Cars2020, log(Price) ~ Acc060 + HwyMPG + Weight + QtrMile)\nsummary(M_Cars5)\n\n\nCall:\nlm(formula = log(Price) ~ Acc060 + HwyMPG + Weight + QtrMile, \n    data = Cars2020)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.73603 -0.15035 -0.00338  0.12601  0.83499 \n\nCoefficients:\n              Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)  4.9556985  0.8375191   5.917 0.0000000415 ***\nAcc060      -0.0001390  0.0718750  -0.002       0.9985    \nHwyMPG       0.0067463  0.0056549   1.193       0.2356    \nWeight       0.0002825  0.0000482   5.862 0.0000000534 ***\nQtrMile     -0.1766120  0.0853267  -2.070       0.0409 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.239 on 105 degrees of freedom\nMultiple R-squared:  0.7028,    Adjusted R-squared:  0.6915 \nF-statistic: 62.07 on 4 and 105 DF,  p-value: &lt; 0.00000000000000022\n\n\nThe coefficient estimate for QtrMile time is negative , which is not surprising. We would expect cars that take longer to drive a quarter mile to be less expensive. But, look at what happened to the Acc060 variable. The estimate now is very slightly negative (a change from Model M_Cars4), but the most striking change is in the standard error column. The standard error associated with the Acc060 variable increased from 0.017 to 0.072, (more than 4 times larger). This has a big impact on the t-statistic, p-value, and confidence intervals associated with Acc060. Since standard errors are used to calculate confidence intervals, it will impact these as well.\nConfidence Interval for Acceleration Time - Model without Quarter Mile Time\n\nexp(confint(M_Cars3, level=0.95, parm=\"Acc060\")) |&gt; round(2)\n\n       2.5 % 97.5 %\nAcc060  0.84   0.89\n\n\nWe are 95% confident that a 1-second increase in acceleration time is associated with an average price decrease between 11 and 16 percent, assuming weight and highway gas mileage are held constant.\nConfidence Interval for Acceleration Time - Model with Quarter Mile Time\n\nexp(confint(M_Cars5, level=0.95, parm=\"Acc060\")) |&gt; round(2)\n\n       2.5 % 97.5 %\nAcc060  0.87   1.15\n\n\nWe are 95% confident that a 1-second increase in acceleration time is associated with an average price change between a 13 decrease and a 15 percent increase, assuming weight, highway gas mileage, and quarter mile time are held constant.\nNotice how the second interval is so much wider than the first that it is practically useless. It tells us almost nothing about the relationship between price and acceleration time. This happens because quarter mile time is strongly correlated with Acc060. Recall their correlation was 0.98. When we add an explanatory variable that is highly correlated with a variable already in the model, the model will be unable to separate the effect of one variable from the effect of the other, causing it to yield high standard errors, reflecting lots of uncertainty about the effect of both variables. Further, recall that interpreting one variable in a multiple regression model requires assuming all others are held constant. But, if a can accelerate faster, it will most surely be able to drive a quarter mile more quickly, so it doesn’t make sense to talk about increasing acceleration time while holding quarter mile time constant.\nKey Point: Stong correlation between explanatory variables in a model is bad. This is called multicollinearity. When building a model, avoid including strongly correlated explanatory variables. Pick the one you think is most relevant and draw conclusions based on it. Generally, explanatory variables with correlation above 0.8 probably shouldn’t be included in the same model. If you are unsure whether multicollinearity will be a problem, you could look at how standard error on one variable changes when a second variable is added to the model. If standard error for the first variable increases then we should be cautious about adding the second variable. (Note: Correlation between the explanatory and response variables is not a problem. It’s generally a good thing because it means our model will be able to make more accurate predictions.)\nImpact of Multicollinearity on Prediction\nWe’ve seen that multicollinearity can have a substantial affect on confidence intervals for regression coefficients \\(b_j\\). Let’s see how multicollinearity affects predicted values.\nSuppose we want to predict the price of a car that can accelerate from 0 to 60 mph in 9.5 seconds, weighs 4000 lbs, gets 25 mpg on the highway, and completes a quarter mile in 17.5 seconds.\nPrediction interval based on Model without Quarter Mile Time:\n\nexp(predict(M_Cars3, newdata = data.frame(Acc060=9.5, \n                                          Weight = 4000, HwyMPG = 25, \n                                          QtrMile=17.3), level=0.95, \n                                          interval=\"prediction\"))\n\n       fit      lwr      upr\n1 24.33366 14.79947 40.01001\n\n\nPrediction interval based on Model with Quarter Mile Time:\n\nexp(predict(M_Cars5, newdata = data.frame(Acc060=9.5, \n                                          Weight = 4000, \n                                          HwyMPG = 25, \n                                          QtrMile=17.3), \n                                          level=0.95, interval=\"prediction\"))\n\n       fit      lwr      upr\n1 24.47924 14.99922 39.95096\n\n\nWe see that the predicted values and intervals are nearly identical. While multicollinearity has a severe impact on confidence intervals for model estimates, it does not affect predictions or prediction intervals.\n\n\n5.5.4 Model Comparison Tests\nNext, we’ll consider adding categorical explanatory variables Size, and Drive. The plot shows the relationship between each of these variables and price.\n\nP1 &lt;- ggplot(data=Cars2020, aes(x=log(Price), y=Size)) + geom_boxplot() + ggtitle(\"Price by Size\")\nP2 &lt;- ggplot(data=Cars2020, aes(x=log(Price), y=Drive)) + geom_boxplot() + ggtitle(\"Price by Drive\")\ngrid.arrange(P1, P2, ncol=2)\n\n\n\n\n\n\n\n\nWe’ve already included information about size, through the weight variable, so we won’t Size it too. Let’s add drive type to the model.\n\nM_Cars6 &lt;- lm(data=Cars2020, log(Price) ~ Acc060 + HwyMPG + Weight + Drive)\nsummary(M_Cars6)\n\n\nCall:\nlm(formula = log(Price) ~ Acc060 + HwyMPG + Weight + Drive, data = Cars2020)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52142 -0.12972 -0.00994  0.13284  0.85043 \n\nCoefficients:\n               Estimate  Std. Error t value         Pr(&gt;|t|)    \n(Intercept)  3.43598628  0.41499637   8.280 0.00000000000045 ***\nAcc060      -0.14665443  0.01848492  -7.934 0.00000000000257 ***\nHwyMPG       0.00895120  0.00586769   1.526          0.13017    \nWeight       0.00023446  0.00004935   4.751 0.00000651159314 ***\nDriveFWD    -0.19735323  0.06537983  -3.019          0.00319 ** \nDriveRWD    -0.20279168  0.12475953  -1.625          0.10709    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2325 on 104 degrees of freedom\nMultiple R-squared:  0.7215,    Adjusted R-squared:  0.7081 \nF-statistic: 53.89 on 5 and 104 DF,  p-value: &lt; 0.00000000000000022\n\n\nThe baseline category for drive is AWD (all wheel drive). We see that FWD (front wheel) and RWD (rear wheel) both have negative estimates, indicating cars of these drive types tend to be less expensive than all wheel drive cars. Because the sample size for RWD cars is very small (only 5 cars), the difference between rear and all wheel drive cars is not statistically discernible, but the difference between FWD and AWD (which is roughly the same size), is statistically discernible.\nWe can use an ANOVA F-test to see if there is evidence of a relationship between price and drive type overall. We’ll compare the model that includes acceleration time, weight, and gas mileage, but not drive type (M_Cars3) to a model that includes these variables plus drive type (M_Cars6) .\n\nanova(M_Cars3, M_Cars6)\n\nAnalysis of Variance Table\n\nModel 1: log(Price) ~ Acc060 + HwyMPG + Weight\nModel 2: log(Price) ~ Acc060 + HwyMPG + Weight + Drive\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1    106 6.2444                                \n2    104 5.6216  2   0.62282 5.7611 0.004238 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe large F-statistic and small p-value provides evidence of a relationship between price and drive type after accounting for the other variables, suggesting we should add drive type to the model. We found evidence of differences in price between front-wheel drive and rear-wheel drive, compared to all wheel drive cars. Adding drive type to the model results in a modest increase in \\(R^2\\), as the model now explains about 72.2 percent of the variability in Log(Price).\nModel comparison tests can be helpful in deciding whether variables should be added to a model. We should keep in mind that when sample size is very large, even very small differences will return statistically discernible differences using the F-test, so we should also consider factors like \\(R^2\\), the size of our estimate(s) \\(b_j\\), background knowledge, and the variable’s relevance to our research question, when deciding whether or not to add it to a model.\n\n\n5.5.5 Checking Model Assumptions\nWe could keep looking at other variables to add, but at this point, we have a model that gives us a good sense of the factors related to price of a car, capturing 72 of total variability in car price, and is still easy to interpret. Furthermore, other variables that are correlated with price (such as fuel capacity, length, width, wheelbase, and UTurn diameter) are highly correlated with variables already in the model. Adding them to the model will do little to explain variability in price, and could create problems associated with multicollinearilty. For our research purposes, our model is good enough.\nWe’ll use residuals to check the model assumptions.\nResidual by Predicted Plot, Histogram of Residuals, and Normal Quantile-Quantile Plot\n\nresid_panel(M_Cars6, smoother=TRUE)\n\n\n\n\n\n\n\n\nThere is slight concern about constant variance, but otherwise, the model assumptions look good.\nResidual by Predictor Plots\n\nresid_xpanel(M_Cars6, smoother=TRUE)\n\n\n\n\n\n\n\n\nThese plots don’t raise any concerns.\n\n\n5.5.6 Interpretations\nThe model coefficients are shown below.\n\nM_Cars6$coefficients |&gt; round(3)\n\n(Intercept)      Acc060      HwyMPG      Weight    DriveFWD    DriveRWD \n      3.436      -0.147       0.009       0.000      -0.197      -0.203 \n\n\nSince we used a log transformation, we should interpret \\(e^{b_j}\\) rather than \\(b_j\\) itself.\n\nexp(M_Cars6$coefficients) |&gt; round(4)\n\n(Intercept)      Acc060      HwyMPG      Weight    DriveFWD    DriveRWD \n    31.0620      0.8636      1.0090      1.0002      0.8209      0.8164 \n\n\nThe intercept theoretically tells us that a car that accelerates from 0 to 60 mph in no time, gets 0 mpg, weighs 0 lbs, and is AWD would be expected to cost 31.062, 0.8636, 1.009, 1.0002, 0.8209, 0.8164 thousand dollars. This statement is nonsensical and we shouldn’t try to interpret the intercept.\nThe price of a car is expected to decrease by 14 percent for each additional second it takes to accelerate from 0 to 60 mph, assumping highway MPG, weight and type of drive are held constant.\nThe price of a car is expected to increase by 0.9 percent for each additional highway MPG, assumping acceleration time, weight and type of drive are held constant.\nThe price of a car is expected to increase by 0.023 percent for each additional pound in weight, assumping acceleration time, highway MPG, and type of drive are held constant. This is equivalent to a 23 percent increase for each additional 1,000 pounds in weight.\nFWD cars are expected to cost 18 less than AWD cars, assuming acceleration time, highway MPG, and weight are held constant.\nRWD cars are expected to cost 18 less than AWD cars, assuming acceleration time, highway MPG, and weight are held constant.\nReferring back to the model summary output, we saw that the effects associated with acceleration time, weight and the difference between FWD and AWD were statistically discernible, while the effect of highway MPG and the difference between RWD and AWD are not.\n\n\n5.5.7 Predictions\nWe’ll use our model to estimate the average price with the following characteristics, and also to predict the price of a new car with the given characteristics.\n\nnewcar &lt;- data.frame(Acc060 = 8, Weight=3000, HwyMPG = 30, Drive = \"AWD\")\n\nThis is an interval for log(Price).\n\npredict(M_Cars6, newdata=newcar, interval=\"confidence\", level=0.95)\n\n       fit      lwr      upr\n1 3.234658 3.100603 3.368712\n\n\nExponentiating, we obtain\n\nexp(predict(M_Cars6, newdata=newcar, interval=\"confidence\", level=0.95))\n\n       fit      lwr     upr\n1 25.39768 22.21135 29.0411\n\n\nWe are 95% confident that the average price of all new 2020 cars that weigh 3000 lbs, take 8 seconds to accelerate from 0 to 60 mph, weigh 3,000 lbs, and have all wheel drive will cost between 22 and 29 thousand dollars.\nNext, we calculate a prediction interval for an individual car with these characteristics.\n\nexp(predict(M_Cars6, newdata=newcar, interval=\"prediction\", level=0.95))\n\n       fit      lwr      upr\n1 25.39768 15.71348 41.05023\n\n\nWe are 95% confident that the price of an individual car that weighs 3000 lbs, takes 8 seconds to accelerate from 0 to 60 mph, weighs 3,000 lbs, and has all wheel drive will cost between 16 and 41 thousand dollars.\n\n\n5.5.8 Model Building Summary\nConsider the following when building a model for the purpose of interpreting parameters and understanding and drawing conclusions about a population or process.\nCharacteristics of a good model:\n\nModel driven by research question\nInclude variables of interest\n\nInclude potential confounding variables\n\nAvoid including highly correlated explanatory variables\n\nAvoid messy transformations and interactions where possible\n\nUse residual plots to assess appropriateness of model assumptions\n\nAim for high \\(R^2\\) but not highest\n\nAim for model complex enough to capture nature of data, but simple enough to give clear interpretations\n\n\nKeep in mind, there is no single correct model, but there are common characteristics of a good model. While two statisticians might use different models for a given set of data, they will hopefully lead to reasonably similar conclusions if constructed carefully.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Building and Assessing Models</span>"
    ]
  },
  {
    "objectID": "Ch5.html#practice-questions",
    "href": "Ch5.html#practice-questions",
    "title": "5  Building and Assessing Models",
    "section": "5.6 Practice Questions",
    "text": "5.6 Practice Questions\nWe’ll work with data on 120 houses that sold in the states of CA, NJ, NY, and PA. in 2019. The data are part of the Lock5Data package.\nThe dataset is called HomesForSale. The first six rows of the dataset are shown here.\n\nhead(HomesForSale)\n\n  State Price Size Beds Baths\n1    CA   533 1589    3   2.5\n2    CA   610 2008    3   2.0\n3    CA   899 2380    5   3.0\n4    CA   929 1868    3   3.0\n5    CA   210 1360    2   2.0\n6    CA   268 2131    3   2.0\n\n\n\n1)\nWe’ll model the price of the houses (which is given in thousands), using the size (in sq. ft.), and state as explanatory variables. We’ll use and ordinary linear regression model of the form:\n\\[\n\\text{Price} = \\beta_0 + \\beta_1\\times\\text{Size}+ \\beta_2\\times\\text{StateNJ}+ \\beta_3\\times\\text{StateNY}+ \\beta_4\\times\\text{StatePA} + \\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\n\nM_Homes_price_size_state &lt;- lm(data=HomesForSale, Price ~ Size + State)\n\nThe plots below can be used to check the assumptions associated with Model M1.\n\n\n\n\n\n\n\n\n\nState each of the four assumptions associated with the model. For each assumption, state which plot we should look at, and whether there is reason to be concerned about the validity of that assumption.\n\n\n2)\nNow we fit a model using log(Price), where log() denotes the natural, base e logarithm.\n\\[\n\\text{log(Price)} = \\beta_0 + \\beta_1\\times\\text{Size}+ \\beta_2\\times\\text{StateNJ}+ \\beta_3\\times\\text{StateNY}+ \\beta_4\\times\\text{StatePA} + \\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nThe model summary output and display the model summary output is displayed below.\nWe fit the model in R and display residual plots.\n\nM_Homes_logprice_size_state &lt;- lm(data=HomesForSale, log(Price) ~ Size + State)\n\n\n\n\n\n\n\n\n\n\nDoes this model appear more consistent with the modeling assumptions than Model 1? Explain your answer.\n\n\n3)\nSummary output for the model in Question 2 is shown below.\n\nsummary(M_Homes_logprice_size_state)\n\n\nCall:\nlm(formula = log(Price) ~ Size + State, data = HomesForSale)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.52375 -0.30947 -0.02936  0.32874  1.54434 \n\nCoefficients:\n               Estimate  Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  5.29974295  0.15559521  34.061 &lt; 0.0000000000000002 ***\nSize         0.00047694  0.00006916   6.896       0.000000000306 ***\nStateNJ     -0.46554934  0.13881407  -3.354              0.00108 ** \nStateNY     -0.66706497  0.13959321  -4.779       0.000005258665 ***\nStatePA     -0.79550507  0.13953556  -5.701       0.000000093536 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5375 on 115 degrees of freedom\nMultiple R-squared:  0.3978,    Adjusted R-squared:  0.3769 \nF-statistic: 18.99 on 4 and 115 DF,  p-value: 0.000000000005162\n\n\nExponentiated model coefficients are also shown.\n\nexp(M_Homes_logprice_size_state$coefficients)\n\n(Intercept)        Size     StateNJ     StateNY     StatePA \n200.2853201   1.0004771   0.6277901   0.5132127   0.4513532 \n\n\nWrite sentences interpreting each of the five estimates in context. If a parameter does not have a meaningful interpretation, explain why.\n\n\n4)\nContinue referring to the R output from the model in Question 3.\n\na)\nSuppose that the model estimates a house in CA to cost 400 thousand dollars. How much is a house in NY that is the same size expected to cost?\n\n\nb)\nIf one house has 500 square feet more than another house in the same state, how are their prices expected to compare?\n\n\nc)\nUse the model to calculate the predicted price of a 2000 square foot house in CA.\n\n\nd)\nUse the model to calculate the predicted price of a 2000 square foot house in PA.\n\n\n\n5)\nAgain, continuing with the model for log house price in Questions 2-4, write sentences interpreting each of the following intervals in context.\n\nexp(predict(M_Homes_logprice_size_state, newdata=data.frame(Size=2000, State=\"NJ\"), \n            interval=\"confidence\", level=0.95))\n\n       fit      lwr      upr\n1 326.3836 268.1397 397.2789\n\n\n\nexp(predict(M_Homes_logprice_size_state, newdata=data.frame(Size=2000, State=\"NJ\"), \n            interval=\"prediction\", level=0.95))\n\n       fit      lwr      upr\n1 326.3836 110.5396 963.6932\n\n\n\n\n6)\nThe following intervals come from the model in Question 1 that did not use the log transformation.\n\npredict(M_Homes_price_size_state, newdata=data.frame(Size=2000, State=\"NJ\"), interval=\"confidence\", level=0.95)\n\n       fit      lwr      upr\n1 360.9054 285.3913 436.4194\n\n\n\npredict(M_Homes_price_size_state, newdata=data.frame(Size=2000, State=\"NJ\"), interval=\"prediction\", level=0.95)\n\n       fit       lwr     upr\n1 360.9054 -55.03327 776.844\n\n\nHow do the predicted prices and confidence and prediction intervals for the model without the log transformation compare to those from the model with the transformation? Which do you think are more reliable? Why?\n\n\n7)\nContinnuing with the house price dataset in Questions 1-6, we add information about the number of bedrooms and bathrooms, in addition to size and state, to the model to predict log(price).\n\nM1 &lt;- lm(data=HomesForSale, log(Price) ~ Beds + Baths + Size + State)\n\nResidual plots and residual by predictor plots are shown below.\n\nresid_panel(M1)\n\n\n\n\n\n\n\n\n\nresid_xpanel(M1, smoother=TRUE)\n\n\n\n\n\n\n\n\n\na)\nIs there anything we should be concerned about in the validity of the regression model assumptions? If so, state which assumption you are concerned about and which plot causes this concern.\n\n\nb)\nIn the residual by predictor plot for beds, we see a downward sloping parabola. This might suggest adding a quadratic term for beds to the model. Why might this NOT be a good idea?\n\n\n\n8)\nThe Boston dataset contains data on the median property value (medv) in various Boston neighborhoods. The data were collected in the 1970’s. The dis variable provides a measure of distance from popular places of employment in Boston. (It was calculated by taking the weighed mean of distances from five different employment centers.) The chas variable is an indicator variable for whether or not the neighborhood borders the Charles River.\n\na)\nWe fit a model for log of median property value, using whether or not the house borders the Charles River, and distance from employment centers as explanatory variables.\nResidual plots and residual by predictor plots are shown below.\n\ndata(Boston)\nM_Boston_logmedv_chas_dis &lt;- lm(data=Boston, log(medv) ~ chas + dis)\n\n\nresid_panel(M_Boston_logmedv_chas_dis, smoother=TRUE)\n\n\n\n\n\n\n\n\n\nresid_xpanel(M_Boston_logmedv_chas_dis, smoother=TRUE)\n\n\n\n\n\n\n\n\nDo you see any reasons for concern about the validity of the regression assumptions? If so, which ones? How might we correct these.\n\n\nb)\nWe now add a quadratic term for distance from employment to the model. Residual and residual by predicted plots are again shown.\n\nM_Boston_logmedv_chas_dis2 &lt;- lm(data=Boston, log(medv) ~ chas + dis + I(dis^2))\n\n\nresid_panel(M_Boston_logmedv_chas_dis2, smoother=TRUE)\n\n\n\n\n\n\n\n\n\nresid_xpanel(M_Boston_logmedv_chas_dis2, smoother=TRUE)\n\n\n\n\n\n\n\n\nDoes adding the quadratic term to the model appear to have helped? Explain your answer.\nModel output for the model with the quadratic term is shown below. Use the output to answer parts (c - e)\n\nsummary(M_Boston_logmedv_chas_dis2)\n\n\nCall:\nlm(formula = log(medv) ~ chas + dis + I(dis^2), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1356 -0.2172 -0.0244  0.1776  1.2313 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  2.413630   0.064225  37.581 &lt; 0.0000000000000002 ***\nchas         0.298927   0.064058   4.667  0.00000393536281267 ***\ndis          0.250086   0.030412   8.223  0.00000000000000171 ***\nI(dis^2)    -0.018530   0.003031  -6.113  0.00000000196889180 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3636 on 502 degrees of freedom\nMultiple R-squared:  0.2134,    Adjusted R-squared:  0.2087 \nF-statistic: 45.41 on 3 and 502 DF,  p-value: &lt; 0.00000000000000022\n\n\nExponentiated model coefficients are also shown.\n\nexp(M_Boston_logmedv_chas_dis2$coefficients)\n\n(Intercept)        chas         dis    I(dis^2) \n 11.1744560   1.3484117   1.2841362   0.9816403 \n\n\n\n\nc)\nWrite a sentence interpreting the coefficient on the chas variable in context. Do the data provide evidence that neighborhoods on the Charles River tend to be more or less expensive than other neighborhoods?\n\n\nd)\nDo the data provide evidence of a quadratic relationship between a neighborhood’s distance from employment and property value? If so, explain, in real life terms what this might tell us about where people might prefer to live relative to where they work (at least in the 1970’s when the data were collected).\n\n\ne)\nCalculate the approximate distance from employment centers where property values are at their highest (hint: recall that a quadratic function of the form \\(f(x) = ax^2 + bx + c\\) attains its vertex at \\(x=-b/(2a)\\)).\n\n\n\n5.6.1 9)\nThe plots below show four different scenarios of a model involving response variable \\(y\\), a quantitative explanatory variable \\(x_1\\) and a categorical explanatory variable \\(x_2\\) with categories \\(A\\) and \\(B\\).\nThe model is\n\\[\ny = b_0 + b_1\\times x_1 + b_2\\times x_2B + b_3\\times x_1 \\times x_2B + \\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nFor each of the four plots, state whether \\(b_0\\), \\(b_1\\), \\(b_2\\), and \\(b_3\\) are positive, negative, or zero.\n\n\n\n\n\n\n\n\n\n\n\n10)\nRecall the roller coaster dataset from the Chapter 2 practice problems. The plots show the relationship between a coaster’s max speed (in mph), the year it was built (in years since 1900), and whether it was a wooden or steel coaster. Plot (A) shows lines of best fit when the slopes are forced to be the same, while Plot (B) shows lines of best fit when slopes are allowed to differ.\n\nPA &lt;- ggplot(data=Coasters, aes(y=Speed, x=Year_Since1900, color=Type)) + geom_point() + geom_parallel_slopes(se=FALSE) + ggtitle(\"Plot A\") + theme_bw()\nPB &lt;- ggplot(data=Coasters, aes(y=Speed, x=Year_Since1900, color=Type)) + geom_point() + stat_smooth(method=\"lm\", se=FALSE) + ggtitle(\"Plot B\") + theme_bw()\ngrid.arrange(PA, PB, ncol=2)\n\n\n\n\n\n\n\n\nConsider the following two models.\nModel 1:\n\\[\n\\text{ Speed} = b_0 + b_1\\times\\text{Year\\_Since1900} + b_2\\times{\\text{TypeWood}} + \\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nModel 2:\n\\[\n\\text{ Speed} = b_0 + b_1\\times\\text{Year\\_Since1900} + b_2\\times{\\text{TypeWood}} + b_3\\times\\text{Year\\_Since1900}\\times {\\text{TypeWood}} + \\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\n\na)\nWhich plot corresponds to Model 1? Which corresponds to Model 2? Based on the data, which model do you think we should use? Why?\n\n\nb)\nModel summary output for both models is shown below.\nModel 1 Output:\n\nM1_Coasters_speed_year_type &lt;- lm(data=Coasters, Speed~Year_Since1900 + Type)\nsummary(M1_Coasters_speed_year_type)\n\n\nCall:\nlm(formula = Speed ~ Year_Since1900 + Type, data = Coasters)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.096  -8.307  -0.551   7.573  56.711 \n\nCoefficients:\n               Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)    38.71917    7.00063   5.531 0.0000002 ***\nYear_Since1900  0.23854    0.07183   3.321    0.0012 ** \nTypeWooden     -2.82890    3.04157  -0.930    0.3543    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.3 on 116 degrees of freedom\nMultiple R-squared:  0.1445,    Adjusted R-squared:  0.1297 \nF-statistic: 9.793 on 2 and 116 DF,  p-value: 0.0001175\n\n\nModel 2 Output:\n\nM2_Coasters_speed_year_type_int &lt;- lm(data=Coasters, Speed~Year_Since1900 * Type)\nsummary(M2_Coasters_speed_year_type_int)\n\n\nCall:\nlm(formula = Speed ~ Year_Since1900 * Type, data = Coasters)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.217  -6.572   0.071   7.964  53.495 \n\nCoefficients:\n                          Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)                -1.2293    15.5957  -0.079   0.93731    \nYear_Since1900              0.6576     0.1629   4.036 0.0000982 ***\nTypeWooden                 44.1321    16.7633   2.633   0.00964 ** \nYear_Since1900:TypeWooden  -0.5130     0.1803  -2.846   0.00525 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.91 on 115 degrees of freedom\nMultiple R-squared:  0.2007,    Adjusted R-squared:  0.1799 \nF-statistic: 9.628 on 3 and 115 DF,  p-value: 0.00001011\n\n\nBased on the R output, which model appears to be more appropriate? Justify your answer by citing specific information contained in the R output.\n\n\nc)\nWrite sentences interpreting the coefficients \\(b_0\\), \\(b_1\\), and \\(b_2\\) in Model M1. Specifically give the numerical estimate of each coefficient in your interpretation. Also state what (if anything) we can conclude from the p-value associated with each coefficient.\n\n\nd)\nWrite sentences interpreting the coefficients \\(b_0\\), \\(b_1\\), \\(b_2\\), and \\(b_3\\) in Model M2. Specifically give the numerical estimate of each coefficient in your interpretation. Also state what (if anything) we can conclude from the p-value associated with each coefficient.\n\n\ne)\nFor each model, calculate the estimated top speed of the following coasters, both located at Cedar Point in Ohio:\n\nBlue Streak, a wooden coaster that opened in 1964.\n\nMillennium Force, a steel coaster that opened in 2000.\n\nWhich model’s predictions do you think are more accurate? Why?\n\n\nf)\nIs the following statement correct? Why or why not?\n“Before 1950, only wooden coasters were built. Over time, steel coasters became more common, and since 1990, the majority of coasters built are steel. Thus, there is an interaction between year built and type of coaster.”\n\n\n\n11)\nWe’ll continue working with the roller coasters dataset. The plots show the relationship between a coaster’s max speed (in mph), duration (in seconds), and design (sit down or other). Plot (A) shows lines of best fit when the slopes are forced to be the same, while Plot (B) shows lines of best fit when slopes are allowed to differ.\n\nPA &lt;- ggplot(data=Coasters, aes(y=Speed, x=Duration, color=Design)) + geom_point() + geom_parallel_slopes(se=FALSE) + ggtitle(\"Plot A\") + theme_bw()\nPB &lt;- ggplot(data=Coasters, aes(y=Speed, x=Duration, color=Design)) + geom_point() + stat_smooth(method=\"lm\", se=FALSE) + ggtitle(\"Plot B\") + theme_bw()\ngrid.arrange(PA, PB, ncol=2)\n\n\n\n\n\n\n\n\nConsider the following two models.\nModel 1:\n\\[\n\\widehat{\\text{ Speed}} = b_0 + b_1\\times\\text{Duration} + b_2\\times{\\text{SitDown}} + \\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\nModel 2:\n\\[\n\\widehat{\\text{ Speed}} = b_0 + b_1\\times\\text{Duration} + b_2\\times{\\text{SitDown}} + b_3\\times\\text{Duration}\\times {\\text{SitDown}} + \\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\n\\]\n\na)\nWhich plot corresponds to Model 1? Which corresponds to Model 2? Based on the data, which model do you think we should use? Why?\n\n\nb)\nModel summary output for both models is shown below.\nModel 1 Output:\n\nM1_Coasters_speed_duration_design &lt;- lm(data=Coasters, Speed ~ Duration + Design)\nsummary(M1_Coasters_speed_duration_design)\n\n\nCall:\nlm(formula = Speed ~ Duration + Design, data = Coasters)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.073  -8.809  -0.290   6.451  65.456 \n\nCoefficients:\n               Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)    48.33888    5.09703   9.484 0.000000000000000385 ***\nDuration        0.05686    0.03140   1.811               0.0728 .  \nDesignSit Down  4.49938    3.23346   1.392               0.1667    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.09 on 116 degrees of freedom\nMultiple R-squared:  0.03959,   Adjusted R-squared:  0.02303 \nF-statistic: 2.391 on 2 and 116 DF,  p-value: 0.09604\n\n\nModel 2 Output:\n\nM2_Coasters_speed_duration_design_int &lt;- lm(data=Coasters, Speed ~  Duration * Design)\nsummary(M2_Coasters_speed_duration_design_int)\n\n\nCall:\nlm(formula = Speed ~ Duration * Design, data = Coasters)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.125  -8.791  -0.242   6.487  65.544 \n\nCoefficients:\n                         Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)             49.099395  11.630842   4.221 0.0000487 ***\nDuration                 0.051186   0.084078   0.609     0.544    \nDesignSit Down           3.623370  12.460529   0.291     0.772    \nDuration:DesignSit Down  0.006605   0.090701   0.073     0.942    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.15 on 115 degrees of freedom\nMultiple R-squared:  0.03964,   Adjusted R-squared:  0.01458 \nF-statistic: 1.582 on 3 and 115 DF,  p-value: 0.1975\n\n\nBased on the R output, which model appears to be more appropriate? Justify your answer by citing specific information contained in the R output.\n\n\nc)\nWrite sentences interpreting the coefficients \\(b_0\\), \\(b_1\\), and \\(b_2\\) in Model M1. Specifically give the numerical estimate of each coefficient in your interpretation. Also state what (if anything) we can conclude from the p-value associated with each coefficient.\n\n\nd)\nWrite sentences interpreting the coefficients \\(b_0\\), \\(b_1\\), \\(b_2\\), and \\(b_3\\) in Model M2. Specifically give the numerical estimate of each coefficient in your interpretation. Also state what (if anything) we can conclude from the p-value associated with each coefficient.\n\n\ne)\nFor each model, calculate the estimated top speed of the following coasters, both located at Six Flags Great America in Illinois:\n\nSuperman Ultimate Flight, a flying (non sit-down) coaster with duration 180 seconds.\n\nViper, a sit-down coaster with duration 105 seconds\n\nWhich model’s predictions do you think are more accurate? Why?\n\n\n\n12)\n\ndata(diamonds)\n\nThe boxplot shows the distribution of prices in a set of 53,940 diamonds, broken down by quality of cut (Fair, Good, Very Good, Premium, Ideal).\n\nggplot(data=diamonds, aes(x=price, y=cut, fill=cut)) + geom_boxplot(outlier.size=0.01, outlier.alpha = 0.1) + \n    stat_summary(fun=mean, geom=\"point\", shape=4, color=\"red\", size=3)\n\n\n\n\nBox plot of prices of diamonds by quality of cut\n\n\n\n\nThe histogram shows the number of diamonds of each cut, along with the carat size (larger diamonds larger carat measurements).\nThe table shows the mean and standard deviation in price by quality of cut.\n\nDTab &lt;- diamonds %&gt;% group_by(cut) %&gt;% summarize(N=n(), \n                                         Avg_carat=mean(carat) |&gt; round(2), \n                                         Avg_price=mean(price) |&gt; round(0)\n)\nkable(DTab, caption=\"Average carat size and price by quality of cut\")\n\n\nAverage carat size and price by quality of cut\n\n\ncut\nN\nAvg_carat\nAvg_price\n\n\n\n\nFair\n1610\n1.05\n4359\n\n\nGood\n4906\n0.85\n3929\n\n\nVery Good\n12082\n0.81\n3982\n\n\nPremium\n13791\n0.89\n4584\n\n\nIdeal\n21551\n0.70\n3458\n\n\n\n\n\n\nggplot(data=diamonds, aes(x=carat, fill=cut)) + geom_histogram()\n\n\n\n\nHistogram of carat size and quality of cut\n\n\n\n\nFinally, the scatterplot shows the relationship between price, carat size, and quality of cut.\n\nggplot(data=diamonds, aes(x=carat, y=price, color=cut)) + geom_point()\n\n\n\n\nScatterplot of carat, price, and cut\n\n\n\n\nWe fit a model for price, using only quality of cut as an explanatory variable. Model summary output is shown below. (L=Good, Q=Very Good, c=Premium, ^4=Ideal).\nModel 1:\n\nM_Diamond_price_cut &lt;- lm(data=diamonds, price~cut)\nsummary(M_Diamond_price_cut)\n\n\nCall:\nlm(formula = price ~ cut, data = diamonds)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4258  -2741  -1494   1360  15348 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  4062.24      25.40 159.923 &lt; 0.0000000000000002 ***\ncut.L        -362.73      68.04  -5.331       0.000000098033 ***\ncut.Q        -225.58      60.65  -3.719               0.0002 ***\ncut.C        -699.50      52.78 -13.253 &lt; 0.0000000000000002 ***\ncut^4        -280.36      42.56  -6.588       0.000000000045 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3964 on 53935 degrees of freedom\nMultiple R-squared:  0.01286,   Adjusted R-squared:  0.01279 \nF-statistic: 175.7 on 4 and 53935 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n5.6.1.1 a)\nIn Model 1, are the higher quality cuts, (L, Q, C, ^4) expected to be more or less expensive than fair cuts? Is this surprising? Why or why not?\nNow, we include carat size in the model.\nModel 2:\n\nM_Diamond_price_cut_carat &lt;- lm(data=diamonds, price~cut + carat)\nsummary(M_Diamond_price_cut_carat)\n\n\nCall:\nlm(formula = price ~ cut + carat, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17540.7   -791.6    -37.6    522.1  12721.4 \n\nCoefficients:\n            Estimate Std. Error  t value             Pr(&gt;|t|)    \n(Intercept) -2701.38      15.43 -175.061 &lt; 0.0000000000000002 ***\ncut.L        1239.80      26.10   47.502 &lt; 0.0000000000000002 ***\ncut.Q        -528.60      23.13  -22.851 &lt; 0.0000000000000002 ***\ncut.C         367.91      20.21   18.201 &lt; 0.0000000000000002 ***\ncut^4          74.59      16.24    4.593           0.00000437 ***\ncarat        7871.08      13.98  563.040 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1511 on 53934 degrees of freedom\nMultiple R-squared:  0.8565,    Adjusted R-squared:  0.8565 \nF-statistic: 6.437e+04 on 5 and 53934 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n5.6.1.2 b)\nAfter accounting for carat size are the higher quality cuts, (L, Q, C, ^4) expected to be more or less expensive than fair cuts? How does this compare to your answer from (a) Why do you think this happens? What do we see in the graphs that explains this behavior?\n\n\n\n13)\nA correlation matrix and plot between different quantitative variables in the diamonds dataset is shown below.\n\nlibrary(corrplot)\nDiamonds_Num &lt;- select_if(diamonds, is.numeric)\nC &lt;- cor(Diamonds_Num, use = \"pairwise.complete.obs\")\nround(C,2)\n\n      carat depth table price     x     y    z\ncarat  1.00  0.03  0.18  0.92  0.98  0.95 0.95\ndepth  0.03  1.00 -0.30 -0.01 -0.03 -0.03 0.09\ntable  0.18 -0.30  1.00  0.13  0.20  0.18 0.15\nprice  0.92 -0.01  0.13  1.00  0.88  0.87 0.86\nx      0.98 -0.03  0.20  0.88  1.00  0.97 0.97\ny      0.95 -0.03  0.18  0.87  0.97  1.00 0.95\nz      0.95  0.09  0.15  0.86  0.97  0.95 1.00\n\ncorrplot(C)\n\n\n\n\n\n\n\n\nSuppose we add the variable x (a measure of the diamond’s width) to the model. State whether the following quantities would be expected to increase, decrease, or stay about the same.\nA. The standard error associated with the carat variable.\nB. The t-statistic associated with the carat variable.\nC. The p-value associated with the carat variable.\nD. The \\(R^2\\) value.\nE. The predicted price for a diamond of Very Good cut, with 1 carat and x=5.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Building and Assessing Models</span>"
    ]
  },
  {
    "objectID": "Ch6.html",
    "href": "Ch6.html",
    "title": "6  Logistic Regression",
    "section": "",
    "text": "Learning Outcomes\nConceptual Learning Outcomes\n24. Calculate probabilities and odds using logistic regression.\n25. Interpret logistic regression coefficients in context.\nComputational Learning Outcomes\nJ. Perform logistic regression in R.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Ch6.html#logistic-regression",
    "href": "Ch6.html#logistic-regression",
    "title": "6  Logistic Regression",
    "section": "6.1 Logistic Regression",
    "text": "6.1 Logistic Regression\n\n6.1.1 Modeling Binary Response\nSo far, we have modeled only quantitative response variables. The normal error regression model makes the assumption that the response variable is normally distributed, given the value(s) of the explanatory variables.\nNow, we’ll look at how to model a categorical response variable. We’ll consider only situations where the response is binary (i.e. has 2 categories). Problems with categorical response variables are sometimes called classification problems, while problems with numeric response variables are sometimes called regression problems.\n\n\n6.1.2 Credit Card Dataset\nWe’ll work with a dataset pertaining to 10,000 credit cards. The goal is to predict whether or not the user will default on the payment, using information on the credit card balance, user’s annual income, and whether or not the user is a student. Data come from Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani.\n\nlibrary(ISLR)\ndata(Default)\nsummary(Default)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\n\nDefault and Balance\nThe plot displays each person’s credit card balance on the x-axis, and whether or not they defaulted (a 0 or 1) on the y-axis.\n\nggplot(data=Default, aes(y=default, x=balance)) + geom_jitter(alpha=0.2, height=0.05) + \n  theme_bw()\n\n\n\n\n\n\n\n\nWe see that defaults are rare when the balance is less than $1,000, and more common for balances above $2,000.\nWe’ll first try fitting a linear regression model to the data to try to estimate the probability of a person defaulting on a loan, using the size of their balance as the explanatory variable.\n\n#convert default from yes/no to 0/1\nDefault &lt;- Default |&gt; mutate(default = as.numeric(default==\"Yes\")) \n\n\nggplot(data=Default, aes(y=default, x= balance)) + geom_jitter(alpha=0.2, height=0.05)  + \n  stat_smooth(method=\"lm\", se=FALSE)  + theme_bw()\n\n\n\n\n\n\n\n\nThere are a lot of problems with this model!\nIt allows the estimated probability of of default to be negative. It also assumes a linear trend that doesn’t seem to fit the data very well.\nA sigmoidal curve, like the one below, seems like a better model for default probabilities. This curve stays between 0 and 1, and its curved nature seems like a better fit for the data.\n\n\n6.1.3 Logistic Regression Model\nA logistic regression model uses a sigmoidal curve like the one we saw to model default probabilities, using balance as an explanatory variable.\nThe model makes use of the function\n\\[ f(x) = \\frac{e^x}{1+x^x}, \\]\nwhose graph is shown below. This function is called an inverse logit function.\n\n\n\n\n\n\n\n\n\nStarting with our linear model \\(E(Y_i) = \\beta_0+\\beta_1x_{i1}\\), we need to transform \\(\\beta_0+\\beta_1x_{i1}\\) into the interval (0,1).\n\nLet \\(\\pi_i = \\frac{e^{\\beta_0+\\beta_1x_{i1} }}{1+e^{\\beta_0+\\beta_1x_{i1}}}\\).\nThen \\(0 \\leq \\pi_i \\leq 1\\), and \\(\\pi_i\\) represents an estimate of \\(P(Y_i=1)\\).\nThis function maps the values of \\(\\beta_0+\\beta_1x_{i1}\\) into the interval (0,1).\n\n\n\n\n\n\n\n\n\n\nThe logistic regression model assumes that:\n\n\n\\(Y_i \\in \\{0,1\\}\\)\n\n\\(E(Y_i) = P(Y_i=1) = \\pi_i=\\frac{e^{\\beta_0+\\beta_1x_{i1} + \\ldots \\beta_px_{ip}}}{1+e^{\\beta_0+\\beta_1x_{i1} + \\ldots \\beta_px_{ip}}}\\) i.e. \\(\\beta_0+\\beta_1x_{i1} + \\ldots \\beta_px_{ip}= \\text{log}\\left(\\frac{\\pi_i}{1-\\pi_i}\\right).\\) (This is called the logit function and can be written \\(\\text{logit}(\\pi_i)\\).\n\nInstead of assuming that the expected response is a linear function of the explanatory variables, we are assuming that it is a function of a linear function of the explanatory variables.\nWe fit the logistic curve to the credit card data.\n\nggplot(data=Default, aes(y=default, x= balance)) + geom_jitter(alpha=0.2, height=0.05) + \n  stat_smooth(method=\"glm\", se=FALSE, method.args = list(family=binomial)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nTo fit the logistic regression model in R, we use the function glm, instead of lm. The function is specified the same way as before, and we add family = binomial(link = \"logit\").\n\nCCDefault_M &lt;- glm(data=Default, default ~ balance, family = binomial(link = \"logit\"))\nsummary(CCDefault_M)\n\n\nCall:\nglm(formula = default ~ balance, family = binomial(link = \"logit\"), \n    data = Default)\n\nCoefficients:\n               Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept) -10.6513306   0.3611574  -29.49 &lt;0.0000000000000002 ***\nbalance       0.0054989   0.0002204   24.95 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n\n\nThe regression equation is:\n\\[ P(\\text{Default}) = \\hat{\\pi}_i = \\frac{e^{-10.65+0.0055\\times\\text{balance}}}{1+e^{-10.65+0.0055\\times\\text{balance}}} \\]\nPredictions\n\nFor a $1,000 balance, the estimated default probability is \\(\\frac{e^{-10.65+0.0055(1000) }}{1+e^{-10.65+0.0055(1000)}} \\approx 0.006\\)\nFor a $1,500 balance, the estimated default probability is \\(\\frac{e^{-10.65+0.0055(1500) }}{1+e^{-10.65+0.0055(1500)}} \\approx 0.08\\)\nFor a $2,000 balance, the estimated default probability is \\(\\frac{e^{-10.65+0.0055(2000) }}{1+e^{-10.65+0.0055(2000)}} \\approx 0.59\\)\n\nWe confirm these, using the predict command in R.\n\npredict(CCDefault_M, newdata=data.frame((balance=1000)), type=\"response\")\n\n          1 \n0.005752145 \n\n\n\npredict(CCDefault_M, newdata=data.frame((balance=1500)), type=\"response\")\n\n         1 \n0.08294762 \n\n\n\npredict(CCDefault_M, newdata=data.frame((balance=2000)), type=\"response\")\n\n        1 \n0.5857694 \n\n\n\nWhere do the b’s come from?\nRecall that for a quantitative response variable, the values of \\(b_1, b_2, \\ldots, b_p\\) are chosen in a way that minimizes \\(\\displaystyle\\sum_{i=1}^n \\left(y_i-(\\beta_0+\\beta_1x_{i1}+\\ldots+\\beta_px_{ip})^2\\right)\\). Least squares does not work well in this generalized setting. Instead, the b’s are calculated using a more advanced technique, known as maximum likelihood estimation. We won’t say anything more about that topic here, but it is a prominent technique, widely used in statistic modeling. It is explored in more detail in advanced statistics courses, such as STAT 445:Mathematical Statistics.\n\n\n\n\n6.1.4 Odds and Odds Ratio\nFor an event with probability \\(p\\), the odds of the event occurring are \\(\\frac{p}{1-p}\\).\nExamples: 1. The odds of a fair coin landing heads are \\(\\frac{0.5}{1-0.5}=1\\), sometimes written 1:1.\n\nThe odds of a fair 6-sided die landing on a 1 are \\(\\frac{1/6}{1-1/6}=\\frac{1}{5}\\), sometimes written 1:5.\n\nIn the credit card example, the odds of default are:\n\nFor a $1,000 balance - odds of default are \\(\\frac{0.00575}{1-0.00575} \\approx 1:173.\\)\nFor a $1,500 balance - odds of default are \\(\\frac{0.0829}{1-0.0829 } \\approx 1:11.\\)\nFor a $2,000 balance - odds of default are \\(\\frac{0.586}{1-0.586} \\approx 1.414:1.\\)\n\nThe quantity \\(\\frac{\\frac{\\pi_i}{1-\\pi_i}}{\\frac{\\pi_j}{1-\\pi_j}}\\) is called the odds ratio and represents the odds ratio of a default for user \\(i\\), compared to user \\(j\\).\nExample:\nThe default odds ratio for a $1,000 payment, compared to a $2,000 payment is\nThe odds ratio is \\(\\frac{\\frac{1}{173}}{\\frac{1.414}{1}}\\approx 1:244.\\)\nThe odds of a default are about 244 times larger for a $2,000 payment than a $1,000 payment.\n\n\n6.1.5 Interpretation and of \\(\\beta_1\\)\nConsider the odds ratio for a case \\(j\\) with explanatory variable \\(x + 1\\), compared to case \\(i\\) with explanatory variable \\(x\\).\nThat is \\(\\text{log}\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\beta_0+\\beta_1x\\), and \\(\\text{log}\\left(\\frac{\\pi_j}{1-\\pi_j}\\right) = \\beta_0+\\beta_1(x+1)\\).\n\\(\\text{log}\\left(\\frac{\\frac{\\pi_j}{1-\\pi_j}}{\\frac{\\pi_i}{1-\\pi_i}}\\right)=\\text{log}\\left(\\frac{\\pi_j}{1-\\pi_j}\\right)-\\text{log}\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)=\\beta_0+\\beta_1(x+1)-(\\beta_0+\\beta_1(x))=\\beta_1.\\)\nFor every 1-unit increase in \\(x\\) we expect the log odds of “success” to multiply by a factor of \\(\\beta_1\\).\nFor every 1-unit increase in \\(x\\) we expect the odds of “success” to multiply by a factor of \\(e^{\\beta_1}\\).\nInterpretation in Credit Card Example\n\\(b_1=0.0055\\)\nFor each 1-dollar increase in balance on the credit card., the odds of default are estimated to multiply by \\(e^{0.0055}\\approx1.0055\\).\nThat is, for each additional dollar on the card balance, the odds of default are estimated to increase by 0.55%\nFor each increase of \\(d\\) dollars in credit card balance, odds of default are estimated to multiply by a factor of \\(e^{0.0055d}\\).\nFor every $1,000 increase in balance, the odds of default are expected to multiply by a factor of \\(e^{0.0055\\times 1000}\\approx 244\\).\nThus, the odds of default for a balance of $2,000 are estimated to be \\(e^{0.0055\\times 1000}\\approx 244\\) times as great as the odds of default for a $1,000 balance. This matches our result when we actually calculated out the probabilities and odds.\nHypothesis test for \\(\\beta_1=0\\)\nThe p-value on the “balance” line of the regression output is associated with the null hypothesis \\(\\beta_1=0\\), that is that there is no relationship between balance and the odds of defaulting on the payment.\nThe fact that the p-value is so small tells us that there is strong evidence of a relationship between balance and odds of default.\nConfidence Intervals for \\(\\beta_1\\)\n\nconfint(CCDefault_M, level = 0.95)\n\n                    2.5 %       97.5 %\n(Intercept) -11.383288936 -9.966565064\nbalance       0.005078926  0.005943365\n\n\nWe are 95% confident that for each 1 dollar increase in credit card balance, the odds of default are expected to multiply by a factor between \\(e^{0.00508}\\approx 1.0051\\) and \\(e^{0.00594}\\approx 1.0060\\).\nThis is a profile-likelihood interval, which you can read more about here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Ch6.html#multiple-logistic-regression",
    "href": "Ch6.html#multiple-logistic-regression",
    "title": "6  Logistic Regression",
    "section": "6.2 Multiple Logistic Regression",
    "text": "6.2 Multiple Logistic Regression\n\n6.2.1 Logistic Regression Models with Multiple Explanatory Variables\nWe can also perform logistic regression in situations where there are multiple explanatory variables. We’ll estimate probability of default, using both balance and whether or not the person is a student (a categorical variable) as explanatory variables.\n\nCCDefault_M2 &lt;- glm(data=Default, default ~ balance + student, family = binomial(link = \"logit\"))\nsummary(CCDefault_M2)\n\n\nCall:\nglm(formula = default ~ balance + student, family = binomial(link = \"logit\"), \n    data = Default)\n\nCoefficients:\n               Estimate  Std. Error z value             Pr(&gt;|z|)    \n(Intercept) -10.7494959   0.3691914 -29.116 &lt; 0.0000000000000002 ***\nbalance       0.0057381   0.0002318  24.750 &lt; 0.0000000000000002 ***\nstudentYes   -0.7148776   0.1475190  -4.846           0.00000126 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.7  on 9997  degrees of freedom\nAIC: 1577.7\n\nNumber of Fisher Scoring iterations: 8\n\n\nThe following graph gives an illustration of the model.\n\nggplot(data=Default, aes(y=default, x= balance, color=student)) + geom_jitter(alpha=0.2, height=0.05) + stat_smooth(method=\"glm\", se=FALSE, method.args = list(family=binomial)) + theme_bw()\n\n\n\n\n\n\n\n\nThe regression equation is:\n\\[\nP(\\text{Default}) = \\hat{\\pi}_i = \\frac{e^{-10.75+0.005738\\times\\text{balance}-0.71\\times\\text{student}_i}}{1+e^{-10.75+0.0057\\times\\text{balance}-0.71\\times\\text{student}_i}}\n\\]\n\nFor each 1 dollar increase in balance, the odds of default are estimated to multiply by a factor \\(e^{0.0057}\\approx 1.00575\\), assuming whether or not the person is a student is held constant. Thus, the estimated odds of default increase by about 0.5%, for each 1-dollar increase in balance..\nFor every $100 increase in balance, the odds of default are estimated to multiply by \\(e^{0.0057\\times100}\\approx 1.775\\), assuming whether or not the person is a student is held constant. Thus, the estimated odds of default increase by about 77.5%.\nThe odds of default for students are estimated to be \\(e^{-0.71} \\approx 0.49\\) as high for students as non-students, assuming balance amount is held constant.\n\nHypothesis Tests in Multiple Logistic Regression Model\n\nSince the p-value associated with balance is very small, there is strong evidence of a relationship between balance and odds of default, after accounting for whether or not the person is a student.\nSince the p-value associated with StudentYes is very small, there is strong evidence that students are less likely to default than nonstudents, provided the balance on the card is the same.\n\n\n\n6.2.2 Multiple Logistic Regression Model with Interaction\nThe previous model assumes the effect of balance on default probability is the same for students as for nonstudents. If we suspect that the effect of having a larger balance might be different for students than for nonstudents, then we could use a model with interaction between the balance and student variables.\n\nCCDefault_M_Int &lt;- glm(data=Default, default ~ balance * student, family = binomial(link = \"logit\"))\nsummary(CCDefault_M_Int)\n\n\nCall:\nglm(formula = default ~ balance * student, family = binomial(link = \"logit\"), \n    data = Default)\n\nCoefficients:\n                      Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        -10.8746818   0.4639679 -23.438 &lt;0.0000000000000002 ***\nbalance              0.0058188   0.0002937  19.812 &lt;0.0000000000000002 ***\nstudentYes          -0.3512310   0.8037333  -0.437               0.662    \nbalance:studentYes  -0.0002196   0.0004781  -0.459               0.646    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.5  on 9996  degrees of freedom\nAIC: 1579.5\n\nNumber of Fisher Scoring iterations: 8\n\n\nInterpretations for Logistic Model with Interaction\n\nThe regression equation is:\n\n\\[\nP(\\text{Default}) = \\hat{\\pi}\\_i = \\frac{e^{-10.87+0.0058\\times\\text{balance}_i-0.35\\times\\text{student}_i-0.0002\\times\\text{balance}_i\\times{\\text{student}_i}}}{1+e^{-10.87+0.0058\\times\\text{balance}_i-0.35\\times\\text{student}_i-0.0002\\times\\text{balance}_i\\times{\\text{student}_i}}}\n\\]\nEquation for Students\n\\[\nP(\\text{Default}) = \\hat{\\pi}\\_i = \\frac{e^{-10.52+0.0056\\times\\text{balance}}}{1+e^{-10.52+0.0056\\times\\text{balance}}}\n\\]\nAssuming a person is a student, for every $100 increase in balance, the odds of default are expected to multiply by a factor of \\(e^{0.0056\\times 100}=1.75\\), a 75% increase.\nEquation for Non-Students\n\\[\nP(\\text{Default}) = \\hat{\\pi}\\_i = \\frac{e^{-10.87+0.0058\\times\\text{balance}}}{1+e^{-10.87+0.0058\\times\\text{balance}}}\n\\]\nAssuming a person is a student, for every $100 increase in balance, the odds of default are expected to multiply by a factor of \\(e^{0.0058\\times 100}=1.786\\), a 78.6% increase.\n\nSince estimate of the interaction effect is so small and the p-value on this estimate is large, it is plausible that there is no interaction at all. Thus, the simpler non-interaction model is preferable.\n\n\n\n6.2.3 Logistic Regression Key Points\n\n\\(Y\\) is a binary response variable.\n\\(\\pi_i\\) is a function of explanatory variables \\(x_{i1}, \\ldots x_{ip}\\).\n\\(E(Y_i) = \\pi_i = \\frac{e^{\\beta_0+\\beta_1x_i + \\ldots\\beta_px_{ip}}}{1+e^{\\beta_0+\\beta_1x_i + \\ldots\\beta_px_{ip}}}\\)\n\\(\\beta_0+\\beta_1x_i + \\ldots\\beta_px_{ip} = \\text{log}\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)\\)\nFor quantitative \\(x_j\\), when all other explanatory variables are held constant, the odds of “success” multiply be a factor of \\(e^{\\beta_j}\\) for each 1 unit increase in \\(x_j\\)\nFor categorical \\(x_j\\), when all other explanatory variables are held constant, the odds of “success” are \\(e^{\\beta_j}\\) times higher for category \\(j\\) than for the “baseline category.”\nFor models with interaction, we can only interpret \\(\\beta_j\\) when the values of all other explanatory variables are given (since the effect of \\(x_j\\) depends on the other variables.)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Ch6.html#practice-questions",
    "href": "Ch6.html#practice-questions",
    "title": "6  Logistic Regression",
    "section": "6.3 Practice Questions",
    "text": "6.3 Practice Questions\n\n1)\nIn which of the following situations would it be appropriate to use a logistic regression model? Choose all that apply.\nA. We want to predict whether or not it will rain on a given day, using information on temperature, wind speed, and amount of cloud cover.\nB. We want to predict the number of hours a person will sleep in a given night, based on whether or not they take medication designed to improve sleep quality.\nC. We want to predict whether the incumbent party will win an election, using information on unemployment and inflation rates, and stock market growth.\nD. We want to predict whether a person will use the term “Pop” or “Soda” based on which state they grew up in.\nE. We want to predict whether or not a person will get into a college, using information including their high school GPA, standardized test scores, and whether or not they participated in certain activities.\nF. We want to predict the number of people crossing a street in a given hour, using information about the time of the day, the day of the week, and the temperature.\nG. We want to predict whether or not a sports team will win a game, using information on the number of games they’ve won and lost previously, as well as the number of games their opponent has won and lost previously, as well as the strength of opponents that they and their opponent have played against in prior games.\n\n\n2)\nWe’ll work with a dataset on admissions to graduate school. We have data on a student’s graduate records exam (GRE) test score out of 800, and whether or not they attended a top-tier undergraduate college. Data are from (https://stats.idre.ucla.edu/stat/data/binary.csv). The response variable is whether or not the student was admitted to the graduate program (1=Yes, 0=No).\nThe dataset contains the following variables.\ngre - score on the GRE exam\nTopTier - whether or not the student attended a top tier undergraduate institution\nadmit - whether or not the student was admitted to the program (1=yes, 0=no)\nA graph showing whether or not a person was admitted to graduate school along with their gre score and whether or not they attended a top tier undergraduate college is shown below. The curves estimating admission probability using logistic regression are also shown.\n\nggplot(data=Grad, aes(y=admit, x= gre, color=TopTier)) + geom_jitter(alpha=0.2, height=0.05) + stat_smooth(method=\"glm\", se=FALSE, method.args = list(family=binomial)) + theme_bw()\n\n\n\n\n\n\n\n\nWe fit a logistic regression model with admit as the response variable and gre and TopTier as explanatory variables.\nR output for the model is shown below.\n\nM_Grad_admit_gre_top &lt;- glm(data=Grad, admit ~ gre + TopTier, family = binomial(link = \"logit\"))\nsummary(M_Grad_admit_gre_top)\n\n\nCall:\nglm(formula = admit ~ gre + TopTier, family = binomial(link = \"logit\"), \n    data = Grad)\n\nCoefficients:\n             Estimate Std. Error z value   Pr(&gt;|z|)    \n(Intercept) -2.960822   0.617269  -4.797 0.00000161 ***\ngre          0.003377   0.001004   3.363   0.000770 ***\nTopTierTRUE  1.065127   0.289486   3.679   0.000234 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 472.56  on 397  degrees of freedom\nAIC: 478.56\n\nNumber of Fisher Scoring iterations: 4\n\n\nExponentiated regression coefficients are shown below.\n\nexp(M_Grad_admit_gre_top$coefficients)\n\n(Intercept)         gre TopTierTRUE \n 0.05177633  1.00338305  2.90120814 \n\n\n\na)\nGive interpretations of each of the three numbers in the Estimate column of the coefficient table, in context. Give your answer in rounded numeric form. (i.e. don’t put \\(e^\\#\\).)\n\n\nb)\nWrite a sentence interpreting the confidence intervals on the “gre” and “HighlyRankedTRUE” lines in context. Give answers in rounded numeric form. (i.e. don’t put \\(e^\\#\\).)\n\nexp(confint(M_Grad_admit_gre_top, level = 0.95))\n\n                 2.5 %    97.5 %\n(Intercept) 0.01494934 0.1688771\ngre         1.00143970 1.0053977\nTopTierTRUE 1.64705190 5.1438511\n\n\n\n\nc)\nDo the data provide evidence that higher GRE scores are associated with increased chance of being admitted to graduate school, for students attending similar institutions? Cite a p-value to support your answer.\n\n\nd)\nDo the data provide evidence that attending a highly ranked institution improves the chances for graduate school admission for students of the same GRE score?\n\n\n\n3)\nContinue with the R output from Question 2.\n\na)\nEstimate the admission probability of a student who attended a top-tier institution and had a GRE score of 575. Give your answer as a probability in rounded numeric form. (i.e. don’t put \\(e^\\#\\).)\n\n\nb)\nEstimate the admission probability of a student who did not attend a top-tier institution and had a GRE score of 650. Give your answer as a probability in rounded numeric form. (i.e. don’t put \\(e^\\#\\).)\n\n\nc)\nIf two students attend the same institution, and one scores 50 points higher on the GRE, how many times greater are the odds of that student to be admitted to the graduate program, compared to the student with the lower GRE score? Give your answer in rounded numeric form. (i.e. don’t put \\(e^\\#\\).)\n\n\nd)\nIf two students achieve the same GRE score, and one attended a top-tier institution, while the other did not, how many times greater are the odds of the student who attended the highly ranked institution being accepted to grad school, compared with the one who did not? Give your answer in rounded numeric form. (i.e. don’t put \\(e^\\#\\).)\n\n\ne)\nSuppose that, according to the above model, Student A is estimated to have a 25% chance of getting into the the graduate program. Student B goes to the same school and scores 55 points higher on the GRE. Approximately, what is the estimated probability of student B getting into the graduate program?\nHint: The following calculation may be helpful.\n\nexp(0.003377*55)\n\n[1] 1.204103\n\n\n\n\n\n4)\nNow, consider the logistic regression model resulting from the following R command.\n\nM_Grad_admit_gre_top_int &lt;- glm(data=Grad, admit ~ gre * TopTier, family = binomial(link = \"logit\"))\nsummary(M_Grad_admit_gre_top_int)\n\n\nCall:\nglm(formula = admit ~ gre * TopTier, family = binomial(link = \"logit\"), \n    data = Grad)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -2.8825292  0.6839726  -4.214 0.000025 ***\ngre              0.0032473  0.0011183   2.904  0.00369 ** \nTopTierTRUE      0.6630025  1.5695442   0.422  0.67272    \ngre:TopTierTRUE  0.0006627  0.0025429   0.261  0.79439    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 472.49  on 396  degrees of freedom\nAIC: 480.49\n\nNumber of Fisher Scoring iterations: 4\n\n\n\na)\nExplain, in context, how this model is different than the one in Questions 2 and 3. (Don’t just say that it contains an interaction, but rather, explain what that means in context.)\n\n\nb)\nDo the data provide evidence that gre score affects the probability of successful grad school admission differently for students at top tier schools than at other schools? Justify your answer.\n\n\nc)\nWhich model should we use (this one or the one in Question 2) to make inferences? Explain your answer.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "Ch7.html",
    "href": "Ch7.html",
    "title": "7  Predictive Modeling",
    "section": "",
    "text": "Learning Outcomes\nConceptual Learning Outcomes\n26. Explain how model complexity relates to training and test error, prediction variance and bias, and overfitting. \n27. Explain how to use cross-validation in model selection. \n28. Explain how complexity parameters associated with ridge regression, decision trees, and splines impact variance, bias, and likelihood of overfitting. \n29. Make predictions from a decision tree. \n30. Calculate classification accuracy, sensitivity, specificity, confusion matrix, and receiver operating characteristic curves.\n31. Assess ethical considerations associated with predictive models in context.\nComputational Learning Outcomes\nK. Perform cross-validation to build and assess predictive models in R.\nL. Make predictions on new data using predictive models in R.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#modeling-for-prediction",
    "href": "Ch7.html#modeling-for-prediction",
    "title": "7  Predictive Modeling",
    "section": "7.1 Modeling for Prediction",
    "text": "7.1 Modeling for Prediction\n\n7.1.1 Overview\nWe’ve previously learned how to build models for the purpose of interpretation, when our primary focus is on understanding relationships between variables in the model. In this chapter, we’ll examine how to build models for situations when we are not interested in understanding relationships between variables, and instead care only about making the most accurate predictions possible.\nWe’ve seen that when we model for interpretation, we encounter a tradeoff between model complexity and interpretability. We wanted to choose a model that is complex enough to reasonably approximate the structure of the underlying data, but at the same time, not so complicated that it becomes hard to interpret. When modeling for prediction, we don’t need to worry about interpretability, which can sometimes make more complex models more desirable. Nevertheless, we’ll encounter a different kind of tradeoff, involving model complexity, that we’ll have to think about, and we’ll see that more complex models do not always lead to better predictions.\nPredictive Modeling Vocabulary\n\nThe new data on which we make predictions are called test data.\nThe data used to fit the model are called training data.\n\nIn the training data, we know the values of the explanatory and response variables. In the test data, we know only the values of the explanatory variables and want to predict the values of the response variable.\n\n\n7.1.2 Illustration of Predictive Modeling\nThe illustration shows observations from a simulated dataset consisting of 100 observations of a single explanatory variable \\(x\\), and response variable \\(y\\). We want to find a model that captures the trend in the data and will be best able to predict new values of y, for given x.\n\n\n\n\n\n\n\n\n\nWe’ll fit several different polynomial models to the data, increasing in complexity from the most simple model we could possibly use, a constant model, to a very complex eighth degree polynomial model.\nConstant Model to Sample Data\n\n\n\n\n\n\n\n\n\nWe see that the flexibility of the model increases as we add higher-order terms. The curve is allowed to have more twists and bends. For higher-order, more complex models, individual points have more influence on the shape of the curve. This can be both a good and bad thing, as it allows the model to better bend and fit the data, but also makes it susceptible to the influence of outliers.\n\n\n7.1.3 Predicting New Data\nNow, suppose we have a new dataset of 100, x-values, and want to predict \\(y\\). The first 5 rows of the new dataset are:\n\n\n[1] 5.5 2.1 3.2 2.1 2.9 7.6\n\n\nWe fit polynomial models of degree 0 through 8 to the data.\n\nSim_M0 &lt;-lm(data=Sampdf, y~1)\nSim_M1 &lt;-lm(data=Sampdf, y~x)\nSim_M2 &lt;- lm(data=Sampdf, y~x+I(x^2))\nSim_M3 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3))\nSim_M4 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4))\nSim_M5 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5))\nSim_M6 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6))\nSim_M7 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7))\nSim_M8 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8))\n\nWe predict the values of the new observations, using each of the 9 models.\n\nNewdf$Deg0Pred &lt;- predict(Sim_M0, newdata=Newdf)\nNewdf$Deg1Pred &lt;- predict(Sim_M1, newdata=Newdf)\nNewdf$Deg2Pred &lt;- predict(Sim_M2, newdata=Newdf)\nNewdf$Deg3Pred &lt;- predict(Sim_M3, newdata=Newdf)\nNewdf$Deg4Pred &lt;- predict(Sim_M4, newdata=Newdf)\nNewdf$Deg5Pred &lt;- predict(Sim_M5, newdata=Newdf)\nNewdf$Deg6Pred &lt;- predict(Sim_M6, newdata=Newdf)\nNewdf$Deg7Pred &lt;- predict(Sim_M7, newdata=Newdf)\nNewdf$Deg8Pred &lt;- predict(Sim_M8, newdata=Newdf)\n\nIn fact, since these data were simulated, we know the true value of \\(y\\), so we can compare the predicted values to the true ones.\n\nprint(Newdf %&gt;% dplyr::select(-c(samp)) %&gt;% round(1) %&gt;% head(5))\n\n       x    y Deg0Pred Deg1Pred Deg2Pred Deg3Pred Deg4Pred Deg5Pred Deg6Pred\n108  5.5  5.5      1.2      1.1      0.4     -0.1     -0.3      0.1      0.4\n4371 2.1  3.9      1.2      1.9      2.0      3.7      3.9      4.3      3.8\n4839 3.2  1.5      1.2      1.6      1.3      3.2      3.4      2.8      2.2\n6907 2.1  6.8      1.2      1.9      2.0      3.7      4.0      4.2      3.8\n7334 2.9 -1.0      1.2      1.7      1.4      3.4      3.6      3.1      2.5\n     Deg7Pred Deg8Pred\n108       0.3      0.3\n4371      3.4      3.5\n4839      2.2      2.1\n6907      3.4      3.5\n7334      2.3      2.2\n\n\n\n\n7.1.4 Prediction Error\nFor quantitative response variables, we can evaluate the predictions by calculating the average of the squared differences between the true and predicted values. Often, we look at the square root of this quantity. This is called the Root Mean Square Prediction Error (RMSPE).\n\\[\n\\text{RMSPE} = \\sqrt{\\displaystyle\\sum_{i=1}^{n'}\\frac{(y_i-\\hat{y}_i)^2}{n'}},\n\\]\nwhere \\(n'\\) represents the number of new cases being predicted.\nWe calculate RMSPE for each of the 9 models.\n\nRMSPE0 &lt;- sqrt(mean((Newdf$y-Newdf$Deg0Pred)^2)) \nRMSPE1 &lt;- sqrt(mean((Newdf$y-Newdf$Deg1Pred)^2))\nRMSPE2 &lt;- sqrt(mean((Newdf$y-Newdf$Deg2Pred)^2))\nRMSPE3 &lt;- sqrt(mean((Newdf$y-Newdf$Deg3Pred)^2))\nRMSPE4 &lt;- sqrt(mean((Newdf$y-Newdf$Deg4Pred)^2))\nRMSPE5 &lt;- sqrt(mean((Newdf$y-Newdf$Deg5Pred)^2))\nRMSPE6 &lt;- sqrt(mean((Newdf$y-Newdf$Deg6Pred)^2))\nRMSPE7 &lt;- sqrt(mean((Newdf$y-Newdf$Deg7Pred)^2))\nRMSPE8 &lt;- sqrt(mean((Newdf$y-Newdf$Deg8Pred)^2))\n\n\n\n\nPrediction error on new data\n\n\nDegree\nRMSPE\n\n\n\n\n0\n4.05\n\n\n1\n3.85\n\n\n2\n3.73\n\n\n3\n3.26\n\n\n4\n3.28\n\n\n5\n3.34\n\n\n6\n3.35\n\n\n7\n3.37\n\n\n8\n3.35\n\n\n\n\n\nThe third degree model did the best at predicting the new data.\nNotice that making the model more complex beyond third degree not only didn’t help, but actually hurt prediction accuracy.\nNow, let’s examine the behavior if we had fit the models to the data, instead of the test data.\n\nRMSE0 &lt;- sqrt(mean(Sim_M0$residuals^2))\nRMSE1 &lt;- sqrt(mean(Sim_M1$residuals^2))\nRMSE2 &lt;- sqrt(mean(Sim_M2$residuals^2))\nRMSE3 &lt;- sqrt(mean(Sim_M3$residuals^2))\nRMSE4 &lt;- sqrt(mean(Sim_M4$residuals^2))\nRMSE5 &lt;- sqrt(mean(Sim_M5$residuals^2))\nRMSE6 &lt;- sqrt(mean(Sim_M6$residuals^2))\nRMSE7 &lt;- sqrt(mean(Sim_M7$residuals^2))\nRMSE8 &lt;- sqrt(mean(Sim_M8$residuals^2))\n\n\n\n\nError on training data\n\n\nDegree\nRMSE\n\n\n\n\n0\n3.43\n\n\n1\n3.37\n\n\n2\n3.30\n\n\n3\n2.91\n\n\n4\n2.91\n\n\n5\n2.84\n\n\n6\n2.81\n\n\n7\n2.80\n\n\n8\n2.80\n\n\n\n\n\n\nDegree &lt;- 0:8\nTest_Error &lt;- c(RMSPE0, RMSPE1, RMSPE2, RMSPE3, RMSPE4, RMSPE5, RMSPE6, RMSPE7, RMSPE8)|&gt; round(2)\nTrain_Error &lt;- c(RMSE0, RMSE1, RMSE2, RMSE3, RMSE4, RMSE5, RMSE6, RMSE7, RMSE8) |&gt; round(2)\nRMSPEdf &lt;- data.frame(Degree, Train_Error, Test_Error)\nRMSPEdf\n\n  Degree Train_Error Test_Error\n1      0        3.43       4.05\n2      1        3.37       3.85\n3      2        3.30       3.73\n4      3        2.91       3.26\n5      4        2.91       3.28\n6      5        2.84       3.34\n7      6        2.81       3.35\n8      7        2.80       3.37\n9      8        2.80       3.35\n\n\nNotice that the most complex model achieves the best performance on the training data, but not on the test data.\nAs the model complexity grows, the model will always fit the training data better, but that does not mean it will perform better on new data. It is possible to start modeling noise, rather than true signal in the training data, which hurts the accuracy of the model when applied to new data.\n\n\n\n\n\n\n\n\n\n\nTraining error decreases as model becomes more complex\n\nTesting error is lowest for the 3rd degree model, then starts to increase again\n\nOf the models we looked at, the third degree model does the best. The estimates of its coefficients are shown below.\n\nSim_M3\n\n\nCall:\nlm(formula = y ~ x + I(x^2) + I(x^3), data = Sampdf)\n\nCoefficients:\n(Intercept)            x       I(x^2)       I(x^3)  \n   -0.54165      4.16638     -1.20601      0.08419  \n\n\nIn fact, the data were generated from the model \\(y_i = 0 + 4.5x  - 1.4x^2 +  0.1x^3 + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,3)\\)\nWe compare the true expected response curve (in yellow) to the estimated curve in blue. We see that they are not identical, but close.\n\n\n\n\n\n\n\n\n\n\n\n7.1.5 Variance Bias Tradeoff\nAs we make a model more complex (such as by adding more variables or higher order polynomial terms), it will always fit the training data better. However, at some point, it will begin to model random noise, rather than true signal in the training data, and thus will perform worse on new data. This is called overfitting.\n\n\n\n\n\n\n\n\n\n\nSuppose \\(Y_i = f(x_i) + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\).\nLet \\(\\hat{f}\\) represent the function of our explanatory variable(s) \\(x^*\\) used to predict the value of response variable \\(y^*\\). Thus \\(\\hat{y}^* = f(x^*)\\).\nPrediction error is given by \\(\\left(y^* - \\hat{y}\\right)^2 = \\left(y^* - \\hat{f}(x^*)\\right)^2\\).\nThere are three factors that contribute to prediction error RMSPE.\n\nModel Bias - Our model might not be complex enough to capture the true relationship between the response and explanatory variable(s). For example, if we use a linear or quadratic model when the true relationship is cubic, our predictions will suffer from model bias. Model bias pertains to the difference between the true response function value \\(f(x^*)\\), and the expected value of \\(\\hat{f}(x^*)\\) that would be obtained in the long run over many samples.\nModel Variance - Individual observations in the training data are subject to random sampling variability. As model complexity increases, it becomes more flexible and prone to being pulled away from the true relationship because of outliers in the training data.\nPrediction Variance - Even if two observations have the same value(s) for every explanatory variable, they will usually have different response values, due to random noise (i.e. the \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\) term). So, even if we really knew the relationship between the response and explanatory variable(s), we would still not get every prediction exactly correct.\n\nWe cannot do anything about prediction variance, but we can try to control model bias and variance through our choice of model. If we could figure out how to minimize bias while also minimizing variance associated with a prediction, that would be great! Unfortunately, this is not possible. As model complexity (flexibility) increases, bias decreases. Variance, however, increases. Thus, we cannot make both of these smallest at the same time. Instead, the goal will be to find a model that is complex enough to keep bias fairly low, while not so complex that variability gets too big.\nIn fact, it can be shown that:\n\\(\\text{Expected RMSPE} = \\text{Variance} + \\text{Bias}^2\\)\nOur goal is the find the “sweetspot” where expected prediction error is minimized.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#cross-validation",
    "href": "Ch7.html#cross-validation",
    "title": "7  Predictive Modeling",
    "section": "7.2 Cross-Validation",
    "text": "7.2 Cross-Validation\n\n7.2.1 Cross-Validation Illustration\nWe’ve seen that training error is not an accurate approximation of test error. Instead, we’ll approximate test error, by setting aside a set of the training data, and using it as if it were a test set. This process is called cross-validation, and the set we put aside is called the validation set.\n\nPartition data into disjoint sets (folds). Approximately 5 folds recommended.\n\nBuild a model using 4 of the 5 folds.\n\nUse model to predict responses for remaining fold.\nCalculate root mean square error \\(RMSPE=\\displaystyle\\sqrt{\\frac{\\sum((\\hat{y}_i-y_i)^2)}{n'}}\\).\n\nRepeat for each of 5 folds.\n\nAverage RMSPE values across folds.\n\nIf computational resources permit, it is often beneficial to perform CV multiple times, using different sets of folds.\n\n\n\n\n\nhttps://www.researchgate.net/figure/A-schematic-illustration-of-K-fold-cross-validation-for-K-5-Original-dataset-shown_fig5_311668395\n\n\n\n\n\n\n7.2.2 Ames Housing Data\nWe’ll use it to compare five different models for house prices among a dataset of 1,000 houses sold in Ames, IA between 2006 and 2010. We have data on 25 different explanatory variables.\nThis is actually a subset of a larger dataset available in the Ames Housing R package.\n\nlibrary(tidyverse)\n#library(AmesHousing)\n#data(\"ames_raw\")\nAmes_Train &lt;- read_csv(\"Ames_Train_Data.csv\")  # Load data\nlibrary(caret)   # load caret package\n\n\nlibrary(AmesHousing)\ndata(\"ames_raw\")\names_raw &lt;- as.data.frame(ames_raw)\names_raw &lt;- ames_raw %&gt;% mutate_if(is.character,as.factor)\nAmes_Num &lt;- select_if(ames_raw, is.numeric)\names_raw &lt;- ames_raw[complete.cases(Ames_Num),]\names_raw &lt;- ames_raw %&gt;% mutate_if(is.character, addNA)\names_raw &lt;- ames_raw %&gt;% mutate_if(is.factor, addNA)\names_raw$PID &lt;- as.numeric(ames_raw$PID)\names_raw &lt;- ames_raw %&gt;% select( `Overall Qual`, `Year Built`, `Mas Vnr Area`, `Central Air`, `Gr Liv Area`, `Lot Frontage`, `1st Flr SF`, `Bedroom AbvGr`, `TotRms AbvGrd`, everything())\names_raw &lt;- ames_raw %&gt;% select(-c(Utilities, `Exterior 2nd`, `Bldg Type`, `Bsmt Cond`, `BsmtFin Type 1`, `Low Qual Fin SF`, `Total Bsmt SF`, `BsmtFin Type 2`, `Bsmt Cond`, `Exterior 1st`, `House Style`))\names_raw &lt;- ames_raw |&gt; mutate_if(is.factor, droplevels)\n\n\n\nset.seed(10302021)\nsamp &lt;- sample(1:nrow(ames_raw))\nAmes_Train &lt;- ames_raw[samp[1:1000],]\nAmes_Test &lt;- ames_raw[samp[1001:2000],]\nAmes_Train &lt;- Ames_Train |&gt; mutate_if(is.factor, droplevels)\nAmes_Test &lt;- Ames_Test |&gt; mutate_if(is.factor, droplevels)\n\n\nhead(Ames_Train)\n\n     Overall Qual Year Built Mas Vnr Area Central Air Gr Liv Area Lot Frontage\n859             5       1972            0           Y         864           36\n1850            7       1997         1600           Y        1950           66\n1301            5       1948            0           Y        1122          100\n981             5       1972            0           Y         796           50\n2694            6       1937            0           Y        1376           50\n2209            5       1938            0           Y         954           50\n     1st Flr SF Bedroom AbvGr TotRms AbvGrd Order  PID MS SubClass MS Zoning\n859         864             3             5   859 2256         020        RL\n1850        975             3             7  1850  976         060        FV\n1301       1122             2             6  1301 1572         020        RM\n981         796             2             4   981 2891         085        RL\n2694        780             3             7  2694 1805         050        RM\n2209        954             2             5  2209 2561         030        RL\n     Lot Area Street Alley Lot Shape Land Contour Lot Config Land Slope\n859     15523   Pave  &lt;NA&gt;       IR1          Lvl    CulDSac        Gtl\n1850     7399   Pave  Pave       IR1          Lvl     Inside        Gtl\n1301    12000   Pave  &lt;NA&gt;       Reg          Lvl     Inside        Gtl\n981      7689   Pave  &lt;NA&gt;       IR1          Lvl     Inside        Gtl\n2694     8600   Pave  &lt;NA&gt;       Reg          Bnk     Inside        Gtl\n2209     6305   Pave  &lt;NA&gt;       Reg          Bnk     Inside        Gtl\n     Neighborhood Condition 1 Condition 2 Overall Cond Year Remod/Add\n859       CollgCr        Norm        Norm            6           1972\n1850      Somerst        Norm        Norm            5           1998\n1301      OldTown        Norm        Norm            7           2005\n981       Mitchel        Norm        Norm            8           1972\n2694       IDOTRR        Norm        Norm            6           1950\n2209      Crawfor        Norm        Norm            7           1950\n     Roof Style Roof Matl Mas Vnr Type Exter Qual Exter Cond Foundation\n859       Gable   CompShg         None         TA         TA     CBlock\n1850        Hip   CompShg      BrkFace         Gd         TA      PConc\n1301      Gable   CompShg         None         TA         TA     CBlock\n981       Gable   CompShg         None         TA         TA     CBlock\n2694      Gable   CompShg         None         TA         TA     BrkTil\n2209      Gable   CompShg         None         TA         Gd      PConc\n     Bsmt Qual Bsmt Exposure BsmtFin SF 1 BsmtFin SF 2 Bsmt Unf SF Heating\n859         TA            Av          460            0         404    GasA\n1850        Gd            No          649            0         326    GasA\n1301        TA            No          144          608         172    GasA\n981         Gd            Av          720           76           0    GasA\n2694        TA            No            0            0         780    GasA\n2209        Fa            No            0            0         920    GasA\n     Heating QC Electrical 2nd Flr SF Bsmt Full Bath Bsmt Half Bath Full Bath\n859          Ex      SBrkr          0              1              0         1\n1850         Ex      SBrkr        975              0              0         2\n1301         Ex      SBrkr          0              1              0         1\n981          Gd      SBrkr          0              0              1         1\n2694         TA      SBrkr        596              0              0         2\n2209         Ex      SBrkr          0              0              0         1\n     Half Bath Kitchen AbvGr Kitchen Qual Functional Fireplaces Fireplace Qu\n859          0             1           TA        Typ          1           Fa\n1850         1             1           Gd        Typ          1           TA\n1301         0             1           Gd        Typ          0         &lt;NA&gt;\n981          0             1           TA        Typ          0         &lt;NA&gt;\n2694         0             1           TA        Typ          1           Gd\n2209         0             1           Fa        Typ          1           Gd\n     Garage Type Garage Yr Blt Garage Finish Garage Cars Garage Area\n859       Attchd          1972           Unf           1         338\n1850      Detchd          1997           RFn           2         576\n1301      Attchd          1948           Unf           2         528\n981       Detchd          1998           Unf           1         336\n2694      Detchd          1937           Unf           1         198\n2209     Basment          1938           Unf           1         240\n     Garage Qual Garage Cond Paved Drive Wood Deck SF Open Porch SF\n859           TA          TA           Y            0             0\n1850          TA          TA           Y            0            10\n1301          TA          TA           Y            0            36\n981           TA          TA           Y          138             0\n2694          TA          TA           N            0             0\n2209          Fa          TA           Y            0             0\n     Enclosed Porch 3Ssn Porch Screen Porch Pool Area Pool QC Fence\n859               0          0            0         0    &lt;NA&gt;  &lt;NA&gt;\n1850              0          0          198         0    &lt;NA&gt;  &lt;NA&gt;\n1301              0          0            0         0    &lt;NA&gt;  GdWo\n981               0          0            0         0    &lt;NA&gt; MnPrv\n2694              0          0            0         0    &lt;NA&gt;  &lt;NA&gt;\n2209              0          0            0         0    &lt;NA&gt; MnPrv\n     Misc Feature Misc Val Mo Sold Yr Sold Sale Type Sale Condition SalePrice\n859          &lt;NA&gt;        0       8    2009       WD          Normal    133500\n1850         &lt;NA&gt;        0       6    2007       WD          Normal    239000\n1301         &lt;NA&gt;        0       5    2008       WD          Normal    147000\n981          &lt;NA&gt;        0       7    2009       WD          Normal    131900\n2694         &lt;NA&gt;        0       6    2006       WD          Normal    119500\n2209         &lt;NA&gt;        0       6    2007       WD          Normal    119750\n\n\n\n\n7.2.3 Cross-Validation with caret\nThe train function in the caret R package performs cross validation automatically. number represents the number of folds, and repeats is the number of repetitions.\nWe’ll consider six different models of increasing complexity.\n\nlibrary(caret)  \n# set cross-validation settings - use 10 repeats of 10-fold CV\ncontrol &lt;- trainControl(method=\"repeatedcv\", number=10, repeats=10, savePredictions = \"all\" )\n\n# define models\n# set same random seed before each model to ensure same partitions are used in CV, making them comparable\n\nset.seed(10302023)   \nmodel1 &lt;- train(data=Ames_Train, \n                SalePrice ~ `Overall Qual` ,  \n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel2 &lt;- train(data=Ames_Train, \n                SalePrice ~ `Overall Qual` +  `Gr Liv Area` + `Garage Area`,  \n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel3 &lt;- train(data=Ames_Train, SalePrice ~ `Overall Qual` + \n                  `Gr Liv Area` + `Garage Area` + \n                  `Neighborhood` + `Year Built`,  \n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel4 &lt;- train(data=Ames_Train, SalePrice ~ `Overall Qual` \n                + `Gr Liv Area`  + `Garage Area` \n                + `Neighborhood` + `Year Built` \n                + `Lot Shape` + `Land Contour` + `Land Slope`,\n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel5 &lt;- train(data=Ames_Train, SalePrice ~ `Overall Qual` \n                + `Gr Liv Area`  + `Garage Area` \n                + `Neighborhood` + `Year Built` \n                 + `Lot Shape` + `Land Contour` + `Land Slope` +\n                 + `Order` + `PID` ,\n                method=\"lm\", trControl=control)\n\nset.seed(10302023) \nmodel6 &lt;- train(data=Ames_Train, SalePrice ~ .,  method=\"lm\", trControl=control)  # include everything linearly\n\nCross validation root mean square error is shown below. This is an estimate of the root mean square prediction error we would see when making predictions on new data.\n\n# Calculate RMSPE for each model\nCVRMSE1 &lt;- sqrt(mean((model1$pred$obs-model1$pred$pred)^2))\nCVRMSE2 &lt;- sqrt(mean((model2$pred$obs-model2$pred$pred)^2))\nCVRMSE3 &lt;- sqrt(mean((model3$pred$obs-model3$pred$pred)^2))\nCVRMSE4 &lt;- sqrt(mean((model4$pred$obs-model4$pred$pred)^2))\nCVRMSE5 &lt;- sqrt(mean((model5$pred$obs-model5$pred$pred)^2))\nCVRMSE6 &lt;- sqrt(mean((model6$pred$obs-model6$pred$pred)^2))\n\nCVRMSE1\n\n[1] 49605.77\n\nCVRMSE2\n\n[1] 43333.4\n\nCVRMSE3\n\n[1] 39057.19\n\nCVRMSE4\n\n[1] 39287.21\n\nCVRMSE5\n\n[1] 39340.58\n\nCVRMSE6\n\n[1] 53849.8\n\n\nWe see that in this case, Model 3 performed the best on the validation data. Adding variables like lot shape, land contour, and land slope did not help with predictions, and actually cause the model to overfit and perform worse. We should use Model 3 to make predictions on new data over the other models seen here. It is likely that there are better models out there than Model 3, likely with complexity somewhere between that of Model 3 and Models 5-6. Perhaps you can find one.\nOnce we have our preferred model, we can read in our test data and make predictions, and display the first 10 predicted values.\n\nAmes_Test &lt;- read_csv(\"Ames_Test_Data.csv\")\n\nThe first ten rows of the test data are shown below.\n\nhead(Ames_Test, 10)\n\n# A tibble: 10 × 25\n     PID `MS SubClass` `Lot Frontage` `Lot Area` `Lot Shape` `Land Contour`\n   &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;         \n 1  1659            30             90       8100 Reg         Lvl           \n 2   677            20             49      15256 IR1         Lvl           \n 3  1976            20             82      20270 IR1         Lvl           \n 4    42            60             50      13128 IR1         HLS           \n 5  1442            20             60       7200 Reg         Lvl           \n 6  1782            70             57       8094 Reg         Lvl           \n 7  2389            85             75       9825 Reg         Low           \n 8  1657            70             60      11340 Reg         Lvl           \n 9   401            60             82       9709 IR1         Lvl           \n10  2571            50             66      21780 Reg         Lvl           \n# ℹ 19 more variables: Neighborhood &lt;chr&gt;, `Bldg Type` &lt;chr&gt;,\n#   `Overall Qual` &lt;dbl&gt;, `Overall Cond` &lt;dbl&gt;, `Year Built` &lt;dbl&gt;,\n#   `Year Remod/Add` &lt;dbl&gt;, `Total Bsmt SF` &lt;dbl&gt;, `Central Air` &lt;chr&gt;,\n#   `1st Flr SF` &lt;dbl&gt;, `2nd Flr SF` &lt;dbl&gt;, `Gr Liv Area` &lt;dbl&gt;,\n#   `Full Bath` &lt;dbl&gt;, `TotRms AbvGrd` &lt;dbl&gt;, Fireplaces &lt;dbl&gt;,\n#   `Garage Cars` &lt;dbl&gt;, `Garage Area` &lt;dbl&gt;, `Mo Sold` &lt;dbl&gt;, `Yr Sold` &lt;dbl&gt;,\n#   SalePrice &lt;lgl&gt;\n\n\nWe use Model 3 to predict the prices of the new houses. The first 10 predictions are shown below.\n\npredictions &lt;- predict(model3, newdata=Ames_Test)  # substitute your best model\nhead(data.frame(predictions), 10)\n\n   predictions\n1    155806.18\n2    245394.28\n3    238933.54\n4    243591.93\n5    115427.13\n6    162572.07\n7    106909.10\n8     47830.52\n9    331618.06\n10   179100.12\n\n\nA quick look at the data reveals that the one predicted to be most expensive was House #9, which was a newer house built in 2007, and was the biggest and one of the highest overall quality houses among the 10.\nWe create a csv file containing the predictions, using the code below.\n\nwrite.csv(predictions, file = \"predictions.csv\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#ridge-regression",
    "href": "Ch7.html#ridge-regression",
    "title": "7  Predictive Modeling",
    "section": "7.3 Ridge Regression",
    "text": "7.3 Ridge Regression\n\n7.3.1 Complexity in Model Coefficients\nWe’ve thought about complexity in terms of the number of terms we include in a model, as well as whether we include quadratic terms and higher order terms and interactions. We can also think about model complexity in terms of the coefficients \\(b_1, \\ldots, b_p\\). Larger values of \\(b_1, \\ldots, b_p\\) are associated with more complex models. Smaller values of \\(b_1, \\ldots, b_p\\) are associated with less complex models. When \\(b_j=0\\), this mean variable \\(j\\) is not used in the model.\nTo illustrate, we fit a regression model to the Ames housing dataset, which includes 24 possible explanatory variables, in addition to price.\nWe’ll begin by standardizing all variables in the dataset using the scale command. This involves subtracting the mean and dividing by the standard deviation. Standardizing ensures that all variables are on the same scale. If we didn’t do this, then the weight given to each variable might differ depending on its units of measure.\n\nAmes_Train_sc &lt;- Ames_Train |&gt;  mutate_if(is.numeric, scale) \n\nThe full list of coefficient estimates is shown below.\n\nM_OLS &lt;- lm(data=Ames_Train_sc, SalePrice ~ .)\nM_OLS$coefficients\n\n            (Intercept)          `Overall Qual`            `Year Built` \n        -13.59130882125           0.12172875370           0.18710242157 \n         `Mas Vnr Area`          `Central Air`Y           `Gr Liv Area` \n          0.08021260696          -0.04619169355           0.23762329102 \n         `Lot Frontage`            `1st Flr SF`         `Bedroom AbvGr` \n         -0.00429094489           0.06991064978          -0.02245793663 \n        `TotRms AbvGrd`                   Order                     PID \n          0.01757415312           0.09500860850           0.00814856877 \n       `MS SubClass`030        `MS SubClass`040        `MS SubClass`045 \n          0.02155059515           0.07236147708           0.08858457552 \n       `MS SubClass`050        `MS SubClass`060        `MS SubClass`070 \n         -0.01640305733          -0.09127505556          -0.01176201580 \n       `MS SubClass`075        `MS SubClass`080        `MS SubClass`085 \n         -0.13055431025          -0.08036368019          -0.08050060537 \n       `MS SubClass`090        `MS SubClass`120        `MS SubClass`160 \n         -0.26998526617          -0.25031708359          -0.42965417371 \n       `MS SubClass`180        `MS SubClass`190      `MS Zoning`C (all) \n         -0.23213470433          -0.15319391551          -0.47834524646 \n          `MS Zoning`FV      `MS Zoning`I (all)           `MS Zoning`RH \n         -0.28507463985          -0.21229264775          -0.16410610735 \n          `MS Zoning`RL           `MS Zoning`RM              `Lot Area` \n         -0.21759850136          -0.30565512401           0.07285454709 \n             StreetPave               AlleyPave                 AlleyNA \n          0.45956145163           0.03116785010           0.00035798897 \n         `Lot Shape`IR2          `Lot Shape`IR3          `Lot Shape`Reg \n          0.10576302368           0.12823643814           0.03300652196 \n      `Land Contour`HLS       `Land Contour`Low       `Land Contour`Lvl \n          0.13958728988          -0.27690361930           0.15084179216 \n    `Lot Config`CulDSac         `Lot Config`FR2         `Lot Config`FR3 \n          0.12860460385          -0.15191268632          -0.17953546763 \n     `Lot Config`Inside         `Land Slope`Mod         `Land Slope`Sev \n         -0.01440783225           0.13011816970          -0.29862915223 \n    NeighborhoodBlueste      NeighborhoodBrDale     NeighborhoodBrkSide \n          0.23434173527           0.25354320980           0.17654221032 \n    NeighborhoodClearCr     NeighborhoodCollgCr     NeighborhoodCrawfor \n          0.09100974262          -0.00057509876           0.35488850236 \n    NeighborhoodEdwards     NeighborhoodGilbert      NeighborhoodGreens \n         -0.09239036865           0.02406940481           0.05756764076 \n     NeighborhoodIDOTRR     NeighborhoodMeadowV     NeighborhoodMitchel \n          0.15383179212           0.23298493233          -0.11438052494 \n      NeighborhoodNAmes     NeighborhoodNoRidge     NeighborhoodNPkVill \n          0.02197635792           0.39600745608           0.24174493098 \n    NeighborhoodNridgHt      NeighborhoodNWAmes     NeighborhoodOldTown \n          0.42434522209          -0.04153578370           0.12156771029 \n     NeighborhoodSawyer     NeighborhoodSawyerW     NeighborhoodSomerst \n          0.08339168198           0.04866431880           0.25224969818 \n    NeighborhoodStoneBr       NeighborhoodSWISU      NeighborhoodTimber \n          0.54951171242           0.10506427869           0.01795586846 \n    NeighborhoodVeenker      `Condition 1`Feedr       `Condition 1`Norm \n          0.07103275546          -0.00389153252           0.12384803301 \n      `Condition 1`PosA       `Condition 1`PosN       `Condition 1`RRAe \n          0.61047137485           0.19577375822          -0.14305492898 \n      `Condition 1`RRAn       `Condition 1`RRNn      `Condition 2`Feedr \n          0.09735755018           0.07455474531           0.09422184185 \n      `Condition 2`Norm       `Condition 2`PosA       `Condition 2`PosN \n          0.08392208658           0.01845182052          -2.77118265740 \n      `Condition 2`RRNn          `Overall Cond`        `Year Remod/Add` \n          0.25685094522           0.07395388864           0.03009699255 \n      `Roof Style`Gable     `Roof Style`Gambrel         `Roof Style`Hip \n         -0.01212949831          -0.04656300430          -0.01259174339 \n    `Roof Style`Mansard        `Roof Style`Shed      `Roof Matl`CompShg \n          0.23077338051          -0.11780553549           8.27597326004 \n     `Roof Matl`Membran      `Roof Matl`Tar&Grv      `Roof Matl`WdShngl \n          9.10486598342           8.05640391369           9.34788521212 \n  `Mas Vnr Type`BrkFace      `Mas Vnr Type`None     `Mas Vnr Type`Stone \n         -0.17632717617          -0.06386385349          -0.14509043700 \n         `Exter Qual`Fa          `Exter Qual`Gd          `Exter Qual`TA \n         -0.27013813954          -0.46168757537          -0.49820871424 \n         `Exter Cond`Fa          `Exter Cond`Gd          `Exter Cond`Po \n         -0.03082238434           0.11871185662          -0.11211143073 \n         `Exter Cond`TA        FoundationCBlock         FoundationPConc \n          0.13537361237           0.02635149973           0.05354235814 \n         FoundationSlab         FoundationStone          FoundationWood \n         -0.09880253750           0.06424781234          -0.26515863060 \n          `Bsmt Qual`Fa           `Bsmt Qual`Gd           `Bsmt Qual`Po \n         -0.23578437965          -0.16449941673           0.72119653468 \n          `Bsmt Qual`TA           `Bsmt Qual`NA       `Bsmt Exposure`Gd \n         -0.18381997798           0.36058921359           0.10416303942 \n      `Bsmt Exposure`Mn       `Bsmt Exposure`No       `Bsmt Exposure`NA \n         -0.06951087282          -0.07062963631          -0.34886062368 \n         `BsmtFin SF 1`          `BsmtFin SF 2`           `Bsmt Unf SF` \n          0.20674038443           0.04997252798           0.07271883848 \n            HeatingGasW             HeatingWall          `Heating QC`Fa \n          0.04383900333           0.20322162437          -0.08974832333 \n         `Heating QC`Gd          `Heating QC`Po          `Heating QC`TA \n         -0.00937967426          -0.22178284792          -0.07042513045 \n        ElectricalFuseF         ElectricalFuseP           ElectricalMix \n          0.00898381678           0.31259189683           0.63780393390 \n        ElectricalSBrkr            `2nd Flr SF`        `Bsmt Full Bath` \n         -0.03542484148           0.06406031762          -0.00322398443 \n       `Bsmt Half Bath`             `Full Bath`             `Half Bath` \n          0.00838482813           0.02519505108           0.02015786337 \n        `Kitchen AbvGr`        `Kitchen Qual`Fa        `Kitchen Qual`Gd \n         -0.03349189241          -0.09607308952          -0.16716388306 \n       `Kitchen Qual`TA          FunctionalMaj2          FunctionalMin1 \n         -0.16401133535          -0.30302784752          -0.10685732156 \n         FunctionalMin2           FunctionalMod           FunctionalTyp \n         -0.15674128599          -0.17753056314           0.05187402523 \n             Fireplaces        `Fireplace Qu`Fa        `Fireplace Qu`Gd \n          0.09346633107          -0.15744505091          -0.17518128333 \n       `Fireplace Qu`Po        `Fireplace Qu`TA        `Fireplace Qu`NA \n         -0.28539807056          -0.21810062721          -0.06851015534 \n    `Garage Type`Attchd    `Garage Type`Basment    `Garage Type`BuiltIn \n         -0.01354841109          -0.00861375121          -0.05669345934 \n   `Garage Type`CarPort     `Garage Type`Detchd         `Garage Yr Blt` \n         -0.12923248607          -0.00171401502          -0.01955800208 \n     `Garage Finish`RFn      `Garage Finish`Unf           `Garage Cars` \n         -0.05356568330          -0.01828384382           0.02471914400 \n          `Garage Area`         `Garage Qual`Fa         `Garage Qual`Gd \n          0.06042408704          -0.86850349015          -0.63039690210 \n        `Garage Qual`Po         `Garage Qual`TA         `Garage Cond`Fa \n         -1.65328010914          -0.83578836183           0.91247325140 \n        `Garage Cond`Gd         `Garage Cond`Po         `Garage Cond`TA \n          0.79060216838           1.39008604944           0.87884284151 \n         `Paved Drive`P          `Paved Drive`Y          `Wood Deck SF` \n          0.06483515908           0.02561801326           0.01158689081 \n        `Open Porch SF`        `Enclosed Porch`            `3Ssn Porch` \n         -0.01299180889          -0.00928427635           0.00590680860 \n         `Screen Porch`             `Pool Area`             `Pool QC`TA \n          0.02956291048          -0.04923465233           0.30278881156 \n            `Pool QC`NA               FenceGdWo              FenceMnPrv \n         -1.24145842019           0.00004591047           0.01932880459 \n              FenceMnWw                 FenceNA      `Misc Feature`Gar2 \n         -0.01085804835           0.01599859694           6.72349662646 \n     `Misc Feature`Othr      `Misc Feature`Shed        `Misc Feature`NA \n          7.02488714846           6.80128988914           6.75717447918 \n             `Misc Val`               `Mo Sold`               `Yr Sold` \n          0.01467591793          -0.00862316912           0.09610313650 \n         `Sale Type`Con        `Sale Type`ConLD        `Sale Type`ConLI \n          0.09849131799           0.29564887816           0.11747454409 \n       `Sale Type`ConLw          `Sale Type`CWD          `Sale Type`New \n          0.13635880661          -0.11779397614           0.28511673010 \n         `Sale Type`Oth          `Sale Type`VWD          `Sale Type`WD  \n          0.49002891445          -0.14466277644           0.05944448834 \n`Sale Condition`AdjLand  `Sale Condition`Alloca  `Sale Condition`Family \n          0.50907010355           0.30168612306           0.00499302864 \n `Sale Condition`Normal `Sale Condition`Partial \n          0.01963743062          -0.05724696756 \n\n\nLet’s focus on the first 10 rows.\n\nhead(coef(M_OLS),10) %&gt;% round(3)\n\n    (Intercept)  `Overall Qual`    `Year Built`  `Mas Vnr Area`  `Central Air`Y \n        -13.591           0.122           0.187           0.080          -0.046 \n  `Gr Liv Area`  `Lot Frontage`    `1st Flr SF` `Bedroom AbvGr` `TotRms AbvGrd` \n          0.238          -0.004           0.070          -0.022           0.018 \n\n\nIf all coefficients in the model were 0, then we would be using the most simple constant model, and the prediction for the price of each house would be exactly the same as the overall mean. As \\(b_j's\\) get farther from 0, predictions begin move away from the overall mean and depend more and more on the values or categories of the explanatory variable(s) associated with individual houses. This creates a risk, however, of overfitting.\nA way to combat this, other than dropping variables from the model, is to shrink some or all of the regression coefficients closer to 0, pushing predictions closer to the overall mean.\nA statistical technique for doing this is called ridge regression.\n\n\n7.3.2 Ridge Regression Penalty\nWe’ve seen that in ordinary least-squares regression, \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that to minimizes\n\\[\n\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}i)^2 =\\displaystyle\\sum_{i=1}^{n} (y_i -(b_0 + b_1x{i1} + b_2{x_i2} + \\ldots +b_px\\_{ip}))^2\n\\]\nWhen \\(p\\) is large and we want to be careful of overfitting, a common approach is to add a “penalty term” to this function, to incentive choosing values of \\(b_1, \\ldots, b_p\\) that are closer to 0, thereby “shrinking” the predictions toward the overall mean house price.\nSpecifically, we minimize:\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\n\\end{aligned}\n\\]\nwhere \\(\\lambda\\) is a pre-determined positive constant.\nLarger values of \\(b_j\\) typically help the model better fit the training data, thereby making the first term smaller, but also make the second term larger. The idea is the find optimal values of \\(b_0, b_1, \\ldots, b_p\\) that are large enough to allow the model to fit the data well, thus keeping the first term (SSR) small, while also keeping the penalty term small as well.\n\n\n7.3.3 Choosing \\(\\lambda\\)\nThe value of \\(\\lambda\\) is predetermined by the user. The larger the value of \\(\\lambda\\), the more heavily large \\(b_j's\\) are penalized. A value of \\(\\lambda=0\\) corresponds to ordinary least-squares.\n\\[\n\\begin{aligned}\nQ=& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\n\\end{aligned}\n\\]\n\nSmall values of \\(\\lambda\\) lead to more complex models, with larger \\(|b_j|\\)’s.\n\nAs \\(\\lambda\\) increases, \\(|b_j|\\)’s shrink toward 0. The model becomes less complex, thus bias increases, but variance decreases.\n\nWe can use cross validation to determine the optimal value of \\(\\lambda\\)\n\n\n\n\n\n\n\n\n\n\nWhen using ridge regression, it is important to standardize each explanatory variable (i.e. subtract the mean and divide by the standard deviation). This ensures each variable has mean 0 and standard deviation 1. Without standardizing the optimal choice of \\(b_j\\)’s would depend on scale, with variables with larger absolute measurements having more influence. We’ll standardize the response variable too. Though this is not strictly necessary, it doesn’t hurt. We can always transform back if necessary.\n\n\n7.3.4 Ridge Regression on Housing Dataset\nWe’ll use the caret package to perform cross validation in order to find the optimal value of \\(\\lambda\\). To use ridge regression, we specify method = \"glmnet\", and tuneGrid=expand.grid(alpha=0, lambda=l_vals). Note the alpha value can be changed to use other types of penalized regression sometimes used in predictive modeling, such as lasso or elastic net.\n\ncontrol = trainControl(\"repeatedcv\", number = 10, repeats=10)\nl_vals = 10^seq(-4, 4, length = 1000)  # test values between 1/10000 and 10000\n\nset.seed(11162020)\nHousing_ridge &lt;- train(SalePrice ~ .,\n                       data = Ames_Train_sc, method = \"glmnet\", trControl=control , \n                      tuneGrid=expand.grid(alpha=0, lambda=l_vals))\n\nValue of \\(\\lambda\\) minimizing RMSPE:\n\nHousing_ridge$bestTune$lambda\n\n[1] 0.6248788\n\n\nWe examine RMSPE on the withheld data as a function of \\(\\lambda\\).\n\n\n\n\n\n\n\n\n\nUsing \\(\\lambda\\) = 0.6248788, obtain the following set of ridge regression coefficients. Notice how the ridge coefficients are typically closer to 0 than the ordinary least squares coefficients, indicating a less complex model.\n\nM_OLS_sc &lt;- lm(data=Ames_Train_sc, SalePrice ~ .)\nOLS_coef &lt;- M_OLS_sc$coefficients \nRidge_coef &lt;- coef(Housing_ridge$finalModel, Housing_ridge$bestTune$lambda)[,1]\ndf &lt;- data.frame(OLS_coef[-1] |&gt; round(3), Ridge_coef[-1] |&gt; round(3))\nnames(df) &lt;-c(\"OLS Coeff\", \"Ridge Coeff\")\nkable(df[1:10,])\n\n\n\n\n\nOLS Coeff\nRidge Coeff\n\n\n\n\nOverall Qual\n0.122\n0.104\n\n\nYear Built\n0.187\n0.035\n\n\nMas Vnr Area\n0.080\n0.062\n\n\nCentral AirY\n-0.046\n0.041\n\n\nGr Liv Area\n0.238\n0.078\n\n\nLot Frontage\n-0.004\n0.009\n\n\n1st Flr SF\n0.070\n0.071\n\n\nBedroom AbvGr\n-0.022\n0.014\n\n\nTotRms AbvGrd\n0.018\n0.050\n\n\nOrder\n0.095\n-0.004\n\n\n\n\n\nPredictions and residuals for the first six houses in the traning data, using ordinary least squares and ridge regression, are shown below.\n\nlibrary(glmnet)\nMAT &lt;- model.matrix(SalePrice~., data=Ames_Train_sc)\nridge_mod &lt;- glmnet(x=MAT, y=Ames_Train_sc$SalePrice, alpha = 0, lambda=Housing_ridge$bestTune$lambda )\n\n\ny &lt;- Ames_Train_sc$SalePrice\nPred_OLS &lt;- predict(M_OLS_sc)\nPred_Ridge &lt;- predict(ridge_mod, newx=MAT)\nOLS_Resid &lt;- y - Pred_OLS\nRidge_Resid &lt;- y - Pred_Ridge\nResdf &lt;- data.frame(y, Pred_OLS, Pred_Ridge, OLS_Resid, Ridge_Resid) |&gt; round(2)\nnames(Resdf) &lt;- c(\"y\", \"OLS Pred\", \"Ridge Pred\", \"OLS Resid\", \"Ridge Resid\")\nkable(head(Resdf))\n\n\n\n\n\ny\nOLS Pred\nRidge Pred\nOLS Resid\nRidge Resid\n\n\n\n\n859\n-0.62\n-0.46\n-0.46\n-0.16\n-0.16\n\n\n1850\n0.68\n1.19\n1.05\n-0.51\n-0.37\n\n\n1301\n-0.45\n-0.45\n-0.50\n0.00\n0.04\n\n\n981\n-0.64\n-0.66\n-0.78\n0.02\n0.14\n\n\n2694\n-0.79\n-0.87\n-0.75\n0.07\n-0.04\n\n\n2209\n-0.79\n-0.70\n-0.64\n-0.10\n-0.15\n\n\n\n\n\n\n\n7.3.5 Ridge vs OLS\nIn OLS, we choose \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that minimizes\n\\[\n\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2\n\\]\nOLS: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\)\n\nsum((y-Pred_OLS)^2)\n\n[1] 56.94383\n\n\nRidge: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\)\n\nsum((y-Pred_Ridge)^2)\n\n[1] 129.0514\n\n\nNot surprisingly the OLS model achieves smaller \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\). This has to be true, since the OLS coefficients are chosen to minimize this quantity.\nIn ridge regression, \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that minimizes\n\\[\n\\begin{aligned}\nQ=& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\n\\end{aligned}\n\\]\nOLS: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\)\n\nsum((y-Pred_OLS)^2) + Housing_ridge$bestTune$lambda*sum(coef(M_OLS_sc)[-1]^2) \n\n[1] 378.9323\n\n\nRidge: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\)\n\nsum((y-Pred_Ridge)^2) + Housing_ridge$bestTune$lambda*sum((Ridge_coef)[-1]^2)\n\n[1] 132.3112\n\n\nWe see that the ridge coefficients achieve a lower value of Q than the OLS ones.\n\n\n7.3.6 Lasso and Elastic Net\nTwo other techniques that are similar to ridge regression are lasso and elastic net. Both also aim to avoid overfitting by shrinking regression coefficients toward 0 in a manner similar to ridge regression.\nLasso regression is very similar to ridge regression. Coefficients \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that to minimizes\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^p|b_j|\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^p|b_j|\n\\end{aligned}\n\\]\nRegression with an elastic net uses both ridge and lasso penalty terms and determines the values of \\(b_0, b_1, \\ldots, b_p\\) by minimizing\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2  + \\lambda\\displaystyle\\sum_{j=1}^p|b_j|\\\\ =  & \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda_1\\displaystyle\\sum_{j=1}^pb_j^2+ \\lambda_2\\displaystyle\\sum_{j=1}^p|b_j|\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#decision-trees",
    "href": "Ch7.html#decision-trees",
    "title": "7  Predictive Modeling",
    "section": "7.4 Decision Trees",
    "text": "7.4 Decision Trees\n\n7.4.1 Basics of Decision Trees\nA decision tree is a flexible alternative to a regression model. It is said to be nonparametric because it does not involve parameters like \\(\\beta_0, \\beta_1, \\ldots \\beta_p\\). A tree makes no assumption about the nature of the relationship between the response and explanatory variables, and instead allows us to learn this relationship from the data. A tree makes prediction by repeatedly grouping together like observations in the training data. We can make predictions for a new case, by tracing it through the tree, and averaging responses of training cases in the same terminal node.\nDecision Tree Example:\nWe fit a decision tree to the Ames Housing dataset, using the rpart function in a package by the same name.\n\nlibrary(rpart)\nlibrary(rpart.plot)\ntree &lt;- rpart(SalePrice ~., data=Ames_Train, cp=0.04)\nrpart.plot(tree, box.palette=\"RdBu\", shadow.col=\"gray\", nn=TRUE, cex=1, extra=1)\n\n\n\n\n\n\n\n\nWe see that the houses are first split based on whether or not their overall quality rating was less than 8. Each of the resulting nodes are then split again, using information from other explanatory variables. Each split partitions the data further, so that houses in the same node can be thought of as being similar to one another.\n\nThe predicted price of a House with overall quality 7, and was built in 1995 is $200,000.\nThe predicted price of a House overall quality 8 and 1,750 sq. ft. on the first floor is $370,000.\n\n\n\n7.4.2 Partitioning in A Decision Tree\nFor a quantitative response variable, data are split into two nodes so that responses in the same node are as similar as possible, while responses in the different nodes are as different as possible.\nLet L and R represent the left and right nodes from a possible split. Let \\(n_L\\) and \\(n_R\\) represent the number of observations in each node, and \\(\\bar{y}_L\\) and \\(\\bar{y}_R\\) represent the mean of the training data responses in each node.\nFor each possible split, involving an explanatory variable, we calculate:\n\\[\n\\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 + \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2\n\\]\nWe choose the split that minimizes this quantity.\nPartitioning Example\nConsider a dataset with two explanatory variables, \\(x_1\\) and \\(x_2\\), and a response variable \\(y\\), whose values are shown numerically in the graph.\n\n\n   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\nx1    8    2    8    1    8    6    2    5    1     8     4    10     9     8\nx2    5    3    1    1    4    3    8    1   10     8     6     5     0     2\ny   253   64  258   21  257  203  246  114  331   256   213   406   326   273\n   [,15]\nx1     6\nx2     1\ny    155\n\n\n\n\n\n\n\n\n\n\n\nThe goal is to split up the data, using information about \\(x_1\\) and \\(x_2\\) in a way that makes the \\(y\\) values grouped together as similar as possible.\n1. One Possible Split (\\(x_1 &lt; 5.5\\))\nWe could split the data into 2 groups depending on whether \\(x_1 &lt; 5.5\\).\n\n\n\n\n\n\n\n\n\nWe calcuate the mean y-value in each resulting node:\n\n\\(\\bar{y}_L = (331+246+213+21+64+114)/6 \\approx 164.84\\)\n\n\\(\\bar{y}_R = (203+155+256+253+257+273+258+326+406)/9 \\approx 265.22\\)\n\nTo measure measure the amount of deviation in the node, we calculate the sum of the squared difference between each individual value and the overall mean in each node.\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2  \\\\\n& =(331-164.83)^2+(246-164.33)^2 + \\ldots+(114-164.33)^2 \\\\\n& =69958.83\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\\\\n& =(203-265.22)^2+(155-265.22)^2 + \\ldots+(406-265.22)^2 \\\\\n& =39947.56\n\\end{aligned}\n\\]\nAdding together these two quantities, we obtain an overall measure of the squared deviations between observations in the same node.\n\n69958.83 + 39947.56 = 109906.4\n\n2.Second Possible Split (\\(x_1 &lt; 6.5\\))\nWe could alternatively split the data into 2 groups depending on whether \\(x_1 &lt; 6.5\\).\n\n\n\n\n\n\n\n\n\nUsing this split,\n\n\\(\\bar{y}_L = (331+246+213+21+64+114 + 203+155)/8 \\approx 168.375\\)\n\n\\(\\bar{y}_R = (256+253+257+273+258+326+406)/7 \\approx 289.857\\)\n\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2  \\\\\n& =(331-168.375)^2+(246-168.375)^2 + \\ldots+(203-168.375)^2 \\\\\n& =71411.88\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\\\\n& =(203-289.857)^2+(155-289.857)^2 + \\ldots+(406-289.857)^2 \\\\\n& =19678.86\n\\end{aligned}\n\\]\nThe total squared deviation is:\n\n71411.88 + 19678.86 = 91090.74\n\nThe split at \\(x1 &lt; 6.5\\) is better than \\(x_1&lt;5.5\\)\n3. Third Possible Split (\\(x_2 &lt; 5.5\\))\nWe could also split the data into 2 groups depending on whether \\(x_2 &lt; 5.5\\).\n\n\n\n\n\n\n\n\n\nUsing this split,\n\n\\(\\bar{y}_L = (331+246+213+256)/4 \\approx 261.5\\)\n\n\\(\\bar{y}_R = (21 + 64 + \\ldots + 406)/11 \\approx 211.82\\)\n\n\\[\n\\begin{aligned}\n& \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2  \\\\\n& =(331-261.5)^2+(246-261.5)^2 + (213-261.5)^2+(256-261.5)^2 \\\\\n& =7453\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\\\\n& =(21-211.82)^2+(64-211.82)^2 + \\ldots+(406-211.82)^2 \\\\\n& =131493.6\n\\end{aligned}\n\\]\nThe sum of squared deviations is:\n\n7453 + 131493.6 = 138946.6\n\nComparison of Splits\n\nOf the three split’s we’ve calculated, \\(\\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 + \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2\\) is minimized using \\(x_1 &lt; 6.5\\).\nIn fact, if we calculate all possible splits over \\(x_1\\) and \\(x_2\\), \\(\\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 + \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2\\) is minimized by splitting on \\(x_1 &lt; 6.5\\)\n\nThus, we perform the first split in the tree, using \\(x_1 &lt; 6.5\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.3 Next Splits\nNext, we find the best splits on the resulting two nodes. It turns out that the left node is best split on \\(x_2 &lt; 4.5\\), and the right node is best split on \\(x_1 &lt; 8.5\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.4 Recursive Partitioning\nSplitting continues until nodes reach a certain predetermined minimal size, or until change improvement in model fit drops below a predetermined value\n\n\n\n\n\n\n\n\n\n\n\n7.4.5 Model Complexity in Trees\nThe more we partition data into smaller nodes, the more complex the model becomes. As we continue to partition, bias decreases, as cases are grouped with those that are more similar to themselves. On the other hand, variance increases, as there are fewer cases in each node to be averaged, putting more weight on each individual observation.\nSplitting into too small of nodes can lead to drastic overfitting. In the extreme case, if we split all the way to nodes of size 1, we would get RMSE of 0 on the training data, but should certainly not expect RMSPE of 0 on the test data.\nThe optimal depth of the tree, or minimal size for terminal nodes can be determined using cross-validation. The rpart package uses a complexity parameter cp, which determines how much a split must improve model fit in order to be made. Smaller values of cp are associated with more complex tree models, since they allow splits even when model fit only improves by a little.\n\n\n7.4.6 Cross-Validation on Housing Data\nWe’ll use caret to determine the optimal value of the cp parameter. We use method=\"rpart\" to grow decision trees.\n\ncp_vals = 10^seq(-8, 1, length = 100) # test values between 1/10^8 and 1\ncolnames(Ames_Train_sc) &lt;- make.names(colnames(Ames_Train_sc))\n\nset.seed(11162020)\nHousing_Tree &lt;- train(data=Ames_Train_sc, SalePrice ~ .,  method=\"rpart\", trControl=control, \n                     tuneGrid=expand.grid(cp=cp_vals))\n\nThe optimal value of cp is:\n\nHousing_Tree$bestTune\n\n             cp\n52 0.0004328761\n\n\nWe plot RMSPE on the holdout data as a function of cp.\n\ncp &lt;- Housing_Tree$results$cp\nRMSPE &lt;- Housing_Tree$results$RMSE\nggplot(data=data.frame(cp, RMSPE), aes(x=cp, y=RMSPE))+geom_line() + xlim(c(0,0.001)) + ylim(c(0.475,0.485))  + \n  ggtitle(\"Regression Tree Cross Validation Results\")\n\n\n\n\n\n\n\n\n\n\n7.4.7 Comparing OLS, Lasso, Ridge, and Tree\n\nset.seed(11162020)\nHousing_OLS &lt;- train(data=Ames_Train_sc, SalePrice ~ .,  method=\"lm\", trControl=control)\nset.seed(11162020)\nHousing_lasso &lt;- train(SalePrice ~., data = Ames_Train_sc, method = \"glmnet\", trControl=control, tuneGrid=expand.grid(alpha=1, lambda=l_vals))\n\nRMSPE on the standardized version of the response variable is displayed below for ordinary least squares, ridge regression, lasso regression, and a decision tree.\n\nmin(Housing_OLS $results$RMSE)\n\n[1] 0.5612835\n\nmin(Housing_ridge$results$RMSE)\n\n[1] 0.4552125\n\nmin(Housing_lasso$results$RMSE)\n\n[1] 0.4764163\n\nmin(Housing_Tree$results$RMSE)\n\n[1] 0.477414\n\n\nIn this situation, the tree outperforms OLS, but does not do as well as the ridge regression model. The best model will vary depending on the nature of the data. We can use cross-validation to determine which model is likely to perform best in prediction.\n\n\n7.4.8 Random Forest\nA popular extension of a decision tree is a random forest. A random forest consists of many (often ~10,000) trees. Predictions are made by averaging predictions from individual trees.\n\nIn order to ensure the trees are different from each other:\n\neach tree is grown from a different bootstrap sample of the training data.\n\nwhen deciding on a split, only a random subset of explanatory variables are considered.\n\n\nGrowing deep trees ensures low bias. In a random forest, averaging across many deep trees decreases variance, while maintaining low bias.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#regression-splines",
    "href": "Ch7.html#regression-splines",
    "title": "7  Predictive Modeling",
    "section": "7.5 Regression Splines",
    "text": "7.5 Regression Splines\n\n7.5.1 Regression Splines\nWe’ve seen that we can use polynomial regression to capture nonlinear trends in data.\n\nA regression spline is a piecewise function of polynomials.\n\nHere we’ll keep thing simple by focusing on a spline with a single explanatory variable. Splines can also be used for multivariate data.\nWe’ll examine the use of splines on the car price prediction dataset.\nWe divide the data into a set of 75 cars, which we’ll use to train the model, and 35 cars, on which we’ll make and evaluate predictions.\nThe 75 cars in the training set are shown below.\n\n\n\n\n\n\n\n\n\n\n\n7.5.2 Two Models with High Bias\n\n\n\n\n\n\n\n\n\nThe constant and linear models have high bias, as they are not complex enough to capture the apparent curvature in the relationship between price and acceleration time.\nA cubic model, on the other hand might better capture the trend.\n\n\n\n\n\n\n\n\n\n\n\n7.5.3 Cubic Splines\nIt’s possible that the behavior of the response variable might differ in different regions of the x-axis. A cubic spline allows us to fit different models in different regions of the x-axis.\n\n\n\n\n\n\n\n\n\nThe region boundaries are called knots\nCubic Spline with 5 Knots\n\n\n\n\n\n\n\n\n\nCubic Spline with 10 Knots\n\n\n\n\n\n\n\n\n\nCubic Spline with 20 Knots\n\n\n\n\n\n\n\n\n\nNotice that as the number of knots increases, the model becomes more and more complex. We would not expect the relationship between price and acceleration time to look like it does in these more complicated pictures. It is likely that as the number of knots gets big, the model overfits the training data.\n\n\n7.5.4 Predicting Test Data\nShown below is a plot of RMSPE when predictions are made on the new test data.\n\n\n\n\n\n\n\n\n\nWe see that RMSPE is minimized using the model with three knots.\n\n\n7.5.5 Implementation of Splines\nImportant Considerations:\n\nhow many knots\n\nwhere to place knots\n\ndegree of polynomial\n\nThe best choices for all of these will vary between datasets and can be assessed through cross-validation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#assessing-a-classifiers-performance",
    "href": "Ch7.html#assessing-a-classifiers-performance",
    "title": "7  Predictive Modeling",
    "section": "7.6 Assessing a Classifier’s Performance",
    "text": "7.6 Assessing a Classifier’s Performance\n\n7.6.1 Measuring Prediction Accuracy\nJust as we’ve done for models with quantitative variables, we’ll want to compare and assess the performance of models for predicting categorical responses. This might involve comparing llogistic regression models with different explanatory variables, or comparing a regression model to another technique such as a decision tree.\nJust as we did before, we’ll divide the data so that we can evaluate predictions on a subset of the data that was not used to fit the model.\nWe’ll divide the credit card dataset into a set of 9,000 observations, on which we’ll fit our models and assess predictions on the remaining 1,000.\n\nset.seed(08172022)\nlibrary(ISLR)\ndata(Default)\nsamp &lt;- sample(1:nrow(Default), 1000)\nDefault_Test &lt;- Default[samp, ]\nDefault_Train &lt;- Default[-samp, ]\n\nWe fit the model with interaction to the training data:\n\nLR_Default_M_Int &lt;- glm(data=Default_Train, default ~ balance * student, family = binomial(link = \"logit\"))\nsummary(LR_Default_M_Int)\n\n\nCall:\nglm(formula = default ~ balance * student, family = binomial(link = \"logit\"), \n    data = Default_Train)\n\nCoefficients:\n                      Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        -11.2714061   0.5188284 -21.725 &lt;0.0000000000000002 ***\nbalance              0.0060696   0.0003273  18.547 &lt;0.0000000000000002 ***\nstudentYes           0.0924588   0.8606304   0.107               0.914    \nbalance:studentYes  -0.0004749   0.0005142  -0.924               0.356    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2617.1  on 8999  degrees of freedom\nResidual deviance: 1385.5  on 8996  degrees of freedom\nAIC: 1393.5\n\nNumber of Fisher Scoring iterations: 8\n\n\nWe then use the model to estimate the probability of a person defaulting on their credit card payment.\nInformation about 10 different credit card users, as well as the logistic regression estimate of their probability of default are shown below. The table also shows whether or not the user really defaulted on their payment.\n\nLR_Prob &lt;- predict(LR_Default_M_Int, newdata=Default_Test, type=\"response\") %&gt;% round(2)\nActual_Default &lt;- Default_Test$default #factor(ifelse(Default_Test$default==1, \"Yes\", \"No\"))\nstudent &lt;- Default_Test$student\nbalance &lt;- Default_Test$balance\nLR_Res_df &lt;- data.frame(student, balance, LR_Prob, Actual_Default)\nkable(head(LR_Res_df, 50)%&gt;% arrange(desc(LR_Prob)) %&gt;% head(10))\n\n\n\n\n\nstudent\nbalance\nLR_Prob\nActual_Default\n\n\n\n\n2465\nYes\n2026.864\n0.54\nNo\n\n\n1228\nNo\n1682.201\n0.26\nNo\n\n\n6656\nNo\n1551.028\n0.14\nNo\n\n\n1185\nNo\n1541.813\n0.13\nNo\n\n\n9963\nYes\n1635.175\n0.12\nNo\n\n\n6635\nNo\n1434.128\n0.07\nYes\n\n\n9691\nNo\n1391.318\n0.06\nNo\n\n\n5921\nYes\n1513.542\n0.06\nNo\n\n\n9755\nNo\n1233.619\n0.02\nNo\n\n\n7569\nYes\n1294.286\n0.02\nNo\n\n\n\n\n\n\n\n7.6.2 Decision Tree Classifier\nFor comparison, let’s use a decision tree to predict whether a person will default.\nIn a binary classification problem, we can treat a default as \\(y=1\\) and non-default as \\(y=0\\), and grow the tree as we would in regression.\nThe mean response in a node \\(\\bar{Y}\\), which is equivalent to the proportion of people in the node who defaulted, can be interpreted as the probability of default.\nThe first few splits of the tree are shown.\n\nlibrary(rpart)\nlibrary(rpart.plot)\n# grow shorter tree for illustration\ntree &lt;- rpart(data=Default_Train, default~balance + student, cp=0.005)\nrpart.plot(tree, box.palette=\"RdBu\", shadow.col=\"gray\", nn=TRUE, cex=1, extra=1)\n\n\n\n\n\n\n\n\n\n# grow full tree\ntree &lt;- rpart(data=Default_Train, default~balance + student)\n\n\nTree_Prob &lt;- predict(tree, newdata = Default_Test)[,2] %&gt;% round(2)\n\nWe add the decision tree probabilities to the table seen previously.\n\nLR_Res_df &lt;- data.frame(student, balance, LR_Prob, Tree_Prob, Actual_Default)\nkable(head(LR_Res_df, 50)%&gt;% arrange(desc(LR_Prob)) %&gt;% head(10))\n\n\n\n\n\nstudent\nbalance\nLR_Prob\nTree_Prob\nActual_Default\n\n\n\n\n2465\nYes\n2026.864\n0.54\n0.77\nNo\n\n\n1228\nNo\n1682.201\n0.26\n0.02\nNo\n\n\n6656\nNo\n1551.028\n0.14\n0.02\nNo\n\n\n1185\nNo\n1541.813\n0.13\n0.02\nNo\n\n\n9963\nYes\n1635.175\n0.12\n0.02\nNo\n\n\n6635\nNo\n1434.128\n0.07\n0.02\nYes\n\n\n9691\nNo\n1391.318\n0.06\n0.02\nNo\n\n\n5921\nYes\n1513.542\n0.06\n0.02\nNo\n\n\n9755\nNo\n1233.619\n0.02\n0.02\nNo\n\n\n7569\nYes\n1294.286\n0.02\n0.02\nNo\n\n\n\n\n\nWe see that the tree estimates that the first person has a 0.77 probability of defaulting on the payment, compared to an estimate of 0.54, given by the logistic regression model. On the other hand, the tree estimates only a 0.16 probability of the second person defaulting, compared to 0.26 for the logistic regression model.\n\n\n7.6.3 Assessing Classifier Accuracy\nWe’ve seen \\(\\text{RMSPE} = \\sqrt{\\displaystyle\\sum_{i=1}^{n}{(\\hat{y}_i-y_i)^2}}\\) used as a measure of predictive accuracy in a regression problem.\nSince our outcome is not numeric, this is not a good measure of predictive accuracy in a classification problem. We’ll examine some alternatives we can use instead.\nClassification Accuracy\nOne simple approach is calculate the proportion of credit card users classified correctly. If a person has model estimates a predicted probability of default greater than 0.5, the person is predicted to default, while if the probability estimate is less than 0.5, the person is predicted to not default.\nThe table shows the prediction for each of the 10 users, using both logistic regression and the decision tree.\n\nLR_Pred &lt;- factor(ifelse(LR_Prob &gt; 0.5, \"Yes\", \"No\"))\nTree_Pred &lt;- factor(ifelse(Tree_Prob &gt; 0.5, \"Yes\", \"No\"))\nLR_Res_df &lt;- data.frame(student, balance, LR_Prob, Tree_Prob, LR_Pred,Tree_Pred, Actual_Default)\nkable(head(LR_Res_df, 50)%&gt;% arrange(desc(LR_Prob)) %&gt;% head(10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudent\nbalance\nLR_Prob\nTree_Prob\nLR_Pred\nTree_Pred\nActual_Default\n\n\n\n\n2465\nYes\n2026.864\n0.54\n0.77\nYes\nYes\nNo\n\n\n1228\nNo\n1682.201\n0.26\n0.02\nNo\nNo\nNo\n\n\n6656\nNo\n1551.028\n0.14\n0.02\nNo\nNo\nNo\n\n\n1185\nNo\n1541.813\n0.13\n0.02\nNo\nNo\nNo\n\n\n9963\nYes\n1635.175\n0.12\n0.02\nNo\nNo\nNo\n\n\n6635\nNo\n1434.128\n0.07\n0.02\nNo\nNo\nYes\n\n\n9691\nNo\n1391.318\n0.06\n0.02\nNo\nNo\nNo\n\n\n5921\nYes\n1513.542\n0.06\n0.02\nNo\nNo\nNo\n\n\n9755\nNo\n1233.619\n0.02\n0.02\nNo\nNo\nNo\n\n\n7569\nYes\n1294.286\n0.02\n0.02\nNo\nNo\nNo\n\n\n\n\n\nNotice that although the probabilities differ, the logistic regression model and classification tree give the same predictions for these ten cases. Both correctly predict 8 out of the 10 cases, but mistakenly predict the first person to default, when they didn’t, and mistakenly predict that the sixth person would not default when they did.\nWe’ll check the classification accuracy for the model and the tree.\n\nbase::sum(LR_Pred == Actual_Default)/1000\n\n[1] 0.972\n\n\n\nbase::sum(Tree_Pred == Actual_Default)/1000\n\n[1] 0.97\n\n\nWe see that the two techniques are each right approximately 97% of the time.\nThis may not really be as good as it sounds. Can you think of a very simple classification strategy that would achieve a similarly impressive predictive accuracy on these data?\n\n\n7.6.4 Confusion Matrix\nIn addition to assessing overall accuracy, it is sometimes helpful to assess how well models are able to predict outcomes in each class. For example, how accurately can a model detect people who do actually default on their payments?\nA confusion matrix is a two-by-two table displaying the number of cases predicted in each category as columns, and the number of cases actually in each category as rows\n\n\n\n\nActually Negative\nActually Positive\n\n\n\n\nPredicted Negative\n# True Negative\n# False Negative\n\n\nPredicted Positive\n# False Positive\n# True Positive\n\n\n\nThe confusionMatrix matrix command in R returns the confusion matrix for all 1,000 test cases.\nLet’s look at the confusion matrix for all 1,000 test cases. The data argument is the predicted outcome, and the reference argument is the true outcome. The positive argument is the category that we’ll classify as a positive.\nLogistic Regression Confusion Matrix\n\nconfusionMatrix(data=LR_Pred, reference=factor(Actual_Default) , positive=\"Yes\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  957  20\n       Yes   8  15\n                                          \n               Accuracy : 0.972           \n                 95% CI : (0.9598, 0.9813)\n    No Information Rate : 0.965           \n    P-Value [Acc &gt; NIR] : 0.12988         \n                                          \n                  Kappa : 0.5035          \n                                          \n Mcnemar's Test P-Value : 0.03764         \n                                          \n            Sensitivity : 0.4286          \n            Specificity : 0.9917          \n         Pos Pred Value : 0.6522          \n         Neg Pred Value : 0.9795          \n             Prevalence : 0.0350          \n         Detection Rate : 0.0150          \n   Detection Prevalence : 0.0230          \n      Balanced Accuracy : 0.7101          \n                                          \n       'Positive' Class : Yes             \n                                          \n\n\nOut of 965 people who did not default, the logistic regression model correctly predicted 957 of them.\nOut of 35 people that did default, the model correctly predicted 15 of them.\nTree Confusion Matrix\n\n# data is predicted class\n# reference is actual class\nconfusionMatrix( data = Tree_Pred , reference= Actual_Default, \"Yes\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  954  19\n       Yes  11  16\n                                          \n               Accuracy : 0.97            \n                 95% CI : (0.9574, 0.9797)\n    No Information Rate : 0.965           \n    P-Value [Acc &gt; NIR] : 0.2225          \n                                          \n                  Kappa : 0.5009          \n                                          \n Mcnemar's Test P-Value : 0.2012          \n                                          \n            Sensitivity : 0.4571          \n            Specificity : 0.9886          \n         Pos Pred Value : 0.5926          \n         Neg Pred Value : 0.9805          \n             Prevalence : 0.0350          \n         Detection Rate : 0.0160          \n   Detection Prevalence : 0.0270          \n      Balanced Accuracy : 0.7229          \n                                          \n       'Positive' Class : Yes             \n                                          \n\n\nOut of 965 people who did not default, the logistic regression model correctly predicted 960 of them.\nOut of 35 people that did default, the model correctly predicted 11 of them.\nNotice that the tree was less likely to predict a person to default in general, returning only 16 positive predictions, compared to 23 for the logistic regression model.\n\n\n7.6.5 Sensitivity and Specificity\nThe sensitivity of a classifier is the proportion of all positive cases that the model correctly identifies as positive. (i.e. probability model says “positive” given actually is positive.)\n\\[\n\\text{Sensitivity} = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Negative}} = \\frac{\\text{Correctly Predicted Positives}}{\\text{Total Number of Actual Positives}}\n\\]\nLR Sensitivity\n\\[\n\\frac{15}{15+20} \\approx 0.4286\n\\]\nTree Sensitivity\n\\[\n\\frac{11}{11+24} \\approx 0.3143\n\\]\nThe specificity of a classifier is the proportion of all negative cases that the model correctly identifies as negative (i.e probabiltiy model says “negative” given truly is negative.)\n\\[\\text{Specificity} = \\frac{\\text{True Negative}}{\\text{True Negative} + \\text{False Positive}}= \\frac{\\text{Correctly Predicted Negatives}}{\\text{Total Number of Actual Negatives}}\n\\]\nLR Specificity\n\\[\\frac{957}{957+8} \\approx 0.9917\\]\nTree Specificity\n\\[\\frac{960}{960+5} \\approx 0.9948 \\]\nIn a given situation, we should think about the cost of a false negative vs a false positive when determining whether to place more weight on sensitivity or specificity. For example, “is it worse to tell a patient they tested positive for a disease when they really don’t have it, or to not tell them they tested positive when they really do have it?”",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#receiver-operating-characteristic-curve",
    "href": "Ch7.html#receiver-operating-characteristic-curve",
    "title": "7  Predictive Modeling",
    "section": "7.7 Receiver Operating Characteristic Curve",
    "text": "7.7 Receiver Operating Characteristic Curve\n\n7.7.1 Separating +’s and -’s\nThe prediction accuracy, sensitivity, and specificity measures, seen in the previous section are based only on the predicted outcome, without considering the probability estimates themselves. These techniques treat a 0.49 estimated probability of default the same as a 0.01 estimated probability.\nWe would hope to see more defaults among people with high estimated default probabilities than low ones. To assess this, we can list the people in order from highest to lowest probability estimates and see where the true defaults lie.\nFor example, consider the following fictional probability estimates produced by two different classifiers (models) for eight credit card users:\nClassifier 1\n\n\n  Classifier1_Probability_Estimate True_Outcome\n1                             0.90          Yes\n2                             0.75          Yes\n3                             0.60           No\n4                             0.40          Yes\n5                             0.30           No\n6                             0.15           No\n7                             0.05           No\n8                             0.01           No\n\n\nClassifier 2\n\n\n  Classifier2_Probability_Estimate True_Outcome\n1                             0.80          Yes\n2                             0.70           No\n3                             0.55           No\n4                             0.40          Yes\n5                             0.35           No\n6                             0.15           No\n7                             0.10          Yes\n8                             0.02           No\n\n\nClassifier 1 is better able to separate the “Yes’s” from “No’s” as the three true “Yes’s” are among the four highest probabilities. Classifier 2 is less able to separate the true “Yes’s” from true “No’s.”\n\n\n7.7.2 ROC Curve\nA receiver operating characteristic (ROC) curve tells us how well a predictor is able to separate positive cases from negative cases.\nThe blog (Toward Data Science) [https://towardsdatascience.com/applications-of-different-parts-of-an-roc-curve-b534b1aafb68] writes\n“Receiver Operating Characteristic (ROC) curve is one of the most common graphical tools to diagnose the ability of a binary classifier, independent of the inherent classification algorithm. The ROC analysis has been used in many fields including medicine, radiology, biometrics, natural hazards forecasting, meteorology, model performance assessment, and other areas for many decades and is increasingly used in machine learning and data mining research [1]. If you are a Data Scientist, you might be using it on a daily basis.”\nThe ROC curve plots the true positive (or hit) rate against the false positive rate (false alarm) rate, as the cutoff for a positive classification varies.\n\n\n\n\n\n\n\n\n\nThe higher the curve, the better the predictor is able to separate positive cases from negative ones.\nPredictions made totally at random would be expected to yield a diagonal ROC curve.\n\n\n7.7.3 Constructing ROC Curve\n\nOrder the probabilities from highest to lowest.\n\nAssume only the case with the highest probability is predicted as a positive.\n\nCalculate the true positive rate (hit rate) \\[\\frac{\\text{\\# True Positives}}{\\text{\\# Actual Positives}}\\] and false positive (false alarm) \\[\\frac{\\text{\\# False Positives}}{\\text{\\# Actual Negatives}}\\]rate.\nPlot the point \\[\\left( \\frac{\\text{\\# False Positives}}{\\text{\\# Actual Negatives}}, \\frac{\\text{\\# True Positives}}{\\text{\\# Actual Positives}} \\right)\\] in the coordinate plane.\n\nNow assume the cases with the two highest probabilities are predicted as positives, and repeat steps 3-4.\n\nContinue, by classifiying one more case as positive in each step.\n\n\n\n7.7.4 Construct ROC Example\nLet’s practice constructing an ROC curve for a small set of probability estimates.\n\nprob &lt;- c(0.9, 0.8, 0.7, 0.65, 0.45, 0.3, 0.2, 0.15, 0.1, 0.05)\nActual &lt;- c(\"+\", \"-\", \"+\", \"+\", \"-\", \"-\", \"-\", \"-\", \"+\", \"-\")\nHit_Rate &lt;- c(\"1/4\", \"1/4\", \"2/4\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")\nFA_Rate &lt;- c(\"0/6\", \"1/6\", \"1/6\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")\nkable(data.frame(prob, Actual, Hit_Rate, FA_Rate))\n\n\n\n\nprob\nActual\nHit_Rate\nFA_Rate\n\n\n\n\n0.90\n+\n1/4\n0/6\n\n\n0.80\n-\n1/4\n1/6\n\n\n0.70\n+\n2/4\n1/6\n\n\n0.65\n+\n\n\n\n\n0.45\n-\n\n\n\n\n0.30\n-\n\n\n\n\n0.20\n-\n\n\n\n\n0.15\n-\n\n\n\n\n0.10\n+\n\n\n\n\n0.05\n-\n\n\n\n\n\n\n\nFinish filling in the table and sketch a graph of the resulting ROC curve.\nQuestion: If the probability estimate of 0.45 were instead 0.5 or 0.55, would this change the ROC curve? Why or why not?\n\n\n7.7.5 AUC\nThe area under the ROC curve, (AUC) provides a measure of the model’s predictive strength.\nWhile there is no standard for what constitutes a good\" AUC, higher is better, andAUC” is useful for comparing models.\nA model that can perfectly separate successes from failures will have an AUC of 1.\nA model that assigns probabilities at random is expected to have an AUC of 0.5.\n\n\n7.7.6 LR and Tree ROC Curves\n\nlibrary(pROC)\nlibrary(verification)\nroc.plot(x=Default_Test$default==\"Yes\", pred = LR_Prob)\n\n\n\n\n\n\n\n\n\nauc(response=Default_Test$default==\"Yes\", predictor = LR_Prob)\n\nArea under the curve: 0.8953\n\n\n\nroc.plot(x=Default_Test$default==\"Yes\", pred = Tree_Prob)\n\n\n\n\n\n\n\n\n\nauc(response=Default_Test$default==\"Yes\", predictor = Tree_Prob)\n\nArea under the curve: 0.75\n\n\n\nRandProb &lt;- runif(1000, 0, 1)\n\n\nroc.plot(x=Default_Test$default==\"Yes\", pred = RandProb)\n\n\n\n\n\n\n\n\n\nauc(response=Default_Test$default, predictor = RandProb)\n\nArea under the curve: 0.563\n\n\nEven though a model that assigns predictions randomly, with 97% predicted as negatives will have a high accuracy rate, it will yield a poor ROC curve indicating an inability to separate positive cases from negative ones.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  },
  {
    "objectID": "Ch7.html#ethical-considerations-in-predictive-modeling",
    "href": "Ch7.html#ethical-considerations-in-predictive-modeling",
    "title": "7  Predictive Modeling",
    "section": "7.8 Ethical Considerations in Predictive Modeling",
    "text": "7.8 Ethical Considerations in Predictive Modeling\n\n7.8.1 Assumptions in Predictive Models\nLike any other statistical technique, predictive inference (sometimes done through machine learning algorithms) depends on the validity of assumptions.\n\nThe response variable observed in the data is actually the thing we want to predict\n\nTraining/Test data representative of population of interest\nPrediction accuracy is appropriate metric\n\nBelow are some examples of real uses of predictive inference in which some of these assumptions were violated, leading to inappropriate and unethical conclusions.\n\n\n7.8.2 Amazon Hiring Algorithm\nIn 2014, Amazon began working on an algorithm to predict whether a job applicant would be suitable for hire for software developer positions, based on characteristics of their job application.\nresponse variable: rating of candidate’s strength (1-5) explanatory variables: many variables based on information included on the resume (e.g. highest degree, major, GPA, college/university, prior job experiences, internships, frequency of certain words on resume, etc.)\nThe algorithm was trained using data from past applications, rated by humans, over the past 10 years. It could then be used to predict ratings of future job applicants.\nAccording to [Reuters])(https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G),\n“In effect, Amazon’s system taught itself that male candidates were preferable. It penalized resumes that included the word “women’s,” as in “women’s chess club captain.” And it downgraded graduates of two all-women’s colleges, according to people familiar with the matter.”\nWhile the algorithm was intended to predict candidate quality, the response variable on the training data actually reflected biases in past hiring decisions, leading the algorithm to do the same.\n\n\n7.8.3 Facial Recognition\nFacial recognition technology is used by law enforcement surveillance, airport passenger screening, and employment and housing decisions. It has, however, been banned for use by police in some cities, including San Francisco and Boston, due to concerns about inequity and privacy.\nResearch has shown that although certain facial recognition algorithms achieve over 90% accuracy overall, accuracy rate is lower among subjects who are female, Black, or 18-30 years old.\nThis is likely due, at least in part, to the algorithms being trained primarily on data an images of people who are not members of these groups.\nAlthough the algorithms might attain strong accuracy overall, it is inappropriate to evaluate them on this basis, without accounting for performance on subgroups in the population.\n\n\n7.8.4 Comments\nThe biases and assumptions noted above are not reasons to abandon predictive modeling, but rather flaws to be aware of and work to correct.\nPredictive algorithms, are only as good as the data on which they are trained and the societies in which they are developed, and will reflect inherent biases. Thus, they should be used cautiously and with with human judgment, just like any other statistical technique.\nBeware of statements like:\n“The data say this!”\n“The algorithm is objective.”\n“The numbers don’t lie.”\nAny data-driven analysis depends on assumptions, and sound judgment and awareness of context are required when assessing the validity of conclusions drawn.\n\n\n7.8.5 Modeling for Prediction\n\nGoal is to make the most accurate predictions possible.\n\nNot concerned with understanding relationships between variables. Not worried model being to complicated to interpret, as long as it yields good predictions.\n\nAim for a model that best captures the signal in the data, without being thrown off by noise.\n\n\nLarge number of predictors is ok\n\nDon’t make model so complicated that it overfits the data.\n\n\nBe sure that model is predicting what you intend it to\n\nReflective of biases inherent in the data on which it was trained",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling</span>"
    ]
  }
]